{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "from dateutil.parser import parse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from NLP_tools import Cleaning_text, top_keywords, top_entities, get_topic_name, best_document, clean_all, topic_documents\n",
    "from core.functions import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "from opensearch_data_model import Topic, TopicKeyword, News, os_client, TOPIC_INDEX_NAME, NEWS_INDEX_NAME\n",
    "from opensearch_io import init_opensearch, get_news\n",
    "from opensearchpy import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El índice Topic ya existe. Saltando inicialización de base de datos.\n",
      "El índice News ya existe. Saltando inicialización de base de datos.\n"
     ]
    }
   ],
   "source": [
    "init_opensearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
    "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
    "PATH\n",
    "\n",
    "if PATH == os.environ.get('PATH_LOCAL'):\n",
    "    client = OpenAI(api_key= os.environ.get('OPENAI_API_KEY'))\n",
    "else:\n",
    "    from google.colab import userdata\n",
    "    client = OpenAI(api_key= userdata.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para testear app streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_1000\n"
     ]
    }
   ],
   "source": [
    "df_params = {'0_1000':'0_1000_data.parquet',\n",
    "             '1000_2000':'1000_2000_data.parquet',\n",
    "             '2000_3000':'2000_3000_data.parquet',\n",
    "             'df_joined':'df_joined_2024-04-01 00_00_00.parquet'\n",
    "            }\n",
    "\n",
    "chunk = '0_1000' \n",
    "#chunk = '1000_2000' \n",
    "df_parquet = pd.read_parquet(PATH+df_params[chunk])\n",
    "data = list(df_parquet['in__text'])\n",
    "\n",
    "# Cantidad total de documentos\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = \"app\"\n",
    "# Cargar modelo entrenado o saltar celda y entrenar\n",
    "topic_model = BERTopic.load(PATH+f\"modelos/bertopic_model_{chunk}\")\n",
    "\n",
    "# Cargar los embeddings\n",
    "docs_embedding = np.load(PATH+f\"modelos/docs_embeddings_{chunk}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ead0b09217d470ea19fbd26c237e190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topics, probs = topic_model.transform(data)\n",
    "\n",
    "#topics = np.load(PATH+f\"modelos/topics_{chunk}.npy\")\n",
    "#probs = np.load(PATH+f\"modelos/probs_{chunk}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion_aux(ID):\n",
    "\n",
    "    query = {\n",
    "                'query': {\n",
    "                    'match': {\n",
    "                        '_id': ID  # Sustituir 'campo' y 'valor' por campo y valor de búsqueda\n",
    "                    }\n",
    "                }\n",
    "    }\n",
    "    \n",
    "    response = os_client.search(index='news', body=query)\n",
    "\n",
    "    # Procesar la respuesta\n",
    "    results = response['hits']['hits']\n",
    "\n",
    "    title = [ result['_source']['title'] for result in results]\n",
    "    news = [ result['_source']['news'] for result in results]\n",
    "    topic = [ result['_source']['topic'] for result in results]\n",
    "    prob = [ result['_source']['prob'] for result in results]\n",
    "    keywords = [ result['_source']['keywords'] for result in results]\n",
    "    entities = [ result['_source']['entities'] for result in results]\n",
    "    fila = df_parquet.index.get_loc(ID)\n",
    "\n",
    "    try:\n",
    "        keywords_df = df_parquet[df_parquet.index==ID]['Keyword Name'].values[0]\n",
    "    except:\n",
    "        keywords_df = \"\"\n",
    "    try:\n",
    "        entities_df = df_parquet[df_parquet.index==ID]['Entity Name'].values[0]\n",
    "    except:\n",
    "        entities_df = \"\"\n",
    "\n",
    "    print(f\"Noticia ID: {ID} {title}\")\n",
    "    print(f\"Entities originales: {entities_df}\")\n",
    "    print(f\"Keywords originales: {keywords_df}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Topico: {topic}\")\n",
    "    print(f\"Fila: {fila}\")\n",
    "    print(f\"Prob. modelo: {prob}\")\n",
    "    print(f\"Entities calculadas: {entities}\")\n",
    "    print(f\"Keywords calculadas: {keywords}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Noticia: {news[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noticia ID: 105579854 ['El cielo estará nublado y se esperan lluvias durante toda la jornada en Tucumán']\n",
      "Entities originales: ['']\n",
      "Keywords originales: ['madrugada' 'precipitaciones' 'humedad' 'lluvias' 'cielo']\n",
      "--------------------------------------------------------------------------------\n",
      "Topico: [14]\n",
      "Fila: 505\n",
      "Prob. modelo: [1.0]\n",
      "Entities calculadas: [[]]\n",
      "Keywords calculadas: [[]]\n",
      "--------------------------------------------------------------------------------\n",
      "Noticia: Por la noche se espera que el cielo continúe cubierto, que la humedad sea del 97 %, mientras que la temperatura retrocederá nuevamente, esta vez hasta los 22 °C. Se espera que las precipitaciones se extienden hasta la madrugada del martes.\n"
     ]
    }
   ],
   "source": [
    "funcion_aux(105579854)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resetear la base de news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'succeeded': True, 'num_freed': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre del índice\n",
    "index_name = 'news'\n",
    "\n",
    "# Campo a actualizar y nuevo valor\n",
    "campo_a_actualizar_1 = 'process'\n",
    "campo_a_actualizar_2 = 'topic'\n",
    "campo_a_actualizar_3 = 'prob'\n",
    "nuevo_valor_1 = False\n",
    "nuevo_valor_2 = -1\n",
    "nuevo_valor_3 = 0\n",
    "\n",
    "# Buscar todos los documentos en el índice\n",
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Obtener todos los documentos\n",
    "response = os_client.search(index=index_name, body=search_query, scroll='2m', size=1000)\n",
    "\n",
    "documents = response['hits']['hits']\n",
    "scroll_id = response['_scroll_id']\n",
    "\n",
    "# Preparar operaciones bulk\n",
    "bulk_operations = []\n",
    "\n",
    "# Procesar el primer lote de documentos\n",
    "for doc in documents:\n",
    "    bulk_operations.append({\n",
    "        \"_op_type\": \"update\",\n",
    "        \"_index\": index_name,\n",
    "        \"_id\": doc[\"_id\"],\n",
    "        \"doc\": {\n",
    "            campo_a_actualizar_1: nuevo_valor_1,\n",
    "            campo_a_actualizar_2: nuevo_valor_2,\n",
    "            campo_a_actualizar_3: nuevo_valor_3\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Obtener y procesar el resto de los documentos con el scroll\n",
    "while len(documents) > 0:\n",
    "    response = os_client.scroll(scroll_id=scroll_id, scroll='2m')\n",
    "    documents = response['hits']['hits']\n",
    "    scroll_id = response['_scroll_id']\n",
    "\n",
    "    for doc in documents:\n",
    "        bulk_operations.append({\n",
    "            \"_op_type\": \"update\",\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc[\"_id\"],\n",
    "            \"doc\": {\n",
    "                campo_a_actualizar_1: nuevo_valor_1,\n",
    "                campo_a_actualizar_2: nuevo_valor_2,\n",
    "                campo_a_actualizar_3: nuevo_valor_3\n",
    "            }\n",
    "        })\n",
    "\n",
    "# Ejecutar las operaciones bulk\n",
    "helpers.bulk(os_client, bulk_operations)\n",
    "\n",
    "# Eliminar el scroll para liberar los recursos\n",
    "os_client.clear_scroll(scroll_id=scroll_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todos los registros de news con sus topicos\n",
    "\n",
    "query = {\n",
    "        \"size\":1000,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"match_all\": {}}\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Ejecutar la consulta\n",
    "response = os_client.search(index='news', body=query)\n",
    "\n",
    "# Procesar la respuesta\n",
    "results = response['hits']['hits']\n",
    "\n",
    "ID = [ result['_id'] for result in results]\n",
    "title = [ result['_source']['title'] for result in results]\n",
    "news = [ result['_source']['news'] for result in results]\n",
    "topic = [ result['_source']['topic'] for result in results]\n",
    "\n",
    "ID = np.array(ID).reshape(-1,1)\n",
    "title = np.array(title).reshape(-1,1)\n",
    "news = np.array(news).reshape(-1,1)\n",
    "topic = np.array(topic).reshape(-1,1)\n",
    "\n",
    "combined_array = np.hstack((ID, title, news, topic))\n",
    "df = pd.DataFrame(combined_array, columns=[\"id\",\"title\",\"text\",\"topic\"])\n",
    "\n",
    "# Grabar df con etiquetas de topicos\n",
    "file_path = 'data_clasif.xlsx'\n",
    "df.to_excel(PATH+file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"a\":1, \"b\":2, \"c\":3}\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
