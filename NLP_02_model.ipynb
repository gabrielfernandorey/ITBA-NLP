{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielfernandorey/ITBA-NLP/blob/main/NLP_02_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### >>> Solo para correr en Colab"
      ],
      "metadata": {
        "id": "GHn0MELjOcoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "git_token = userdata.get('GIT_TOKEN')\n",
        "git_username = \"gabrielfernandorey\"\n",
        "git_repository = \"ITBA-NLP.git\"\n",
        "\n",
        "!git clone https://{git_token}@github.com/{git_username}/{git_repository}\n",
        "\n",
        "!pip install -r /content/ITBA-NLP/requirements.txt"
      ],
      "metadata": {
        "id": "svVAqf7jOkko",
        "outputId": "c0fbcd3a-4044-4c92-c574-b4edb81eea9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ITBA-NLP' already exists and is not an empty directory.\n",
            "Collecting es-core-news-lg@ https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.7.0/es_core_news_lg-3.7.0-py3-none-any.whl#sha256=08020b83e0c6da1584e567551a5e0de7b15dc0534eaaee21acc1ce908d1be742 (from -r /content/ITBA-NLP/requirements.txt (line 26))\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.7.0/es_core_news_lg-3.7.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py==2.1.0 (from -r /content/ITBA-NLP/requirements.txt (line 1))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting altair==5.3.0 (from -r /content/ITBA-NLP/requirements.txt (line 2))\n",
            "  Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 3)) (0.7.0)\n",
            "Collecting anyio==4.4.0 (from -r /content/ITBA-NLP/requirements.txt (line 4))\n",
            "  Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asttokens==2.4.1 (from -r /content/ITBA-NLP/requirements.txt (line 5))\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: attrs==23.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 6)) (23.2.0)\n",
            "Collecting bertopic==0.16.2 (from -r /content/ITBA-NLP/requirements.txt (line 7))\n",
            "  Downloading bertopic-0.16.2-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blinker==1.8.2 (from -r /content/ITBA-NLP/requirements.txt (line 8))\n",
            "  Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: blis==0.7.11 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 9)) (0.7.11)\n",
            "Collecting cachetools==5.3.3 (from -r /content/ITBA-NLP/requirements.txt (line 10))\n",
            "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: catalogue==2.0.10 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 11)) (2.0.10)\n",
            "Collecting certifi==2024.2.2 (from -r /content/ITBA-NLP/requirements.txt (line 12))\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 13)) (3.3.2)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 14)) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib==0.18.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 15)) (0.18.1)\n",
            "Collecting colorama==0.4.6 (from -r /content/ITBA-NLP/requirements.txt (line 16))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting comm==0.2.2 (from -r /content/ITBA-NLP/requirements.txt (line 17))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: confection==0.1.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 18)) (0.1.5)\n",
            "Requirement already satisfied: contourpy==1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 19)) (1.2.1)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 20)) (0.12.1)\n",
            "Requirement already satisfied: cymem==2.0.8 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 21)) (2.0.8)\n",
            "Collecting Cython==0.29.37 (from -r /content/ITBA-NLP/requirements.txt (line 22))\n",
            "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting debugpy==1.8.1 (from -r /content/ITBA-NLP/requirements.txt (line 23))\n",
            "  Downloading debugpy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator==5.1.1 (from -r /content/ITBA-NLP/requirements.txt (line 24))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting distro==1.9.0 (from -r /content/ITBA-NLP/requirements.txt (line 25))\n",
            "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 27)) (1.1.0)\n",
            "Collecting Events==0.5 (from -r /content/ITBA-NLP/requirements.txt (line 28))\n",
            "  Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
            "Collecting executing==2.0.1 (from -r /content/ITBA-NLP/requirements.txt (line 29))\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Collecting fastjsonschema==2.19.1 (from -r /content/ITBA-NLP/requirements.txt (line 30))\n",
            "  Downloading fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\n",
            "Collecting filelock==3.14.0 (from -r /content/ITBA-NLP/requirements.txt (line 31))\n",
            "  Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
            "Collecting fonttools==4.51.0 (from -r /content/ITBA-NLP/requirements.txt (line 32))\n",
            "  Downloading fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec==2024.6.0 (from -r /content/ITBA-NLP/requirements.txt (line 33))\n",
            "  Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb==4.0.11 (from -r /content/ITBA-NLP/requirements.txt (line 34))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython==3.1.43 (from -r /content/ITBA-NLP/requirements.txt (line 35))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11==0.14.0 (from -r /content/ITBA-NLP/requirements.txt (line 36))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py==3.11.0 (from -r /content/ITBA-NLP/requirements.txt (line 37))\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hdbscan==0.8.36 (from -r /content/ITBA-NLP/requirements.txt (line 38))\n",
            "  Downloading hdbscan-0.8.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.0.5 (from -r /content/ITBA-NLP/requirements.txt (line 39))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx==0.27.0 (from -r /content/ITBA-NLP/requirements.txt (line 40))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub==0.23.3 (from -r /content/ITBA-NLP/requirements.txt (line 41))\n",
            "  Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 42)) (3.7)\n",
            "Collecting intel-openmp==2021.4.0 (from -r /content/ITBA-NLP/requirements.txt (line 43))\n",
            "  Downloading intel_openmp-2021.4.0-py2.py3-none-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipykernel==6.29.4 (from -r /content/ITBA-NLP/requirements.txt (line 44))\n",
            "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython==8.24.0 (from -r /content/ITBA-NLP/requirements.txt (line 45))\n",
            "  Downloading ipython-8.24.0-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.5/816.5 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipywidgets==8.1.3 (from -r /content/ITBA-NLP/requirements.txt (line 46))\n",
            "  Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jedi==0.19.1 (from -r /content/ITBA-NLP/requirements.txt (line 47))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 48)) (3.1.4)\n",
            "Requirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 49)) (1.4.2)\n",
            "Collecting jsonschema==4.22.0 (from -r /content/ITBA-NLP/requirements.txt (line 50))\n",
            "  Downloading jsonschema-4.22.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema-specifications==2023.12.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 51)) (2023.12.1)\n",
            "Collecting jupyter_client==8.6.1 (from -r /content/ITBA-NLP/requirements.txt (line 52))\n",
            "  Downloading jupyter_client-8.6.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 53)) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab_widgets==3.0.11 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 54)) (3.0.11)\n",
            "Collecting keras==3.3.3 (from -r /content/ITBA-NLP/requirements.txt (line 55))\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 56)) (1.4.5)\n",
            "Requirement already satisfied: langcodes==3.4.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 57)) (3.4.0)\n",
            "Requirement already satisfied: language_data==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 58)) (1.2.0)\n",
            "Collecting llvmlite==0.42.0 (from -r /content/ITBA-NLP/requirements.txt (line 59))\n",
            "  Downloading llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: marisa-trie==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 60)) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 61)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 62)) (2.1.5)\n",
            "Collecting matplotlib==3.8.4 (from -r /content/ITBA-NLP/requirements.txt (line 63))\n",
            "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 64)) (0.1.7)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 65)) (0.1.2)\n",
            "Collecting mkl==2021.4.0 (from -r /content/ITBA-NLP/requirements.txt (line 66))\n",
            "  Downloading mkl-2021.4.0-py2.py3-none-manylinux1_x86_64.whl (280.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.9/280.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes==0.4.0 (from -r /content/ITBA-NLP/requirements.txt (line 67))\n",
            "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 68)) (1.3.0)\n",
            "Requirement already satisfied: murmurhash==1.0.10 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 69)) (1.0.10)\n",
            "Collecting namex==0.0.8 (from -r /content/ITBA-NLP/requirements.txt (line 70))\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: nbformat==5.10.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 71)) (5.10.4)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 72)) (1.6.0)\n",
            "Requirement already satisfied: networkx==3.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 73)) (3.3)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 74)) (3.8.1)\n",
            "Collecting numba==0.59.1 (from -r /content/ITBA-NLP/requirements.txt (line 75))\n",
            "  Downloading numba-0.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.26.4 (from -r /content/ITBA-NLP/requirements.txt (line 76))\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==1.30.3 (from -r /content/ITBA-NLP/requirements.txt (line 77))\n",
            "  Downloading openai-1.30.3-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openpyxl==3.1.2 (from -r /content/ITBA-NLP/requirements.txt (line 78))\n",
            "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opensearch-py==2.6.0 (from -r /content/ITBA-NLP/requirements.txt (line 79))\n",
            "  Downloading opensearch_py-2.6.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optree==0.11.0 (from -r /content/ITBA-NLP/requirements.txt (line 80))\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==24.0 (from -r /content/ITBA-NLP/requirements.txt (line 81))\n",
            "  Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.2.2 (from -r /content/ITBA-NLP/requirements.txt (line 82))\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 83)) (0.8.4)\n",
            "Collecting pillow==10.3.0 (from -r /content/ITBA-NLP/requirements.txt (line 84))\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting platformdirs==4.2.1 (from -r /content/ITBA-NLP/requirements.txt (line 85))\n",
            "  Downloading platformdirs-4.2.1-py3-none-any.whl (17 kB)\n",
            "Collecting plotly==5.22.0 (from -r /content/ITBA-NLP/requirements.txt (line 86))\n",
            "  Downloading plotly-5.22.0-py3-none-any.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: preshed==3.0.9 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 87)) (3.0.9)\n",
            "Collecting prompt-toolkit==3.0.43 (from -r /content/ITBA-NLP/requirements.txt (line 88))\n",
            "  Downloading prompt_toolkit-3.0.43-py3-none-any.whl (386 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.1/386.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==4.25.3 (from -r /content/ITBA-NLP/requirements.txt (line 89))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil==5.9.8 (from -r /content/ITBA-NLP/requirements.txt (line 90))\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pure-eval==0.2.2 (from -r /content/ITBA-NLP/requirements.txt (line 91))\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting pyarrow==16.0.0 (from -r /content/ITBA-NLP/requirements.txt (line 92))\n",
            "  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==2.7.1 (from -r /content/ITBA-NLP/requirements.txt (line 93))\n",
            "  Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic_core==2.18.2 (from -r /content/ITBA-NLP/requirements.txt (line 94))\n",
            "  Downloading pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck==0.9.1 (from -r /content/ITBA-NLP/requirements.txt (line 95))\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pygments==2.18.0 (from -r /content/ITBA-NLP/requirements.txt (line 96))\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynndescent==0.5.12 (from -r /content/ITBA-NLP/requirements.txt (line 97))\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/ITBA-NLP/requirements.txt (line 98)) (3.1.2)\n",
            "Collecting python-dateutil==2.9.0.post0 (from -r /content/ITBA-NLP/requirements.txt (line 99))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv==1.0.1 (from -r /content/ITBA-NLP/requirements.txt (line 100))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting pytz==2024.1 (from -r /content/ITBA-NLP/requirements.txt (line 101))\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 0.36.0 Requires-Python >=3.6,<3.10; 0.37.0 Requires-Python >=3.7,<3.10; 0.52.0 Requires-Python >=3.6,<3.9; 0.52.0rc3 Requires-Python >=3.6,<3.9; 0.53.0 Requires-Python >=3.6,<3.10; 0.53.0rc1.post1 Requires-Python >=3.6,<3.10; 0.53.0rc2 Requires-Python >=3.6,<3.10; 0.53.0rc3 Requires-Python >=3.6,<3.10; 0.53.1 Requires-Python >=3.6,<3.10; 0.54.0 Requires-Python >=3.7,<3.10; 0.54.0rc2 Requires-Python >=3.7,<3.10; 0.54.0rc3 Requires-Python >=3.7,<3.10; 0.54.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==306 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==306\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "R7IjMbD1S3O8",
        "outputId": "f47f3243-7e8b-4b3c-d467-2aa401933391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e59GBP66N_BK"
      },
      "source": [
        "## MODELO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "5c718fba-d95f-4b07-91d3-cad4d8c81c00"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'NLP_tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1e0dba5e57b9>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNLP_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCleaning_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_keywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_topic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_document\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'NLP_tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, date\n",
        "from dateutil.parser import parse\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from NLP_tools import Cleaning_text, top_keywords, top_entities, get_topic_name, best_document, clean_all, topic_documents\n",
        "from core.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QSVkkTWN_BP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from bertopic.vectorizers import ClassTfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yECzCViiN_BP"
      },
      "outputs": [],
      "source": [
        "from opensearch_data_model import Topic, TopicKeyword, News, os_client, TOPIC_INDEX_NAME, NEWS_INDEX_NAME\n",
        "from opensearch_io import init_opensearch, get_news\n",
        "from opensearchpy import helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V_iZG5ubN_BQ",
        "outputId": "28130c63-0e41-4b85-a24c-52019d87b2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5ee9a4e68b89>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmkslbsDN_BT"
      },
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZAu1QkwYN_BU",
        "outputId": "86b5d60e-88a3-4ffa-cfab-e89ce3e6086c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ITBA-NLP/data/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ofyYU7FzN_BU",
        "outputId": "cbbb459c-53ed-445a-b37b-5eb0a94bc370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'OpenAI' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a0d4f2c1b62a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'OpenAI' is not defined"
          ]
        }
      ],
      "source": [
        "if PATH == os.environ.get('PATH_LOCAL'):\n",
        "    client = OpenAI(api_key= os.environ.get('OPENAI_API_KEY'))\n",
        "else:\n",
        "    from google.colab import userdata\n",
        "    client = OpenAI(api_key= userdata.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrAEQ0VIN_BR"
      },
      "source": [
        "### Inicializamos la base vectorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOEOjKN4N_BS",
        "outputId": "2e883aa5-49ab-4a7a-8dfb-8574ba88700b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 17:51:46.665 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El índice Topic ya existe. Saltando inicialización de base de datos.\n",
            "El índice News ya existe. Saltando inicialización de base de datos.\n"
          ]
        }
      ],
      "source": [
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Data de noticias original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmp3cLLv28-T"
      },
      "outputs": [],
      "source": [
        "df_params = {'0_1000':'0_1000_data.parquet',\n",
        "             '1000_2000':'1000_2000_data.parquet',\n",
        "             '2000_3000':'2000_3000_data.parquet',\n",
        "             'df_joined':'df_joined_2024-04-01 00_00_00.parquet'\n",
        "            }\n",
        "\n",
        "chunk = os.environ.get('CHUNK')\n",
        "chunk = '0_1000'\n",
        "\n",
        "df_parquet = pd.read_parquet(PATH+df_params[chunk])\n",
        "data = list(df_parquet['in__text'])\n",
        "\n",
        "# Cantidad total de documentos\n",
        "print(chunk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxbMdlz-N_BV"
      },
      "outputs": [],
      "source": [
        "# Cargar vocabulario\n",
        "with open(PATH+f'modelos/vocabulary_{chunk}.json', 'r') as json_file:\n",
        "    vocab = json.load(json_file)\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPRI0LxLN_BV"
      },
      "source": [
        "### StopWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly9P34g0N_BW"
      },
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "SPANISH_STOPWORDS = list(pd.read_csv(PATH+'spanish_stop_words.csv' )['stopwords'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl06QkGoN_BW"
      },
      "source": [
        "### Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U8qmnllN_BW"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "        tokenizer=None,\n",
        "        max_df=0.9,\n",
        "        min_df=0.1,\n",
        "        ngram_range=(1, 2),\n",
        "        vocabulary=vocab,\n",
        "        # max_features=100_000\n",
        ")\n",
        "tfidf_vectorizer.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KB5tMtLN_BW"
      },
      "source": [
        "Capas del modelo BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpb75EAM3R53"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Extract embeddings\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "# Step 2 - Reduce dimensionality\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "# Step 3 - Cluster reduced embeddings\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "# Step 4 - Tokenize topics\n",
        "vectorizer_model = tfidf_vectorizer\n",
        "# Step 5 - Create topic representation\n",
        "ctfidf_model = ClassTfidfTransformer()\n",
        "# Step 6 - (Optional) Fine-tune topic representations with a `bertopic.representation` model\n",
        "representation_model = KeyBERTInspired()\n",
        "\n",
        "# All steps together\n",
        "topic_model = BERTopic(\n",
        "  embedding_model=embedding_model,              # Step 1 - Extract embeddings\n",
        "  umap_model=umap_model,                        # Step 2 - Reduce dimensionality\n",
        "  hdbscan_model=hdbscan_model,                  # Step 3 - Cluster reduced embeddings\n",
        "  vectorizer_model=vectorizer_model,            # Step 4 - Tokenize topics\n",
        "  ctfidf_model=ctfidf_model,                    # Step 5 - Extract topic words\n",
        "  # representation_model=representation_model,  # Step 6 - (Optional) Fine-tune topic represenations\n",
        "  # language='multilingual',                    # This is not used if embedding_model is used.\n",
        "  verbose=True,\n",
        "  # calculate_probabilities=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTgqIgThN_BX"
      },
      "outputs": [],
      "source": [
        "def my_callback(stage, **kwargs):\n",
        "    print(f\"Stage: {stage}\")\n",
        "    for key, value in kwargs.items():\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMZlvrHJN_BX"
      },
      "outputs": [],
      "source": [
        "my_callback(stage=\"start_training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYk1SbbdN_BX"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZb5UYa5N_BX"
      },
      "outputs": [],
      "source": [
        "# Cargar modelo entrenado o saltar celda y entrenar\n",
        "ahora = datetime.today()\n",
        "topic_model = BERTopic.load(PATH+f\"modelos/bertopic_model_{chunk}\")\n",
        "topics = np.load(PATH+f\"modelos/topics_{chunk}.npy\")\n",
        "probs = np.load(PATH+f\"modelos/probs_{chunk}.npy\")\n",
        "\n",
        "# Cargar los embeddings\n",
        "docs_embedding = np.load(PATH+f\"modelos/topic_embeddings_{chunk}.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUSgR75RN_BY"
      },
      "outputs": [],
      "source": [
        "topics, probs = topic_model.fit_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fssvPfyN_BY"
      },
      "outputs": [],
      "source": [
        "# Obtenemos embeddings de todos los documentos\n",
        "docs_embedding = topic_model.embedding_model.embed(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NXVfA04N_BY"
      },
      "outputs": [],
      "source": [
        "# Grabar modelo\n",
        "topic_model.save(PATH+f\"modelos/bertopic_model_{chunk}\")\n",
        "np.save(PATH+f\"modelos/topics_{chunk}.npy\", topics)\n",
        "np.save(PATH+f\"modelos/probs_{chunk}.npy\", probs)\n",
        "\n",
        "# Guardar los embeddings en un archivo de NumPy\n",
        "np.save(PATH+f\"modelos/topic_embeddings_{chunk}.npy\", docs_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McTgk142N_BY"
      },
      "source": [
        "## Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oHKP19MN_BY"
      },
      "outputs": [],
      "source": [
        "print(f\"Cantidad de tópicos {len(set(topics))} (incluye topico -1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZusn9JUN_BY"
      },
      "outputs": [],
      "source": [
        "# Obtener documentos de cada tópico\n",
        "topic_freq = topic_model.get_topic_freq()\n",
        "\n",
        "# Imprimir el número de tópicos encontrados (incluyendo el tópico -1)\n",
        "num_topics = len(topic_freq)\n",
        "print(f\"Número de tópicos encontrados: {num_topics} (incluye el topico -1)\")\n",
        "\n",
        "# Imprimir la cant de documentos de cada tópico\n",
        "print(topic_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8NuMpv7N_BZ"
      },
      "source": [
        "### Recuperar todos los topicos y sus etiquetas generadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyGA8qOBN_BZ"
      },
      "outputs": [],
      "source": [
        "topic_model.generate_topic_labels()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szmrk5CcN_BZ"
      },
      "source": [
        "### Buscar topicos ingresando un texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDMc0KZVN_BZ"
      },
      "outputs": [],
      "source": [
        "topic_res = topic_model.find_topics(\"cambio climatico\")\n",
        "topic_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4lJj7ySN_BZ"
      },
      "source": [
        "### Buscar los titulos de los primeros n documentos de un topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDaQzIduN_BZ"
      },
      "outputs": [],
      "source": [
        "# Obtener los n documentos de un tópico\n",
        "topic_id = 10\n",
        "n_docs = 5\n",
        "topic_docs_idx = [i for i, (doc, topic) in enumerate(zip(list(df_parquet['in__title']), topics)) if topic == topic_id]\n",
        "n_docs = n_docs if n_docs <= len(topic_docs_idx) else len(topic_docs_idx)\n",
        "\n",
        "print(f\"{n_docs} de {len(topic_docs_idx)} titulos de noticias encontrados en el tópico {topic_id:}\")\n",
        "for idx in topic_docs_idx[:n_docs]:\n",
        "    print(\"- \",df_parquet.iloc[idx]['in__title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe7n7ofuN_Ba"
      },
      "source": [
        "#### Busqueda de documentos por topico, ordenados por mayor probabilidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFpaYMzpN_Ba"
      },
      "outputs": [],
      "source": [
        "\n",
        "T = topic_model.get_document_info(data)\n",
        "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
        "\n",
        "topic = 10\n",
        "# topic = np.random.randint(0, len(docs_per_topics)-1) # Aleatorio\n",
        "\n",
        "print(\"Ejemplo para tópico:\", topic)\n",
        "\n",
        "doc_probs_x_topic = []\n",
        "for doc in docs_per_topics[topic]:\n",
        "    doc_probs_x_topic.append([df_parquet.index[doc], df_parquet.iloc[doc].in__title, round(probs[doc],4)])\n",
        "\n",
        "df_query_1 = pd.DataFrame(doc_probs_x_topic)\n",
        "print(len(df_query_1), \"docs encontrados\")\n",
        "df_query_1.sort_values(2, ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaIQUMQfN_Ba"
      },
      "source": [
        "#### Palabras clave del topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COEmgwdeN_Bf"
      },
      "outputs": [],
      "source": [
        "# Obtener las palabras clave para un topico dado\n",
        "topic_keywords = topic_model.get_topic(topic)\n",
        "print(\"Topico:\", topic)\n",
        "print(topic_keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9oQK911N_Bf"
      },
      "source": [
        "### Criterio de corte (umbral)\n",
        "El criterio de corte utilizado para filtrar las noticias que pertenecen a un topico es el valor de -1 desvio std."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAt7-_Q8N_Bf"
      },
      "outputs": [],
      "source": [
        "# Calcular la media, el desvío estándar\n",
        "\n",
        "mean = np.mean([fila[2] for fila in doc_probs_x_topic])\n",
        "std_dev = np.std([fila[2] for fila in doc_probs_x_topic])\n",
        "\n",
        "# Crear el histograma\n",
        "plt.hist([fila[2] for fila in doc_probs_x_topic], bins=10, edgecolor='black')\n",
        "\n",
        "# Añadir líneas para la media, la moda y el desvío estándar\n",
        "plt.axvline(mean, color='r', linestyle='dashed', linewidth=1, label=f'Media: {mean:.2f}')\n",
        "plt.axvline(mean - std_dev, color='b', linestyle='dashed', linewidth=1, label=f'-1 STD: {mean - std_dev:.2f}')\n",
        "\n",
        "\n",
        "# Añadir títulos y etiquetas\n",
        "plt.title(f'Histograma de probabilidades topico: {topic}')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P-1fHJ2N_Bf"
      },
      "source": [
        "#### Documentos mas representativos de un topico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7uKkzfWN_Bg"
      },
      "source": [
        "- [1]  Obtenido por el metodo del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LifmcWPCN_Bg"
      },
      "outputs": [],
      "source": [
        "docs_representative = topic_model.get_representative_docs(topic=topic)\n",
        "docs_representative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5X4ReLMN_Bg"
      },
      "source": [
        "- [2] Obtenido por busqueda de probabilidad de documentos perteneciente al topico ( utilizando el umbral de corte )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ydu-y_jN_Bg"
      },
      "outputs": [],
      "source": [
        "# Definir la función de estilo\n",
        "def color_rows(row, label, value):\n",
        "    if row[label] >= value:\n",
        "        return ['color: cyan'] * len(row)\n",
        "    else:\n",
        "        return [''] * len(row)\n",
        "\n",
        "# Agrupamos documentos por topico\n",
        "T = topic_model.get_document_info(data)\n",
        "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
        "\n",
        "# Obtener los IDs de los documentos y sus probabilidades\n",
        "docs_ids = []\n",
        "docs_topic = []\n",
        "doc_probs_x_topic = []\n",
        "for doc_ID in tqdm(docs_per_topics[topic]):\n",
        "    docs_ids.append(df_parquet.index[doc_ID])\n",
        "    doc_probs_x_topic.append(probs[doc_ID])\n",
        "\n",
        "# Calcular la media, el desvío estándar\n",
        "mean = np.mean(doc_probs_x_topic)\n",
        "std_dev = np.std(doc_probs_x_topic)\n",
        "threshold = mean - std_dev\n",
        "\n",
        "# Crear una consulta de múltiples IDs\n",
        "index_name = 'news'\n",
        "mget_query = {\n",
        "    \"docs\": [{\"_index\": index_name, \"_id\": doc_id} for doc_id in docs_ids]\n",
        "}\n",
        "# Realizar la búsqueda de múltiples IDs\n",
        "response = os_client.mget(body=mget_query, index=index_name)\n",
        "\n",
        "# Procesar la respuesta\n",
        "for i, doc in enumerate(response['docs']):\n",
        "    if doc['found']:\n",
        "        idx = doc['_id']\n",
        "        title = df_parquet.iloc[docs_per_topics[topic][i]].in__title\n",
        "        prob_doc = probs[docs_per_topics[topic][i]]\n",
        "\n",
        "        if 'entities' in doc['_source']:\n",
        "            ent = doc['_source']['entities']\n",
        "        else:\n",
        "            ent = []\n",
        "\n",
        "        docs_topic.append([idx, title, prob_doc, ent])\n",
        "\n",
        "\n",
        "df_view = pd.DataFrame(docs_topic, columns = ['indice','titulo','prob','entidades']).sort_values('prob', ascending=False)\n",
        "df_view.style.apply(lambda row: color_rows(row, 'prob', threshold), axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwt-eN0tN_Bg"
      },
      "source": [
        "Nota: Los documentos mas representativos encontrados utilizando el metodo \"get_representative_docs\" no refleja lo mismo que encontrando los documentos por probabilidades maximas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-1Q3ReVN_Bg"
      },
      "source": [
        "- [3] Por similitud coseno del topico a los tres documento mas cercanos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFCtpfj-N_Bh"
      },
      "outputs": [],
      "source": [
        "# Obtenemos la matriz de similitud coseno entre topicos y documentos\n",
        "sim_matrix = cosine_similarity(topic_model.topic_embeddings_, docs_embedding)\n",
        "sim_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCtO8yBMN_Bh"
      },
      "outputs": [],
      "source": [
        "# Similitud coseno entre el topico y los documentos del topico elegido\n",
        "s_coseno = []\n",
        "for i in docs_per_topics[topic]:\n",
        "    s_coseno.append(cosine_similarity([topic_model.topic_embeddings_[topic + 1]], [docs_embedding[i]])[0][0])\n",
        "\n",
        "# Indices\n",
        "idx_coseno_sort = np.argsort(s_coseno)[::-1]\n",
        "\n",
        "for idx in idx_coseno_sort[:3]:\n",
        "    print(idx, df_parquet.iloc[docs_per_topics[topic][idx]].in__title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRRm3tWeN_Bh"
      },
      "source": [
        "Nota: Del mismo modo que en el punto anterior, los documentos mas cercanos al topico no coinciden no son exactamente los mismos que los hallados en el punto 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAvGGp2JN_Bh"
      },
      "source": [
        "- [4] Primer documento mas cercano al embedding del topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmKsuTeVN_Bi"
      },
      "outputs": [],
      "source": [
        "# Documento de maxima similitud con el topico\n",
        "\n",
        "simil_docs_topic = sim_matrix[topic + 1].argmax()\n",
        "print(f\"Noticia de maxima similitud con el topico: {topic}\")\n",
        "print(f\"Doc ID: {df_parquet.index[simil_docs_topic]}\")\n",
        "print(f\"Titulo: {df_parquet.iloc[simil_docs_topic].in__title}\")\n",
        "print(f\"Noticia: {data[simil_docs_topic][:80]}...\")\n",
        "best_doc = data[simil_docs_topic]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyWurS2NN_Bi"
      },
      "source": [
        "#### Keywords de solo un topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pK_nj_pN_Bi"
      },
      "outputs": [],
      "source": [
        "keywords = topic_model.topic_representations_[topic]\n",
        "topic_keywords = [TopicKeyword(name=keyword, score=score) for keyword, score in keywords if keyword != '']\n",
        "topic_keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nCejcoRN_Bi"
      },
      "outputs": [],
      "source": [
        "# Calculo de umbral de corte para las keywords\n",
        "freq_k = []\n",
        "for name_score in topic_keywords:\n",
        "    freq_k.append(name_score['score'])\n",
        "umbral_k = np.array(freq_k).mean()\n",
        "print(umbral_k)\n",
        "\n",
        "topic_keywords_top = {}\n",
        "for name_score in topic_keywords:\n",
        "    if name_score['score'] >= umbral_k:\n",
        "        topic_keywords_top[name_score['name']] = name_score['score']\n",
        "\n",
        "topic_keywords_top"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6rf17PMN_Bi"
      },
      "source": [
        "#### Entidades de un Topico a partir de los n documentos mas cercanos al embedding del topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABm53qVRN_Bj"
      },
      "outputs": [],
      "source": [
        "# Entidades de documentos ordenados por similitud del topico elelgido\n",
        "n_docs = 5 # n docs cercanos\n",
        "entities_topic = []\n",
        "for doc in df_view[:n_docs].iterrows():\n",
        "    entities_topic.append(doc[1][3])\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Crear un diccionario para contar en cuántos documentos aparece cada palabra\n",
        "document_frequencies = defaultdict(int)\n",
        "\n",
        "# Crear un conjunto para cada documento y contar las palabras únicas\n",
        "for lista in entities_topic:\n",
        "    unique_words = set(lista)\n",
        "    for palabra in unique_words:\n",
        "        document_frequencies[palabra] += 1\n",
        "\n",
        "# Ordenar las palabras por la frecuencia de documentos de mayor a menor\n",
        "sorted_frequencies = sorted(document_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "freq_e = []\n",
        "for item in sorted_frequencies:\n",
        "    freq_e.append(item[1])\n",
        "umbral_e = np.array(freq_e).mean()\n",
        "\n",
        "# Imprimir el resultado ordenado de las primeras 10 entidades segun criterio de corte\n",
        "topic_entities_top = {}\n",
        "c=0\n",
        "for idx in range(len(sorted_frequencies)):\n",
        "    if sorted_frequencies[idx][1] >= umbral_e:\n",
        "        if c != 10:\n",
        "            topic_entities_top[sorted_frequencies[idx][0]] = sorted_frequencies[idx][1]\n",
        "        else:\n",
        "            break\n",
        "        c += 1\n",
        "\n",
        "topic_entities_top\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUUejCL-N_Bj"
      },
      "source": [
        "### Grabar todos los registros en Topic y actualizar en News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9aF8xISN_Bj"
      },
      "outputs": [],
      "source": [
        "def topic_documents(topic, topic_model, probs, df_news):\n",
        "    \"\"\"\n",
        "    función que devuelve los ids de los documentos del tópico por encima del umbral,\n",
        "    los titulos de los documentos del tópico,\n",
        "    y el umbral de corte.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cantidad de documentos por topico\n",
        "        docs_per_topics = [i for i, x in enumerate(topic_model.topics_) if x == topic]\n",
        "\n",
        "        # Obtener los IDs de los documentos y sus probabilidades\n",
        "        docs_IDs = {}\n",
        "        doc_probs = []\n",
        "        for doc_idx in docs_per_topics:\n",
        "\n",
        "            docs_IDs[df_news.indice[doc_idx]] = probs[doc_idx]\n",
        "            doc_probs.append(probs[doc_idx])\n",
        "\n",
        "        # Calcular la media, el desvío estándar\n",
        "        mean = np.mean(doc_probs)\n",
        "        std_dev = np.std(doc_probs)\n",
        "        threshold = mean - std_dev\n",
        "\n",
        "        # Filtra los docs que superan o igualan al valor del umbral calculado\n",
        "        filter = {}\n",
        "        for k,v in docs_IDs.items():\n",
        "            if v >= threshold:\n",
        "                filter[k] = v\n",
        "\n",
        "        # Ordeno de mayor a menor\n",
        "        ids_filter_sort = dict(sorted(filter.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "        title_filter_sort = [ df_news.loc[df_news['indice'] == idx].values[0][1] for idx in ids_filter_sort.keys() ]\n",
        "\n",
        "        return ids_filter_sort, title_filter_sort, threshold\n",
        "    except:\n",
        "        return {}, {}, 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMyrZPyRN_Bj"
      },
      "outputs": [],
      "source": [
        "topic_documents(topic, topic_model, probs, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--slN0q4N_Bj"
      },
      "outputs": [],
      "source": [
        "docs_per_topics = [i for i, x in enumerate(topic_model.topics_) if x == topic]\n",
        "print(docs_per_topics)\n",
        "\n",
        "# Obtener los IDs de los documentos y sus probabilidades\n",
        "docs_IDs = {}\n",
        "doc_probs = []\n",
        "for doc_idx in docs_per_topics:\n",
        "\n",
        "    docs_IDs[df_parquet.indice[doc_idx]] = probs[doc_idx]\n",
        "    doc_probs.append(probs[doc_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrT7t0CZN_Bj"
      },
      "outputs": [],
      "source": [
        "df_parquet.indice[doc_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrY9gGKTN_Bk"
      },
      "outputs": [],
      "source": [
        "# Busqueda de todas las noticias no procesadas de la base ( en False ) (al menos 10.000)\n",
        "db_news = news_no_process()\n",
        "\n",
        "df_news = pd.DataFrame(db_news , columns=[\"indice\", \"titulo\", \"noticia\", \"keywords\", \"entidades\", \"creado\"])\n",
        "id_data    = list(df_news['indice'])\n",
        "title_data = list(df_news['titulo'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-vn-lgFN_Bk"
      },
      "outputs": [],
      "source": [
        "# Cargar entities\n",
        "with open(PATH+f'modelos/entities_{chunk}.json', 'r') as json_file:\n",
        "    entities = json.load(json_file)\n",
        "\n",
        "# Grabar todos los topicos en la base\n",
        "for topic in topic_model.get_topics().keys():\n",
        "    if topic > -1:\n",
        "\n",
        "        topic_keywords_top  = top_keywords(topic, topic_model)\n",
        "        topic_entities_top  = top_entities(topic, topic_model, docs_embedding, data, entities)\n",
        "        topic_documents_ids, topic_documents_title, threshold  = topic_documents(topic, topic_model, probs, df_parquet, data)\n",
        "        id_best_doc, title_best_doc, best_doc = best_document(topic, topic_model, docs_embedding, id_data, title_data, data)\n",
        "\n",
        "        topic_doc = Topic(\n",
        "            index = topic,\n",
        "            name = get_topic_name(''.join({**topic_keywords_top, **topic_entities_top}), client),\n",
        "            vector = list(topic_model.topic_embeddings_[topic + 1 ]),\n",
        "            similarity_threshold = threshold,\n",
        "            created_at = datetime.now(),\n",
        "            to_date = parse('2024-04-02'),\n",
        "            from_date = parse('2024-04-01'),\n",
        "            keywords = topic_keywords_top,\n",
        "            entities = topic_entities_top,\n",
        "            id_best_doc = id_best_doc,\n",
        "            title_best_doc = title_best_doc,\n",
        "            best_doc = best_doc,\n",
        "        )\n",
        "\n",
        "        topic_doc.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQhCtSmFN_Bk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k147r3sEN_Bk"
      },
      "source": [
        "##### Actualizar datos en News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq8SK9Q7N_Bk"
      },
      "outputs": [],
      "source": [
        "# Marcar registros de noticias procesados\n",
        "index_name = 'news'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            'process': False\n",
        "        }\n",
        "    },\n",
        "    'size': 10000\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search( body=search_query, index=index_name )\n",
        "\n",
        "for i, reg in enumerate(response['hits']['hits']):\n",
        "    doc_id = reg['_id']\n",
        "\n",
        "    update_body = {\n",
        "                    \"doc\": {\n",
        "                        \"vector\": ,\n",
        "                        \"process\": True\n",
        "                    }\n",
        "    }\n",
        "\n",
        "    # Realizar la actualización\n",
        "    os_client.update(index=index_name, id=doc_id, body=update_body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJUr6_NTN_Bk"
      },
      "source": [
        "### Recuperar todos los topicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgldS9M_N_Bk"
      },
      "outputs": [],
      "source": [
        "index_name = 'topic'\n",
        "\n",
        "db_topics = []\n",
        "for i, doc in enumerate(Topic.search().query().scan()):\n",
        "    db_topics.append(doc.to_dict())\n",
        "    print(db_topics[i]['index'], db_topics[i]['name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5yBNnkoN_Bl"
      },
      "outputs": [],
      "source": [
        "T = topic_model.find_topics(\"israel\")\n",
        "T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx1i9yqZN_Bl"
      },
      "source": [
        "### Recuperar de la base el documento mas cercano a un topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Yg_ryV3N_Bl"
      },
      "outputs": [],
      "source": [
        "index_name = 'topic'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            'index': 10  # Sustituir 'campo' y 'valor' por campo y valor de búsqueda\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search(\n",
        "                            body=search_query,\n",
        "                            index=index_name\n",
        ")\n",
        "\n",
        "texto = response['hits']['hits'][0]['_source']\n",
        "\n",
        "#Imprimir los resultados\n",
        "print(f\"Topico: {response['hits']['hits'][0]['_source']['name']}\")\n",
        "print(\"\\n\"+ texto['best_doc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is8FQUqbN_Bl"
      },
      "outputs": [],
      "source": [
        "ver_embedding = response['hits']['hits'][0]['_source']['vector']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo9Kjy1zN_Bl"
      },
      "outputs": [],
      "source": [
        "np.array(ver_embedding).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMH2jt5YN_Bl"
      },
      "source": [
        "### Nuevo documento consultando embeddings generados por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c0ZTllTN_Bl"
      },
      "outputs": [],
      "source": [
        "new_doc = \"Fuga de presos en San Telmo\"\n",
        "\n",
        "new_doc_embedding = topic_model.embedding_model.embed(new_doc)\n",
        "sim_matrix_new = cosine_similarity(topic_model.topic_embeddings_, new_doc_embedding.reshape(1, -1))\n",
        "\n",
        "idx = np.argmax(sim_matrix_new)-1         # Topicos desde -1, 0, 1, ..., n\n",
        "print(db_topics[idx]['index'], db_topics[idx]['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lae8_RU7N_Bm"
      },
      "source": [
        "### Nuevo documento consultando embeddings de la base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVylIBiFN_Bm"
      },
      "outputs": [],
      "source": [
        "new_doc = \"Fuga de presos en San Telmo\"\n",
        "\n",
        "new_doc_embedding = topic_model.embedding_model.embed(new_doc)\n",
        "\n",
        "# Buscamos en la base a que topico pertenece el nuevo documento\n",
        "knn_query = {\n",
        "    \"size\": 1,\n",
        "    \"query\": {\n",
        "        \"knn\": {\n",
        "            \"vector\": {\n",
        "                \"vector\": new_doc_embedding,\n",
        "                \"k\" : 3\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "response = os_client.search(index='topic', body=knn_query)\n",
        "\n",
        "if response['hits']['total']['value'] > 0:\n",
        "    print(f\"Topico: {response['hits']['hits'][0]['_source']['name']}\")\n",
        "    print(f\"Estimacion: {response['hits']['hits'][0]['_score']}\")\n",
        "else:\n",
        "    print(f\"Topico no encontrado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzKqZk3HN_Bm"
      },
      "source": [
        "### Graficar los topicos en 3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6E5CljkN_Bm"
      },
      "outputs": [],
      "source": [
        "# Obtener los embeddings de los tópicos\n",
        "topic_embeddings = topic_model.topic_embeddings_\n",
        "\n",
        "new_doc = \"Fuga de presos en San Telmo\"\n",
        "new_doc_embedding = topic_model.embedding_model.embed([new_doc])[0]\n",
        "\n",
        "total = np.vstack((topic_embeddings, new_doc_embedding))\n",
        "\n",
        "# Reducir la dimensionalidad de los embeddings a 3D usando UMAP\n",
        "umap_model = UMAP(n_components=3)\n",
        "embeddings_3d = umap_model.fit_transform(total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKNFKczxN_Bm"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Obtener las etiquetas de los tópicos\n",
        "topic_labels = topic_model.get_topic_info()['Topic']\n",
        "\n",
        "# Crear un DataFrame para Plotly\n",
        "df = pd.DataFrame(embeddings_3d, columns=['Dim1', 'Dim2', 'Dim3'])\n",
        "df['Topico'] = list(range(len(topic_embeddings))) + ['Nuevo Doc'] # Identificamos el nuevo documento\n",
        "df['Etiqueta'] = list(topic_labels) + ['Nuevo Documento']\n",
        "\n",
        "# Graficar los tópicos en 3D de manera interactiva usando Plotly, ajustando el tamaño del gráfico\n",
        "fig = px.scatter_3d(df, x='Dim1', y='Dim2', z='Dim3', color='Topico', text='Etiqueta', title='Visualización 3D de Tópicos con BERTopic')\n",
        "\n",
        "\n",
        "# Ajustar el tamaño del gráfico\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    width=1200,  # Ancho del gráfico\n",
        "    height=800,  # Altura del gráfico\n",
        "    margin=dict(l=65, r=50, b=65, t=90)\n",
        ")\n",
        "\n",
        "# Mostrar las etiquetas en el gráfico\n",
        "fig.update_traces(marker=dict(size=5),\n",
        "                  selector=dict(mode='markers+text'))\n",
        "\n",
        "# Mostrar el gráfico\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqRmxlC3N_Bm"
      },
      "outputs": [],
      "source": [
        "type(topic_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEuORqA6N_Bn"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq_OuRNGN_Bn"
      },
      "source": [
        "### Busqueda de un documento por su indice, topico asociado, keywords, entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXk-XIFWN_Bn"
      },
      "outputs": [],
      "source": [
        "\n",
        "index_name = 'news'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            '_id': '105640350'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search(\n",
        "                            body=search_query,\n",
        "                            index=index_name\n",
        ")\n",
        "print(f\"Texto de Noticia: {response['hits']['hits'][0]['_source']['news'][:200]}...\\n\")\n",
        "\n",
        "new_doc_embedding = topic_model.embedding_model.embed(response['hits']['hits'][0]['_source']['news'])\n",
        "\n",
        "# Define el índice y el campo del vector\n",
        "index_name = 'topic'\n",
        "vector_field = 'vector'\n",
        "\n",
        "# Crear una consulta KNN para buscar el embedding más similar\n",
        "knn_query = {\n",
        "    \"size\": 1,  # Número de resultados que deseas obtener, en este caso 1\n",
        "    \"query\": {\n",
        "        \"knn\": {\n",
        "            vector_field: {\n",
        "                \"vector\": new_doc_embedding,\n",
        "                \"k\": 3  # Número de vecinos más cercanos\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response_topic = os_client.search(index=index_name, body=knn_query)\n",
        "\n",
        "# Obtener el tópico más cercano\n",
        "if response_topic['hits']['total']['value'] > 0:\n",
        "    closest_topic = response_topic['hits']['hits'][0]['_source']\n",
        "    print(f\"El nuevo documento pertenece al tópico: {closest_topic['index']}\")\n",
        "    print(closest_topic['name'])\n",
        "    print(f\"Estimacion: {closest_topic['similarity_threshold']}\")\n",
        "    print(f\"Keywords del topico: {closest_topic['keywords']}\")\n",
        "    print(f\"Entidades del topico: {closest_topic['entities']}\")\n",
        "\n",
        "else:\n",
        "    print(\"No se encontró un tópico cercano.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siTmejNYN_Bn"
      },
      "source": [
        "### Agrupamiento de topicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "344j-GhSN_Bn"
      },
      "outputs": [],
      "source": [
        "index_name = 'topic'\n",
        "\n",
        "db_topics = []\n",
        "for i, doc in enumerate(Topic.search().query().scan()):\n",
        "    db_topics.append(doc.to_dict())\n",
        "    print(db_topics[i]['index'], db_topics[i]['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzGhZXeHN_Bn"
      },
      "source": [
        "#### Agrupando por embeddings cercanos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf2gbRdCN_Bn"
      },
      "outputs": [],
      "source": [
        "topic_embeddings = topic_model.topic_embeddings_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JyQN4MbN_Bo"
      },
      "outputs": [],
      "source": [
        "# Reducir la dimensionalidad a 3D usando UMAP\n",
        "umap_model = UMAP(n_neighbors=5, n_components=3, metric='cosine')\n",
        "umap_embeddings = umap_model.fit_transform(topic_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxh3igeDN_Bo"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Crear un DataFrame para los embeddings y los IDs de los tópicos\n",
        "df = pd.DataFrame(umap_embeddings, columns=['UMAP Dimension 1', 'UMAP Dimension 2', 'UMAP Dimension 3'])\n",
        "df['Topic'] = topic_model.get_topic_info()['Topic'].values\n",
        "\n",
        "# Crear la gráfica 3D interactiva usando Plotly\n",
        "fig = px.scatter_3d(df, x='UMAP Dimension 1', y='UMAP Dimension 2', z='UMAP Dimension 3',\n",
        "                    text='Topic', title='Embeddings de los Tópicos Reducidos a 3D con UMAP',\n",
        "                    width=1000, height=800)  # Ajustar el tamaño del gráfico\n",
        "\n",
        "# Mostrar la gráfica\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaBUMOTJN_Bo"
      },
      "outputs": [],
      "source": [
        "# Eliminamos el topico -1\n",
        "new_topic_embeddings = topic_embeddings[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56733kEBN_Bo"
      },
      "outputs": [],
      "source": [
        "# Calcular la similitud del coseno entre los embeddings de los tópicos\n",
        "similarities = cosine_similarity(new_topic_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCbSPOqcN_Bo"
      },
      "outputs": [],
      "source": [
        "# Crear una matriz de similitud excluyendo la diagonal\n",
        "np.fill_diagonal(similarities, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MFTdlBJN_Bo"
      },
      "outputs": [],
      "source": [
        "# Encontrar los pares de tópicos más cercanos\n",
        "topic_pairs = np.dstack(np.unravel_index(np.argsort(similarities.ravel())[::-1], similarities.shape))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjgAyq0XN_Bo"
      },
      "outputs": [],
      "source": [
        "# Mostrar los 5 pares de tópicos más cercanos\n",
        "for i in range(10):\n",
        "    topic_id_1, topic_id_2 = topic_pairs[i]\n",
        "    similarity_score = similarities[topic_id_1, topic_id_2]\n",
        "    print(f\"Topico {topic_id_1} y Topico {topic_id_2} tienen una similitud de: {similarity_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpGQv22cN_Bo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZhB9tVkN_Bo"
      },
      "outputs": [],
      "source": [
        "topics_to_merge = [16, 17]\n",
        "\n",
        "id_docs_to_merge = []\n",
        "for topic in db_topics:\n",
        "    if topic['index'] in topics_to_merge:\n",
        "        id_docs_to_merge.append(topic['docs'].keys())\n",
        "\n",
        "list_id_docs_to_merge = [ item for sublist in id_docs_to_merge for item in sublist ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05h2Pyv4N_Bp"
      },
      "outputs": [],
      "source": [
        "index_name = 'news'\n",
        "\n",
        "# Construir el cuerpo de la solicitud para `mget`\n",
        "body = {\n",
        "    \"docs\": [{\"_index\": \"news\", \"_id\": int(doc_id)} for doc_id in list_id_docs_to_merge]\n",
        "}\n",
        "\n",
        "# Realizar la solicitud `mget`\n",
        "response = os_client.mget(body=body)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "329yTzpfN_Bp"
      },
      "outputs": [],
      "source": [
        "idx_relativo = []\n",
        "docs_input = []\n",
        "for i, doc in enumerate(response['docs']):\n",
        "    idx_relativo.append(doc['_id'])\n",
        "    docs_input.append(doc['_source']['news'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoPa57zsN_Bp"
      },
      "outputs": [],
      "source": [
        "len(docs_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzS8HA-dN_Bp"
      },
      "outputs": [],
      "source": [
        "topic_model.merge_topics(docs_input, topics_to_merge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAvvkpQ7N_Bp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqhB0DWtN_Bp"
      },
      "outputs": [],
      "source": [
        "def style_tags(tags):\n",
        "    styled_tags = ' | '.join([f' {tag} ' for tag in tags])\n",
        "    return styled_tags\n",
        "\n",
        "db_news = []\n",
        "for doc in News.search().query().scan():\n",
        "    index       = doc.meta.id\n",
        "    title       = doc.to_dict()['title']\n",
        "    author      = doc.to_dict()['author']\n",
        "    try:\n",
        "        keywords =  doc.to_dict()['keywords']\n",
        "    except:\n",
        "        keywords = [\"\"]\n",
        "    try:\n",
        "        entities =  doc.to_dict()['entities']\n",
        "    except:\n",
        "        entities = [\"\"]\n",
        "\n",
        "    created_at  = doc.to_dict()['created_at']\n",
        "    process     = doc.to_dict()['process']\n",
        "\n",
        "    db_news.append([index, title, style_tags(keywords), style_tags(entities), author, created_at, process])\n",
        "\n",
        "T = topic_model.get_document_info(data)\n",
        "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
        "\n",
        "# Obtener los IDs de los documentos y sus probabilidades\n",
        "docs_ids = []\n",
        "docs_topic = []\n",
        "doc_probs_x_topic = []\n",
        "for doc_ID in tqdm(docs_per_topics[topic]):\n",
        "    docs_ids.append(df_parquet.index[doc_ID])\n",
        "    doc_probs_x_topic.append(probs[doc_ID])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30jSXRhlN_Bp"
      },
      "outputs": [],
      "source": [
        "doc_probs_x_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0li9XxRN_Bp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWFya6flN_Bp"
      },
      "outputs": [],
      "source": [
        "db_news = get_news()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQhzUVdUN_Bq"
      },
      "outputs": [],
      "source": [
        "# Crear un diccionario para agrupar los registros por fecha (solo día, mes y año)\n",
        "fechas_dict = defaultdict(list)\n",
        "\n",
        "# Agrupar registros por fecha\n",
        "for registro in db_news:\n",
        "    fecha_completa = registro[-1]\n",
        "    fecha_solo_dia = fecha_completa.split('T')[0]  # Tomar solo el día, mes y año\n",
        "    if fecha_solo_dia not in fechas_dict:\n",
        "        fechas_dict[fecha_solo_dia] = 1\n",
        "    else:\n",
        "        fechas_dict[fecha_solo_dia] += 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kMPj_ujN_Bq",
        "outputId": "ce84a060-810c-4b30-fd1a-b9a34faab809"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list, {'2024-04-01': 1000})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fechas_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaQtwmsfN_Bq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}