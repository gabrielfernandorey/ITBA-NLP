{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielfernandorey/ITBA-NLP/blob/main/ITBA_nlp01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MODELO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "1e5d8d12-903f-4a10-ddd3-6d7d9ae83cc7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, date\n",
        "from dateutil.parser import parse\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from NLP_tools import Cleaning_text, top_keywords, top_entities, get_topic_name, best_document, clean_all, topic_documents\n",
        "from core.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from bertopic.vectorizers import ClassTfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from opensearch_data_model import Topic, TopicKeyword, News, os_client, TOPIC_INDEX_NAME, NEWS_INDEX_NAME\n",
        "from opensearch_io import init_opensearch, get_news\n",
        "from opensearchpy import helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializamos la base vectorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 17:51:46.665 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El índice Topic ya existe. Saltando inicialización de base de datos.\n",
            "El índice News ya existe. Saltando inicialización de base de datos.\n"
          ]
        }
      ],
      "source": [
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:/Users/gabri/OneDrive/Machine Learning/Github/ITBA-NLP/data/'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PATH == os.environ.get('PATH_LOCAL'):\n",
        "    client = OpenAI(api_key= os.environ.get('OPENAI_API_KEY'))\n",
        "else:\n",
        "    from google.colab import userdata\n",
        "    client = OpenAI(api_key= userdata.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Data de noticias original "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "cmp3cLLv28-T",
        "outputId": "2d83d8fc-9241-448a-98a1-6230eb29ce2a"
      },
      "outputs": [],
      "source": [
        "df_params = {'0_1000':'0_1000_data.parquet',\n",
        "             '1000_2000':'1000_2000_data.parquet',\n",
        "             '2000_3000':'2000_3000_data.parquet',\n",
        "             'df_joined':'df_joined_2024-04-01 00_00_00.parquet'\n",
        "            }\n",
        "\n",
        "chunk = os.environ.get('CHUNK')\n",
        "chunk = '0_1000'\n",
        "\n",
        "df_parquet = pd.read_parquet(PATH+df_params[chunk])\n",
        "data = list(df_parquet['in__text'])\n",
        "\n",
        "# Cantidad total de documentos\n",
        "print(chunk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar vocabulario\n",
        "with open(PATH+f'modelos/vocabulary_{chunk}.json', 'r') as json_file:\n",
        "    vocab = json.load(json_file)\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StopWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "SPANISH_STOPWORDS = list(pd.read_csv(PATH+'spanish_stop_words.csv' )['stopwords'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "        tokenizer=None,\n",
        "        max_df=0.9,\n",
        "        min_df=0.1,\n",
        "        ngram_range=(1, 2),\n",
        "        vocabulary=vocab,\n",
        "        # max_features=100_000\n",
        ")\n",
        "tfidf_vectorizer.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Capas del modelo BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpb75EAM3R53"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Extract embeddings\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "# Step 2 - Reduce dimensionality\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "# Step 3 - Cluster reduced embeddings\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "# Step 4 - Tokenize topics\n",
        "vectorizer_model = tfidf_vectorizer\n",
        "# Step 5 - Create topic representation\n",
        "ctfidf_model = ClassTfidfTransformer()\n",
        "# Step 6 - (Optional) Fine-tune topic representations with a `bertopic.representation` model\n",
        "representation_model = KeyBERTInspired()\n",
        "\n",
        "# All steps together\n",
        "topic_model = BERTopic(\n",
        "  embedding_model=embedding_model,              # Step 1 - Extract embeddings\n",
        "  umap_model=umap_model,                        # Step 2 - Reduce dimensionality\n",
        "  hdbscan_model=hdbscan_model,                  # Step 3 - Cluster reduced embeddings\n",
        "  vectorizer_model=vectorizer_model,            # Step 4 - Tokenize topics\n",
        "  ctfidf_model=ctfidf_model,                    # Step 5 - Extract topic words\n",
        "  # representation_model=representation_model,  # Step 6 - (Optional) Fine-tune topic represenations\n",
        "  # language='multilingual',                    # This is not used if embedding_model is used.\n",
        "  verbose=True,\n",
        "  # calculate_probabilities=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def my_callback(stage, **kwargs):\n",
        "    print(f\"Stage: {stage}\")\n",
        "    for key, value in kwargs.items():\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_callback(stage=\"start_training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar modelo entrenado o saltar celda y entrenar\n",
        "ahora = datetime.today()\n",
        "topic_model = BERTopic.load(PATH+f\"modelos/bertopic_model_{chunk}\")\n",
        "topics = np.load(PATH+f\"modelos/topics_{chunk}.npy\")\n",
        "probs = np.load(PATH+f\"modelos/probs_{chunk}.npy\")\n",
        "\n",
        "# Cargar los embeddings \n",
        "docs_embedding = np.load(PATH+f\"modelos/topic_embeddings_{chunk}.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics, probs = topic_model.fit_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos embeddings de todos los documentos\n",
        "docs_embedding = topic_model.embedding_model.embed(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar modelo\n",
        "topic_model.save(PATH+f\"modelos/bertopic_model_{chunk}\")\n",
        "np.save(PATH+f\"modelos/topics_{chunk}.npy\", topics)\n",
        "np.save(PATH+f\"modelos/probs_{chunk}.npy\", probs)\n",
        "\n",
        "# Guardar los embeddings en un archivo de NumPy\n",
        "np.save(PATH+f\"modelos/topic_embeddings_{chunk}.npy\", docs_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Cantidad de tópicos {len(set(topics))} (incluye topico -1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener documentos de cada tópico\n",
        "topic_freq = topic_model.get_topic_freq()\n",
        "\n",
        "# Imprimir el número de tópicos encontrados (incluyendo el tópico -1)\n",
        "num_topics = len(topic_freq) \n",
        "print(f\"Número de tópicos encontrados: {num_topics} (incluye el topico -1)\")\n",
        "\n",
        "# Imprimir la cant de documentos de cada tópico\n",
        "print(topic_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recuperar todos los topicos y sus etiquetas generadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.generate_topic_labels()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Buscar topicos ingresando un texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_res = topic_model.find_topics(\"cambio climatico\")\n",
        "topic_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Buscar los titulos de los primeros n documentos de un topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener los n documentos de un tópico \n",
        "topic_id = 10\n",
        "n_docs = 5\n",
        "topic_docs_idx = [i for i, (doc, topic) in enumerate(zip(list(df_parquet['in__title']), topics)) if topic == topic_id]\n",
        "n_docs = n_docs if n_docs <= len(topic_docs_idx) else len(topic_docs_idx)\n",
        "\n",
        "print(f\"{n_docs} de {len(topic_docs_idx)} titulos de noticias encontrados en el tópico {topic_id:}\")\n",
        "for idx in topic_docs_idx[:n_docs]:\n",
        "    print(\"- \",df_parquet.iloc[idx]['in__title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Busqueda de documentos por topico, ordenados por mayor probabilidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "T = topic_model.get_document_info(data)\n",
        "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
        "\n",
        "topic = 10\n",
        "# topic = np.random.randint(0, len(docs_per_topics)-1) # Aleatorio\n",
        "\n",
        "print(\"Ejemplo para tópico:\", topic)\n",
        "\n",
        "doc_probs_x_topic = []\n",
        "for doc in docs_per_topics[topic]:\n",
        "    doc_probs_x_topic.append([df_parquet.index[doc], df_parquet.iloc[doc].in__title, round(probs[doc],4)])\n",
        "\n",
        "df_query_1 = pd.DataFrame(doc_probs_x_topic)\n",
        "print(len(df_query_1), \"docs encontrados\")\n",
        "df_query_1.sort_values(2, ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Palabras clave del topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener las palabras clave para un topico dado\n",
        "topic_keywords = topic_model.get_topic(topic)\n",
        "print(\"Topico:\", topic)\n",
        "print(topic_keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criterio de corte (umbral)\n",
        "El criterio de corte utilizado para filtrar las noticias que pertenecen a un topico es el valor de -1 desvio std."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la media, el desvío estándar\n",
        "\n",
        "mean = np.mean([fila[2] for fila in doc_probs_x_topic])\n",
        "std_dev = np.std([fila[2] for fila in doc_probs_x_topic])\n",
        "\n",
        "# Crear el histograma \n",
        "plt.hist([fila[2] for fila in doc_probs_x_topic], bins=10, edgecolor='black')\n",
        "\n",
        "# Añadir líneas para la media, la moda y el desvío estándar\n",
        "plt.axvline(mean, color='r', linestyle='dashed', linewidth=1, label=f'Media: {mean:.2f}')\n",
        "plt.axvline(mean - std_dev, color='b', linestyle='dashed', linewidth=1, label=f'-1 STD: {mean - std_dev:.2f}')\n",
        "\n",
        "\n",
        "# Añadir títulos y etiquetas\n",
        "plt.title(f'Histograma de probabilidades topico: {topic}')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Documentos mas representativos de un topico "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [1]  Obtenido por el metodo del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs_representative = topic_model.get_representative_docs(topic=topic)\n",
        "docs_representative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [2] Obtenido por busqueda de probabilidad de documentos perteneciente al topico ( utilizando el umbral de corte )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir la función de estilo\n",
        "def color_rows(row, label, value):\n",
        "    if row[label] >= value:\n",
        "        return ['color: cyan'] * len(row)\n",
        "    else:\n",
        "        return [''] * len(row)\n",
        "    \n",
        "# Agrupamos documentos por topico\n",
        "T = topic_model.get_document_info(data)\n",
        "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
        "\n",
        "# Obtener los IDs de los documentos y sus probabilidades \n",
        "docs_ids = []\n",
        "docs_topic = []\n",
        "doc_probs_x_topic = []\n",
        "for doc_ID in tqdm(docs_per_topics[topic]):\n",
        "    docs_ids.append(df_parquet.index[doc_ID])\n",
        "    doc_probs_x_topic.append(probs[doc_ID])\n",
        "\n",
        "# Calcular la media, el desvío estándar\n",
        "mean = np.mean(doc_probs_x_topic)\n",
        "std_dev = np.std(doc_probs_x_topic)\n",
        "threshold = mean - std_dev\n",
        "\n",
        "# Crear una consulta de múltiples IDs\n",
        "index_name = 'news'\n",
        "mget_query = {\n",
        "    \"docs\": [{\"_index\": index_name, \"_id\": doc_id} for doc_id in docs_ids]\n",
        "}\n",
        "# Realizar la búsqueda de múltiples IDs\n",
        "response = os_client.mget(body=mget_query, index=index_name)\n",
        "\n",
        "# Procesar la respuesta\n",
        "for i, doc in enumerate(response['docs']):\n",
        "    if doc['found']:\n",
        "        idx = doc['_id']\n",
        "        title = df_parquet.iloc[docs_per_topics[topic][i]].in__title\n",
        "        prob_doc = probs[docs_per_topics[topic][i]]\n",
        "        \n",
        "        if 'entities' in doc['_source']:\n",
        "            ent = doc['_source']['entities']\n",
        "        else:\n",
        "            ent = []\n",
        "\n",
        "        docs_topic.append([idx, title, prob_doc, ent])\n",
        "\n",
        "\n",
        "df_view = pd.DataFrame(docs_topic, columns = ['indice','titulo','prob','entidades']).sort_values('prob', ascending=False)\n",
        "df_view.style.apply(lambda row: color_rows(row, 'prob', threshold), axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nota: Los documentos mas representativos encontrados utilizando el metodo \"get_representative_docs\" no refleja lo mismo que encontrando los documentos por probabilidades maximas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [3] Por similitud coseno del topico a los tres documento mas cercanos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos la matriz de similitud coseno entre topicos y documentos\n",
        "sim_matrix = cosine_similarity(topic_model.topic_embeddings_, docs_embedding)\n",
        "sim_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Similitud coseno entre el topico y los documentos del topico elegido\n",
        "s_coseno = []\n",
        "for i in docs_per_topics[topic]:\n",
        "    s_coseno.append(cosine_similarity([topic_model.topic_embeddings_[topic + 1]], [docs_embedding[i]])[0][0])\n",
        "\n",
        "# Indices\n",
        "idx_coseno_sort = np.argsort(s_coseno)[::-1]\n",
        "\n",
        "for idx in idx_coseno_sort[:3]:\n",
        "    print(idx, df_parquet.iloc[docs_per_topics[topic][idx]].in__title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nota: Del mismo modo que en el punto anterior, los documentos mas cercanos al topico no coinciden no son exactamente los mismos que los hallados en el punto 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [4] Primer documento mas cercano al embedding del topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Documento de maxima similitud con el topico\n",
        "\n",
        "simil_docs_topic = sim_matrix[topic + 1].argmax()\n",
        "print(f\"Noticia de maxima similitud con el topico: {topic}\")\n",
        "print(f\"Doc ID: {df_parquet.index[simil_docs_topic]}\")\n",
        "print(f\"Titulo: {df_parquet.iloc[simil_docs_topic].in__title}\")\n",
        "print(f\"Noticia: {data[simil_docs_topic][:80]}...\")\n",
        "best_doc = data[simil_docs_topic]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Keywords de solo un topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords = topic_model.topic_representations_[topic]\n",
        "topic_keywords = [TopicKeyword(name=keyword, score=score) for keyword, score in keywords if keyword != '']\n",
        "topic_keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculo de umbral de corte para las keywords\n",
        "freq_k = []\n",
        "for name_score in topic_keywords:\n",
        "    freq_k.append(name_score['score'])\n",
        "umbral_k = np.array(freq_k).mean()\n",
        "print(umbral_k)\n",
        "\n",
        "topic_keywords_top = {}\n",
        "for name_score in topic_keywords:\n",
        "    if name_score['score'] >= umbral_k:\n",
        "        topic_keywords_top[name_score['name']] = name_score['score']\n",
        "\n",
        "topic_keywords_top"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Entidades de un Topico a partir de los n documentos mas cercanos al embedding del topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entidades de documentos ordenados por similitud del topico elelgido\n",
        "n_docs = 5 # n docs cercanos\n",
        "entities_topic = []\n",
        "for doc in df_view[:n_docs].iterrows():\n",
        "    entities_topic.append(doc[1][3])\n",
        "\n",
        "from collections import defaultdict \n",
        "\n",
        "# Crear un diccionario para contar en cuántos documentos aparece cada palabra\n",
        "document_frequencies = defaultdict(int)\n",
        "\n",
        "# Crear un conjunto para cada documento y contar las palabras únicas\n",
        "for lista in entities_topic:\n",
        "    unique_words = set(lista)\n",
        "    for palabra in unique_words:\n",
        "        document_frequencies[palabra] += 1\n",
        "\n",
        "# Ordenar las palabras por la frecuencia de documentos de mayor a menor\n",
        "sorted_frequencies = sorted(document_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "freq_e = []\n",
        "for item in sorted_frequencies:\n",
        "    freq_e.append(item[1])\n",
        "umbral_e = np.array(freq_e).mean()\n",
        "\n",
        "# Imprimir el resultado ordenado de las primeras 10 entidades segun criterio de corte\n",
        "topic_entities_top = {}\n",
        "c=0\n",
        "for idx in range(len(sorted_frequencies)):\n",
        "    if sorted_frequencies[idx][1] >= umbral_e:\n",
        "        if c != 10:\n",
        "            topic_entities_top[sorted_frequencies[idx][0]] = sorted_frequencies[idx][1]\n",
        "        else:\n",
        "            break\n",
        "        c += 1\n",
        "\n",
        "topic_entities_top\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grabar todos los registros en Topic y actualizar en News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def topic_documents(topic, topic_model, probs, df_news):\n",
        "    \"\"\"\n",
        "    función que devuelve los ids de los documentos del tópico por encima del umbral, \n",
        "    los titulos de los documentos del tópico,\n",
        "    y el umbral de corte.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cantidad de documentos por topico\n",
        "        docs_per_topics = [i for i, x in enumerate(topic_model.topics_) if x == topic]\n",
        "\n",
        "        # Obtener los IDs de los documentos y sus probabilidades \n",
        "        docs_IDs = {}\n",
        "        doc_probs = []\n",
        "        for doc_idx in docs_per_topics:\n",
        "            \n",
        "            docs_IDs[df_news.indice[doc_idx]] = probs[doc_idx]\n",
        "            doc_probs.append(probs[doc_idx])\n",
        "        \n",
        "        # Calcular la media, el desvío estándar\n",
        "        mean = np.mean(doc_probs)\n",
        "        std_dev = np.std(doc_probs)\n",
        "        threshold = mean - std_dev\n",
        "\n",
        "        # Filtra los docs que superan o igualan al valor del umbral calculado\n",
        "        filter = {}\n",
        "        for k,v in docs_IDs.items():\n",
        "            if v >= threshold:\n",
        "                filter[k] = v\n",
        "        \n",
        "        # Ordeno de mayor a menor\n",
        "        ids_filter_sort = dict(sorted(filter.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "        title_filter_sort = [ df_news.loc[df_news['indice'] == idx].values[0][1] for idx in ids_filter_sort.keys() ]\n",
        "\n",
        "        return ids_filter_sort, title_filter_sort, threshold\n",
        "    except:\n",
        "        return {}, {}, 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_documents(topic, topic_model, probs, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs_per_topics = [i for i, x in enumerate(topic_model.topics_) if x == topic]\n",
        "print(docs_per_topics)\n",
        "\n",
        "# Obtener los IDs de los documentos y sus probabilidades \n",
        "docs_IDs = {}\n",
        "doc_probs = []\n",
        "for doc_idx in docs_per_topics:\n",
        "    \n",
        "    docs_IDs[df_parquet.indice[doc_idx]] = probs[doc_idx]\n",
        "    doc_probs.append(probs[doc_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_parquet.indice[doc_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Busqueda de todas las noticias no procesadas de la base ( en False ) (al menos 10.000)\n",
        "db_news = news_no_process()\n",
        "\n",
        "df_news = pd.DataFrame(db_news , columns=[\"indice\", \"titulo\", \"noticia\", \"keywords\", \"entidades\", \"creado\"])\n",
        "id_data    = list(df_news['indice'])\n",
        "title_data = list(df_news['titulo'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar entities\n",
        "with open(PATH+f'modelos/entities_{chunk}.json', 'r') as json_file:\n",
        "    entities = json.load(json_file)\n",
        "\n",
        "# Grabar todos los topicos en la base\n",
        "for topic in topic_model.get_topics().keys():\n",
        "    if topic > -1:\n",
        "\n",
        "        topic_keywords_top  = top_keywords(topic, topic_model)\n",
        "        topic_entities_top  = top_entities(topic, topic_model, docs_embedding, data, entities)\n",
        "        topic_documents_ids, topic_documents_title, threshold  = topic_documents(topic, topic_model, probs, df_parquet, data)\n",
        "        id_best_doc, title_best_doc, best_doc = best_document(topic, topic_model, docs_embedding, id_data, title_data, data)\n",
        "\n",
        "        topic_doc = Topic(\n",
        "            index = topic,   \n",
        "            name = get_topic_name(''.join({**topic_keywords_top, **topic_entities_top}), client),\n",
        "            vector = list(topic_model.topic_embeddings_[topic + 1 ]), \n",
        "            similarity_threshold = threshold,                      \n",
        "            created_at = datetime.now(),\n",
        "            to_date = parse('2024-04-02'),\n",
        "            from_date = parse('2024-04-01'),         \n",
        "            keywords = topic_keywords_top,\n",
        "            entities = topic_entities_top,\n",
        "            id_best_doc = id_best_doc,\n",
        "            title_best_doc = title_best_doc,\n",
        "            best_doc = best_doc,\n",
        "        ) \n",
        "\n",
        "        topic_doc.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Actualizar datos en News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Marcar registros de noticias procesados\n",
        "index_name = 'news'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            'process': False  \n",
        "        }\n",
        "    },\n",
        "    'size': 10000\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search( body=search_query, index=index_name )\n",
        "\n",
        "for i, reg in enumerate(response['hits']['hits']):\n",
        "    doc_id = reg['_id']\n",
        "    \n",
        "    update_body = {\n",
        "                    \"doc\": {\n",
        "                        \"vector\": ,\n",
        "                        \"process\": True\n",
        "                    }\n",
        "    }\n",
        "\n",
        "    # Realizar la actualización\n",
        "    os_client.update(index=index_name, id=doc_id, body=update_body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recuperar todos los topicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name = 'topic'\n",
        "\n",
        "db_topics = []\n",
        "for i, doc in enumerate(Topic.search().query().scan()):\n",
        "    db_topics.append(doc.to_dict())\n",
        "    print(db_topics[i]['index'], db_topics[i]['name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T = topic_model.find_topics(\"israel\")\n",
        "T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recuperar de la base el documento mas cercano a un topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name = 'topic'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            'index': 10  # Sustituir 'campo' y 'valor' por campo y valor de búsqueda\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search(\n",
        "                            body=search_query,\n",
        "                            index=index_name\n",
        ")\n",
        "\n",
        "texto = response['hits']['hits'][0]['_source']\n",
        "\n",
        "#Imprimir los resultados\n",
        "print(f\"Topico: {response['hits']['hits'][0]['_source']['name']}\")\n",
        "print(\"\\n\"+ texto['best_doc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ver_embedding = response['hits']['hits'][0]['_source']['vector']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.array(ver_embedding).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nuevo documento consultando embeddings generados por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_doc = \"Fuga de presos en San Telmo\" \n",
        "\n",
        "new_doc_embedding = topic_model.embedding_model.embed(new_doc)\n",
        "sim_matrix_new = cosine_similarity(topic_model.topic_embeddings_, new_doc_embedding.reshape(1, -1))\n",
        "\n",
        "idx = np.argmax(sim_matrix_new)-1         # Topicos desde -1, 0, 1, ..., n\n",
        "print(db_topics[idx]['index'], db_topics[idx]['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nuevo documento consultando embeddings de la base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_doc = \"Fuga de presos en San Telmo\" \n",
        "\n",
        "new_doc_embedding = topic_model.embedding_model.embed(new_doc)\n",
        "\n",
        "# Buscamos en la base a que topico pertenece el nuevo documento\n",
        "knn_query = {\n",
        "    \"size\": 1,\n",
        "    \"query\": {\n",
        "        \"knn\": {\n",
        "            \"vector\": {\n",
        "                \"vector\": new_doc_embedding,\n",
        "                \"k\" : 3\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "response = os_client.search(index='topic', body=knn_query)\n",
        "\n",
        "if response['hits']['total']['value'] > 0:\n",
        "    print(f\"Topico: {response['hits']['hits'][0]['_source']['name']}\")\n",
        "    print(f\"Estimacion: {response['hits']['hits'][0]['_score']}\")\n",
        "else:\n",
        "    print(f\"Topico no encontrado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graficar los topicos en 3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener los embeddings de los tópicos\n",
        "topic_embeddings = topic_model.topic_embeddings_\n",
        "\n",
        "new_doc = \"Fuga de presos en San Telmo\" \n",
        "new_doc_embedding = topic_model.embedding_model.embed([new_doc])[0]\n",
        "\n",
        "total = np.vstack((topic_embeddings, new_doc_embedding))\n",
        "\n",
        "# Reducir la dimensionalidad de los embeddings a 3D usando UMAP\n",
        "umap_model = UMAP(n_components=3)\n",
        "embeddings_3d = umap_model.fit_transform(total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Obtener las etiquetas de los tópicos\n",
        "topic_labels = topic_model.get_topic_info()['Topic'] \n",
        "\n",
        "# Crear un DataFrame para Plotly\n",
        "df = pd.DataFrame(embeddings_3d, columns=['Dim1', 'Dim2', 'Dim3'])\n",
        "df['Topico'] = list(range(len(topic_embeddings))) + ['Nuevo Doc'] # Identificamos el nuevo documento\n",
        "df['Etiqueta'] = list(topic_labels) + ['Nuevo Documento']\n",
        "\n",
        "# Graficar los tópicos en 3D de manera interactiva usando Plotly, ajustando el tamaño del gráfico\n",
        "fig = px.scatter_3d(df, x='Dim1', y='Dim2', z='Dim3', color='Topico', text='Etiqueta', title='Visualización 3D de Tópicos con BERTopic')\n",
        "\n",
        "\n",
        "# Ajustar el tamaño del gráfico\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    width=1200,  # Ancho del gráfico\n",
        "    height=800,  # Altura del gráfico\n",
        "    margin=dict(l=65, r=50, b=65, t=90)\n",
        ")\n",
        "\n",
        "# Mostrar las etiquetas en el gráfico\n",
        "fig.update_traces(marker=dict(size=5),\n",
        "                  selector=dict(mode='markers+text'))\n",
        "\n",
        "# Mostrar el gráfico\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(topic_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Busqueda de un documento por su indice, topico asociado, keywords, entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "index_name = 'news'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            '_id': '105640350'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search(\n",
        "                            body=search_query,\n",
        "                            index=index_name\n",
        ")\n",
        "print(f\"Texto de Noticia: {response['hits']['hits'][0]['_source']['news'][:200]}...\\n\")\n",
        "\n",
        "new_doc_embedding = topic_model.embedding_model.embed(response['hits']['hits'][0]['_source']['news'])\n",
        "\n",
        "# Define el índice y el campo del vector\n",
        "index_name = 'topic'\n",
        "vector_field = 'vector'\n",
        "\n",
        "# Crear una consulta KNN para buscar el embedding más similar\n",
        "knn_query = {\n",
        "    \"size\": 1,  # Número de resultados que deseas obtener, en este caso 1\n",
        "    \"query\": {\n",
        "        \"knn\": {\n",
        "            vector_field: {\n",
        "                \"vector\": new_doc_embedding,\n",
        "                \"k\": 3  # Número de vecinos más cercanos\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response_topic = os_client.search(index=index_name, body=knn_query)\n",
        "\n",
        "# Obtener el tópico más cercano\n",
        "if response_topic['hits']['total']['value'] > 0:\n",
        "    closest_topic = response_topic['hits']['hits'][0]['_source']\n",
        "    print(f\"El nuevo documento pertenece al tópico: {closest_topic['index']}\")\n",
        "    print(closest_topic['name'])\n",
        "    print(f\"Estimacion: {closest_topic['similarity_threshold']}\")\n",
        "    print(f\"Keywords del topico: {closest_topic['keywords']}\")\n",
        "    print(f\"Entidades del topico: {closest_topic['entities']}\")\n",
        "\n",
        "else:\n",
        "    print(\"No se encontró un tópico cercano.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agrupamiento de topicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name = 'topic'\n",
        "\n",
        "db_topics = []\n",
        "for i, doc in enumerate(Topic.search().query().scan()):\n",
        "    db_topics.append(doc.to_dict())\n",
        "    print(db_topics[i]['index'], db_topics[i]['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agrupando por embeddings cercanos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_embeddings = topic_model.topic_embeddings_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reducir la dimensionalidad a 3D usando UMAP\n",
        "umap_model = UMAP(n_neighbors=5, n_components=3, metric='cosine')\n",
        "umap_embeddings = umap_model.fit_transform(topic_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Crear un DataFrame para los embeddings y los IDs de los tópicos\n",
        "df = pd.DataFrame(umap_embeddings, columns=['UMAP Dimension 1', 'UMAP Dimension 2', 'UMAP Dimension 3'])\n",
        "df['Topic'] = topic_model.get_topic_info()['Topic'].values\n",
        "\n",
        "# Crear la gráfica 3D interactiva usando Plotly\n",
        "fig = px.scatter_3d(df, x='UMAP Dimension 1', y='UMAP Dimension 2', z='UMAP Dimension 3',\n",
        "                    text='Topic', title='Embeddings de los Tópicos Reducidos a 3D con UMAP',\n",
        "                    width=1000, height=800)  # Ajustar el tamaño del gráfico\n",
        "\n",
        "# Mostrar la gráfica\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eliminamos el topico -1\n",
        "new_topic_embeddings = topic_embeddings[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la similitud del coseno entre los embeddings de los tópicos\n",
        "similarities = cosine_similarity(new_topic_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear una matriz de similitud excluyendo la diagonal\n",
        "np.fill_diagonal(similarities, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encontrar los pares de tópicos más cercanos\n",
        "topic_pairs = np.dstack(np.unravel_index(np.argsort(similarities.ravel())[::-1], similarities.shape))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar los 5 pares de tópicos más cercanos\n",
        "for i in range(10):\n",
        "    topic_id_1, topic_id_2 = topic_pairs[i]\n",
        "    similarity_score = similarities[topic_id_1, topic_id_2]\n",
        "    print(f\"Topico {topic_id_1} y Topico {topic_id_2} tienen una similitud de: {similarity_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics_to_merge = [16, 17]\n",
        "\n",
        "id_docs_to_merge = []\n",
        "for topic in db_topics:\n",
        "    if topic['index'] in topics_to_merge:\n",
        "        id_docs_to_merge.append(topic['docs'].keys())\n",
        "\n",
        "list_id_docs_to_merge = [ item for sublist in id_docs_to_merge for item in sublist ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name = 'news'\n",
        "\n",
        "# Construir el cuerpo de la solicitud para `mget`\n",
        "body = {\n",
        "    \"docs\": [{\"_index\": \"news\", \"_id\": int(doc_id)} for doc_id in list_id_docs_to_merge]\n",
        "}\n",
        "\n",
        "# Realizar la solicitud `mget`\n",
        "response = os_client.mget(body=body)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx_relativo = []\n",
        "docs_input = []\n",
        "for i, doc in enumerate(response['docs']):\n",
        "    idx_relativo.append(doc['_id'])\n",
        "    docs_input.append(doc['_source']['news'])\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(docs_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.merge_topics(docs_input, topics_to_merge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def style_tags(tags):\n",
        "    styled_tags = ' | '.join([f' {tag} ' for tag in tags])\n",
        "    return styled_tags\n",
        "\n",
        "db_news = []\n",
        "for doc in News.search().query().scan():\n",
        "    index       = doc.meta.id\n",
        "    title       = doc.to_dict()['title']\n",
        "    author      = doc.to_dict()['author']\n",
        "    try:\n",
        "        keywords =  doc.to_dict()['keywords']\n",
        "    except:\n",
        "        keywords = [\"\"]\n",
        "    try:\n",
        "        entities =  doc.to_dict()['entities']\n",
        "    except:\n",
        "        entities = [\"\"]\n",
        "\n",
        "    created_at  = doc.to_dict()['created_at'] \n",
        "    process     = doc.to_dict()['process'] \n",
        "\n",
        "    db_news.append([index, title, style_tags(keywords), style_tags(entities), author, created_at, process])\n",
        "\n",
        "T = topic_model.get_document_info(data)\n",
        "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
        "\n",
        "# Obtener los IDs de los documentos y sus probabilidades \n",
        "docs_ids = []\n",
        "docs_topic = []\n",
        "doc_probs_x_topic = []\n",
        "for doc_ID in tqdm(docs_per_topics[topic]):\n",
        "    docs_ids.append(df_parquet.index[doc_ID])\n",
        "    doc_probs_x_topic.append(probs[doc_ID])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_probs_x_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_news = get_news()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un diccionario para agrupar los registros por fecha (solo día, mes y año)\n",
        "fechas_dict = defaultdict(list)\n",
        "\n",
        "# Agrupar registros por fecha\n",
        "for registro in db_news:\n",
        "    fecha_completa = registro[-1]\n",
        "    fecha_solo_dia = fecha_completa.split('T')[0]  # Tomar solo el día, mes y año\n",
        "    if fecha_solo_dia not in fechas_dict:\n",
        "        fechas_dict[fecha_solo_dia] = 1\n",
        "    else:\n",
        "        fechas_dict[fecha_solo_dia] += 1\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list, {'2024-04-01': 1000})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fechas_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGxTNO4yTgb9k2r4ffbTN0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
