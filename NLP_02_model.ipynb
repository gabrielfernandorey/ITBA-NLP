{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielfernandorey/ITBA-NLP/blob/main/ITBA_nlp01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MODELO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "1e5d8d12-903f-4a10-ddd3-6d7d9ae83cc7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, date\n",
        "from dateutil.parser import parse\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from NLP_tools import Cleaning_text, top_keywords, top_entities, get_topic_name, best_document, clean_all, topic_documents\n",
        "from core.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from bertopic.vectorizers import ClassTfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from opensearch_data_model import Topic, TopicKeyword, News, os_client, TOPIC_INDEX_NAME, NEWS_INDEX_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializamos la base vectorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-08 11:21:41.356 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El índice Topic ya existe. Saltando inicialización de base de datos.\n",
            "El índice News ya existe. Saltando inicialización de base de datos.\n"
          ]
        }
      ],
      "source": [
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:/Users/gabri/OneDrive/Machine Learning/Github/ITBA-NLP/data/'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PATH == os.environ.get('PATH_LOCAL'):\n",
        "    client = OpenAI(api_key= os.environ.get('OPENAI_API_KEY'))\n",
        "else:\n",
        "    from google.colab import userdata\n",
        "    client = OpenAI(api_key= userdata.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Data de noticias original "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "cmp3cLLv28-T",
        "outputId": "2d83d8fc-9241-448a-98a1-6230eb29ce2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3104"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_params = {'0_1000':'0_1000_data.parquet',\n",
        "             '1000_2000':'1000_2000_data.parquet',\n",
        "             '2000_3000':'2000_3000_data.parquet',\n",
        "             'df_joined':'df_joined_2024-04-01 00_00_00.parquet'\n",
        "            }\n",
        "\n",
        "chunk = os.environ.get('CHUNK')\n",
        "\n",
        "df_parquet = pd.read_parquet(PATH+df_params[chunk])\n",
        "data = list(df_parquet['in__text'])\n",
        "\n",
        "# Cantidad total de documentos\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar vocabulario\n",
        "with open(PATH+f'modelos/entities_vocabulary{chunk}.json', 'r') as json_file:\n",
        "    vocab_entities = json.load(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StopWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "SPANISH_STOPWORDS = list(pd.read_csv(PATH+'spanish_stop_words.csv' )['stopwords'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "        tokenizer=None,\n",
        "        max_df=0.9,\n",
        "        min_df=0.1,\n",
        "        ngram_range=(1, 2),\n",
        "        vocabulary=vocab_entities,\n",
        "        # max_features=100_000\n",
        ")\n",
        "#tfidf_vectorizer.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Capas del modelo BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mpb75EAM3R53"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Extract embeddings\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "# Step 2 - Reduce dimensionality\n",
        "umap_model = UMAP(n_neighbors=15, n_components=6, min_dist=0.0, metric='cosine', random_state=42)\n",
        "# Step 3 - Cluster reduced embeddings\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "# Step 4 - Tokenize topics\n",
        "vectorizer_model = tfidf_vectorizer\n",
        "# Step 5 - Create topic representation\n",
        "ctfidf_model = ClassTfidfTransformer()\n",
        "# Step 6 - (Optional) Fine-tune topic representations with a `bertopic.representation` model\n",
        "representation_model = KeyBERTInspired()\n",
        "\n",
        "# All steps together\n",
        "topic_model = BERTopic(\n",
        "  embedding_model=embedding_model,              # Step 1 - Extract embeddings\n",
        "  umap_model=umap_model,                        # Step 2 - Reduce dimensionality\n",
        "  hdbscan_model=hdbscan_model,                  # Step 3 - Cluster reduced embeddings\n",
        "  vectorizer_model=vectorizer_model,            # Step 4 - Tokenize topics\n",
        "  ctfidf_model=ctfidf_model,                    # Step 5 - Extract topic words\n",
        "  # representation_model=representation_model,  # Step 6 - (Optional) Fine-tune topic represenations\n",
        "  # language='multilingual',                    # This is not used if embedding_model is used.\n",
        "  verbose=True,\n",
        "  # calculate_probabilities=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def my_callback(stage, **kwargs):\n",
        "    print(f\"Stage: {stage}\")\n",
        "    for key, value in kwargs.items():\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_callback(stage=\"start_training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar modelo entrenado o saltar celda y entrenar\n",
        "ahora = datetime.today()\n",
        "topic_model = BERTopic.load(PATH+\"modelos/bertopic_model_app\")\n",
        "topics = np.load(PATH+\"modelos/topics_app.npy\")\n",
        "probs = np.load(PATH+\"modelos/probs_app.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-08 11:45:26,030 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ccda9d4509a4a07b75a69b5bf091fa0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/97 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-08 11:50:35,676 - BERTopic - Embedding - Completed ✓\n",
            "2024-07-08 11:50:35,678 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-07-08 11:50:59,757 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-07-08 11:50:59,757 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-07-08 11:50:59,913 - BERTopic - Cluster - Completed ✓\n",
            "2024-07-08 11:50:59,927 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-07-08 11:51:01,759 - BERTopic - Representation - Completed ✓\n"
          ]
        }
      ],
      "source": [
        "topics, probs = topic_model.fit_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-08 11:54:44,487 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
          ]
        }
      ],
      "source": [
        "# Grabar modelo\n",
        "topic_model.save(PATH+\"modelos/bertopic_model_app\")\n",
        "np.save(PATH+\"modelos/topics_app.npy\", topics)\n",
        "np.save(PATH+\"modelos/probs_app.npy\", probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de tópicos 76 (incluye topico -1)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Cantidad de tópicos {len(set(topics))} (incluye topico -1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplo para tópico: 70\n",
            "11 docs encontrados\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>105638712</td>\n",
              "      <td>Pedro Castillo: Magistrado del TC votó a favor de anular arresto y proceso penal por el golpe de Estado</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>105594298</td>\n",
              "      <td>Podemos recurre al TC para que el Supremo investigue su querella contra el juez García Castellón por supuesto 'lawfare'</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>105593952</td>\n",
              "      <td>Uno de los jueces que juzgará a Cristina por los Cuadernos de las Coimas le pide audiencia a Milei</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>105580987</td>\n",
              "      <td>Susana Medina, la jueza entrerriana que quiere llegar a la Corte Suprema</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>105593793</td>\n",
              "      <td>Un senador de La Libertad Avanza rechazó la postulación de Ariel Lijo a la Corte Suprema</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>105558658</td>\n",
              "      <td>Las empresas de EEUU juegan fuerte contra la postulación de Lijo para la Corte</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>105580339</td>\n",
              "      <td>Paoltroni se opone a la designación de Lijo porque salvó a Insfrán en una causa vinculada al caso Ciccone</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>105594425</td>\n",
              "      <td>La Corte Suprema acumula demandas de gobernadores por más de $600.000 millones y preocupa a Milei</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>105591618</td>\n",
              "      <td>Montenegro, sobre la candidatura de Lijo a la Corte: \"Está bien, es un tipo preparado\"</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>105595372</td>\n",
              "      <td>Javier Milei: Papas fritas y jamoncito</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>105579979</td>\n",
              "      <td>Yacobitti aseguró que el juez Lijo tendrá el consenso necesario para integrar la Corte Suprema</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0  \\\n",
              "0   105638712   \n",
              "1   105594298   \n",
              "2   105593952   \n",
              "3   105580987   \n",
              "4   105593793   \n",
              "5   105558658   \n",
              "6   105580339   \n",
              "7   105594425   \n",
              "8   105591618   \n",
              "9   105595372   \n",
              "10  105579979   \n",
              "\n",
              "                                                                                                                          1  \\\n",
              "0                   Pedro Castillo: Magistrado del TC votó a favor de anular arresto y proceso penal por el golpe de Estado   \n",
              "1   Podemos recurre al TC para que el Supremo investigue su querella contra el juez García Castellón por supuesto 'lawfare'   \n",
              "2                        Uno de los jueces que juzgará a Cristina por los Cuadernos de las Coimas le pide audiencia a Milei   \n",
              "3                                                  Susana Medina, la jueza entrerriana que quiere llegar a la Corte Suprema   \n",
              "4                                  Un senador de La Libertad Avanza rechazó la postulación de Ariel Lijo a la Corte Suprema   \n",
              "5                                            Las empresas de EEUU juegan fuerte contra la postulación de Lijo para la Corte   \n",
              "6                 Paoltroni se opone a la designación de Lijo porque salvó a Insfrán en una causa vinculada al caso Ciccone   \n",
              "7                         La Corte Suprema acumula demandas de gobernadores por más de $600.000 millones y preocupa a Milei   \n",
              "8                                    Montenegro, sobre la candidatura de Lijo a la Corte: \"Está bien, es un tipo preparado\"   \n",
              "9                                                                                    Javier Milei: Papas fritas y jamoncito   \n",
              "10                           Yacobitti aseguró que el juez Lijo tendrá el consenso necesario para integrar la Corte Suprema   \n",
              "\n",
              "      2  \n",
              "0   1.0  \n",
              "1   1.0  \n",
              "2   1.0  \n",
              "3   1.0  \n",
              "4   1.0  \n",
              "5   1.0  \n",
              "6   1.0  \n",
              "7   1.0  \n",
              "8   1.0  \n",
              "9   1.0  \n",
              "10  1.0  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Busqueda de documentos por topico, ordenados por mayor probabilidad\n",
        "\n",
        "T = topic_model.get_document_info(data)\n",
        "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
        "\n",
        "topic =70\n",
        "#topic = np.random.randint(0, len(docs_per_topics)-1)\n",
        "\n",
        "print(\"Ejemplo para tópico:\", topic)\n",
        "\n",
        "doc_probs_x_topic = []\n",
        "for doc in docs_per_topics[topic]:\n",
        "    doc_probs_x_topic.append([df_parquet.index[doc], df_parquet.iloc[doc].in__title, round(probs[doc],4)])\n",
        "\n",
        "df_query_1 = pd.DataFrame(doc_probs_x_topic)\n",
        "print(len(df_query_1), \"docs encontrados\")\n",
        "df_query_1.sort_values(2, ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Palabras clave del topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('corte', 0.029647818069535512), ('lijo', 0.02596069496028331), ('suprema', 0.0157537929430417), ('la', 0.014801340287951928), ('tribunal', 0.014216508490334208), ('jueces', 0.01398201336864679), ('que', 0.013643079164380269), ('el', 0.01348919625347898), ('juez', 0.012589202175765337), ('de', 0.012503295638684627)]\n"
          ]
        }
      ],
      "source": [
        "# Obtener las palabras clave para un tema específico (por ejemplo, el tema 0)\n",
        "topic_keywords = topic_model.get_topic(topic)\n",
        "print(topic_keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criterio de corte (umbral)\n",
        "El criterio de corte utilizado para filtrar las noticias que pertenecen a un topico es el valor de -1 desvio std."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la media, el desvío estándar\n",
        "mean = np.mean(doc_probs_x_topic)\n",
        "std_dev = np.std(doc_probs_x_topic)\n",
        "\n",
        "# Crear el histograma \n",
        "plt.hist(doc_probs_x_topic, bins=10, edgecolor='black')\n",
        "\n",
        "# Añadir líneas para la media, la moda y el desvío estándar\n",
        "plt.axvline(mean, color='r', linestyle='dashed', linewidth=1, label=f'Media: {mean:.2f}')\n",
        "plt.axvline(mean - std_dev, color='b', linestyle='dashed', linewidth=1, label=f'-1 STD: {mean - std_dev:.2f}')\n",
        "\n",
        "\n",
        "# Añadir títulos y etiquetas\n",
        "plt.title(f'Histograma de probabilidades topico: {topic}')\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Documentos mas representativos de un topico "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [1]  Obtenido por el metodo del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs_representative = topic_model.get_representative_docs(topic=topic)\n",
        "docs_representative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [2] Obtenido por busqueda de probabilidad de documentos perteneciente al topico ( utilizando el umbral de corte )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir la función de estilo\n",
        "def color_rows(row, label, value):\n",
        "    if row[label] >= value:\n",
        "        return ['color: cyan'] * len(row)\n",
        "    else:\n",
        "        return [''] * len(row)\n",
        "    \n",
        "# Agrupamos documentos por topico\n",
        "T = topic_model.get_document_info(data)\n",
        "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
        "\n",
        "# Obtener los IDs de los documentos y sus probabilidades \n",
        "docs_ids = []\n",
        "docs_topic = []\n",
        "doc_probs_x_topic = []\n",
        "for doc_ID in tqdm(docs_per_topics[topic]):\n",
        "    docs_ids.append(df_parquet.index[doc_ID])\n",
        "    doc_probs_x_topic.append(probs[doc_ID])\n",
        "\n",
        "# Calcular la media, el desvío estándar\n",
        "mean = np.mean(doc_probs_x_topic)\n",
        "std_dev = np.std(doc_probs_x_topic)\n",
        "threshold = mean - std_dev\n",
        "\n",
        "# Crear una consulta de múltiples IDs\n",
        "index_name = 'news'\n",
        "mget_query = {\n",
        "    \"docs\": [{\"_index\": index_name, \"_id\": doc_id} for doc_id in docs_ids]\n",
        "}\n",
        "# Realizar la búsqueda de múltiples IDs\n",
        "response = os_client.mget(body=mget_query, index=index_name)\n",
        "\n",
        "# Procesar la respuesta\n",
        "for i, doc in enumerate(response['docs']):\n",
        "    if doc['found']:\n",
        "        idx = doc['_id']\n",
        "        title = df_parquet.iloc[docs_per_topics[topic][i]].in__title\n",
        "        prob_doc = probs[docs_per_topics[topic][i]]\n",
        "        \n",
        "        if 'entities' in doc['_source']:\n",
        "            ent = doc['_source']['entities']\n",
        "        else:\n",
        "            ent = []\n",
        "\n",
        "        docs_topic.append([idx, title, prob_doc, ent])\n",
        "\n",
        "\n",
        "df_view = pd.DataFrame(docs_topic, columns = ['indice','titulo','prob','entidades']).sort_values('prob', ascending=False)\n",
        "df_view.style.apply(lambda row: color_rows(row, 'prob', threshold), axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nota: Los documentos mas representativos encontrados utilizando el metodo \"get_representative_docs\" no refleja lo mismo que encontrando los documentos por probabilidades maximas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [3] Por similitud coseno del topico a los tres documento mas cercanos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar los embeddings en un archivo de NumPy o procesar la celda inferior\n",
        "docs_embedding = np.load(PATH+f\"modelos/topic_embeddings_{chunk}.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos embeddings de todos los documentos\n",
        "docs_embedding = topic_model.embedding_model.embed(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar los embeddings en un archivo de NumPy\n",
        "np.save(PATH+f\"modelos/topic_embeddings_{chunk}.npy\", docs_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos la matriz de similitud coseno entre topicos y documentos\n",
        "sim_matrix = cosine_similarity(topic_model.topic_embeddings_, docs_embedding)\n",
        "sim_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Similitud coseno entre el topico y los documentos del topico elegido\n",
        "s_coseno = []\n",
        "for i in docs_per_topics[topic]:\n",
        "    s_coseno.append(cosine_similarity([topic_model.topic_embeddings_[topic + 1]], [docs_embedding[i]])[0][0])\n",
        "\n",
        "# Indices\n",
        "idx_coseno_sort = np.argsort(s_coseno)[::-1]\n",
        "\n",
        "for idx in idx_coseno_sort[:3]:\n",
        "    print(idx, df_parquet.iloc[docs_per_topics[topic][idx]].in__title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nota: Del mismo modo que en el punto anterior, los documentos mas cercanos al topico no coinciden no son exactamente los mismos que los hallados en el punto 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [4] Primer documento mas cercano al embedding del topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Documento de maxima similitud con el topico\n",
        "\n",
        "simil_docs_topic = sim_matrix[topic + 1].argmax()\n",
        "print(f\"Noticia de maxima similitud con el topico: {topic}\")\n",
        "print(f\"Doc ID: {df_parquet.index[simil_docs_topic]}\")\n",
        "print(f\"Titulo: {df_parquet.iloc[simil_docs_topic].in__title}\")\n",
        "print(f\"Noticia: {data[simil_docs_topic][:80]}...\")\n",
        "best_doc = data[simil_docs_topic]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"\"Ojo, con la selección del mas cercano respecto a las probabilidades de los docs del topico\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Keywords de solo un topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords = topic_model.topic_representations_[topic]\n",
        "topic_keywords = [TopicKeyword(name=keyword, score=score) for keyword, score in keywords if keyword != '']\n",
        "topic_keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Nota: el umbral de corte para las keywords es la media, se tomaran hasta 10 keywords mientras no supere el umbral de corte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_k = []\n",
        "for name_score in topic_keywords:\n",
        "    freq_k.append(name_score['score'])\n",
        "umbral_k = np.array(freq_k).mean()\n",
        "umbral_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_keywords_top = {}\n",
        "for name_score in topic_keywords:\n",
        "    if name_score['score'] >= umbral_k:\n",
        "        topic_keywords_top[name_score['name']] = name_score['score']\n",
        "\n",
        "topic_keywords_top\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Entidades de un Topico a partir de los n documentos mas cercanos al embedding del topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entidades de documentos ordenados por similitud del topico elelgido\n",
        "n_docs = 5 # n docs cercanos\n",
        "entities_topic = []\n",
        "for doc in df_view[:n_docs].iterrows():\n",
        "    entities_topic.append(doc[1][3])\n",
        "\n",
        "from collections import defaultdict \n",
        "\n",
        "# Crear un diccionario para contar en cuántos documentos aparece cada palabra\n",
        "document_frequencies = defaultdict(int)\n",
        "\n",
        "# Crear un conjunto para cada documento y contar las palabras únicas\n",
        "for lista in entities_topic:\n",
        "    unique_words = set(lista)\n",
        "    for palabra in unique_words:\n",
        "        document_frequencies[palabra] += 1\n",
        "\n",
        "# Ordenar las palabras por la frecuencia de documentos de mayor a menor\n",
        "sorted_frequencies = sorted(document_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "freq_e = []\n",
        "for item in sorted_frequencies:\n",
        "    freq_e.append(item[1])\n",
        "umbral_e = np.array(freq_e).mean()\n",
        "\n",
        "# Imprimir el resultado ordenado de las primeras 10 entidades segun criterio de corte\n",
        "topic_entities_top = {}\n",
        "c=0\n",
        "for idx in range(len(sorted_frequencies)):\n",
        "    if sorted_frequencies[idx][1] >= umbral_e:\n",
        "        if c != 10:\n",
        "            topic_entities_top[sorted_frequencies[idx][0]] = sorted_frequencies[idx][1]\n",
        "        else:\n",
        "            break\n",
        "        c += 1\n",
        "\n",
        "topic_entities_top\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grabar todos los registros en Topic y actualizar en News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar o saltar carga y procesar celda inferior\n",
        "with open(PATH+f'modelos/entities{chunk}.json', 'r') as json_file:\n",
        "    entities = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar todos los topicos en la base\n",
        "for topic in topic_model.get_topics().keys():\n",
        "    if topic > -1:\n",
        "\n",
        "        topic_keywords_top  = top_keywords(topic, topic_model)\n",
        "        topic_entities_top  = top_entities(topic, topic_model, docs_embedding, data, entities)\n",
        "        topic_documents_ids, threshold  = topic_documents(topic, topic_model, probs, df_parquet, data)\n",
        "\n",
        "        topic_doc = Topic(\n",
        "            index = topic,   \n",
        "            name = get_topic_name(''.join({**topic_keywords_top, **topic_entities_top}), client),\n",
        "            vector = list(topic_model.topic_embeddings_[topic + 1 ]), \n",
        "            similarity_threshold = threshold,                      \n",
        "            created_at = datetime.now(),\n",
        "            to_date = parse('2024-04-02'),\n",
        "            from_date = parse('2024-04-01'),         \n",
        "            keywords = topic_keywords_top,\n",
        "            entities = topic_entities_top,\n",
        "            best_doc = best_document(topic, topic_model, docs_embedding, data),\n",
        "            docs = topic_documents_ids\n",
        "        ) \n",
        "\n",
        "        topic_doc.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Actualizar datos en News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Marcar registros de noticias procesados\n",
        "index_name = 'news'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            'process': False  \n",
        "        }\n",
        "    },\n",
        "    'size': 10000\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search( body=search_query, index=index_name )\n",
        "\n",
        "for i, reg in enumerate(response['hits']['hits']):\n",
        "    doc_id = reg['_id']\n",
        "    \n",
        "    update_body = {\n",
        "                    \"doc\": {\n",
        "                        \n",
        "                        \"process\": True\n",
        "                    }\n",
        "    }\n",
        "\n",
        "    # Realizar la actualización\n",
        "    os_client.update(index=index_name, id=doc_id, body=update_body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recuperar todos los topicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name = 'topic'\n",
        "\n",
        "db_topics = []\n",
        "for i, doc in enumerate(Topic.search().query().scan()):\n",
        "    db_topics.append(doc.to_dict())\n",
        "    print(db_topics[i]['index'], db_topics[i]['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recuperar documento mas cercano a un topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name = 'topic'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            'index': 6  # Sustituir 'campo' y 'valor' por campo y valor de búsqueda\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search(\n",
        "                            body=search_query,\n",
        "                            index=index_name\n",
        ")\n",
        "\n",
        "texto = response['hits']['hits'][0]['_source']\n",
        "\n",
        "#Imprimir los resultados\n",
        "print(f\"Topico: {response['hits']['hits'][0]['_source']['name']}\")\n",
        "print(\"\\n\"+ texto['best_doc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nuevo documento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_doc = \"Bitcoin esta en alza\"\n",
        "new_doc = response['hits']['hits'][0]['_source']['news']\n",
        "new_doc_embedding = topic_model.embedding_model.embed(new_doc)\n",
        "\n",
        "# Buscamos en la base a que topico pertenece el nuevo documento\n",
        "query = {\n",
        "    \"size\": 1,\n",
        "    \"query\": {\n",
        "        \"knn\": {\n",
        "            \"vector\": {\n",
        "                \"vector\": list(new_doc_embedding),\n",
        "                \"k\" : 3\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "response = os_client.search(index='topic', body=query)\n",
        "\n",
        "print(f\"Topico: {response['hits']['hits'][0]['_source']['name']}\")\n",
        "print(f\"Estimacion: {response['hits']['hits'][0]['_source']['similarity_threshold']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Busqueda de un documento por su indice, topico asociado, keywords, entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "index_name = 'news'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            '_id': '105640350'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search(\n",
        "                            body=search_query,\n",
        "                            index=index_name\n",
        ")\n",
        "print(f\"Texto de Noticia: {response['hits']['hits'][0]['_source']['news'][:200]}...\\n\")\n",
        "\n",
        "new_doc_embedding = topic_model.embedding_model.embed(response['hits']['hits'][0]['_source']['news'])\n",
        "\n",
        "# Define el índice y el campo del vector\n",
        "index_name = 'topic'\n",
        "vector_field = 'vector'\n",
        "\n",
        "# Crear una consulta KNN para buscar el embedding más similar\n",
        "knn_query = {\n",
        "    \"size\": 1,  # Número de resultados que deseas obtener, en este caso 1\n",
        "    \"query\": {\n",
        "        \"knn\": {\n",
        "            vector_field: {\n",
        "                \"vector\": new_doc_embedding,\n",
        "                \"k\": 3  # Número de vecinos más cercanos\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response_topic = os_client.search(index=index_name, body=knn_query)\n",
        "\n",
        "# Obtener el tópico más cercano\n",
        "if response_topic['hits']['total']['value'] > 0:\n",
        "    closest_topic = response_topic['hits']['hits'][0]['_source']\n",
        "    print(f\"El nuevo documento pertenece al tópico: {closest_topic['index']}\")\n",
        "    print(closest_topic['name'])\n",
        "    print(f\"Estimacion: {closest_topic['similarity_threshold']}\")\n",
        "    print(f\"Keywords del topico: {closest_topic['keywords']}\")\n",
        "    print(f\"Entidades del topico: {closest_topic['entities']}\")\n",
        "\n",
        "else:\n",
        "    print(\"No se encontró un tópico cercano.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agrupamiento de topicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name = 'topic'\n",
        "\n",
        "db_topics = []\n",
        "for i, doc in enumerate(Topic.search().query().scan()):\n",
        "    db_topics.append(doc.to_dict())\n",
        "    print(db_topics[i]['index'], db_topics[i]['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Agrupando por embeddings cercanos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_embeddings = topic_model.topic_embeddings_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reducir la dimensionalidad a 3D usando UMAP\n",
        "umap_model = UMAP(n_neighbors=5, n_components=3, metric='cosine')\n",
        "umap_embeddings = umap_model.fit_transform(topic_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Crear un DataFrame para los embeddings y los IDs de los tópicos\n",
        "df = pd.DataFrame(umap_embeddings, columns=['UMAP Dimension 1', 'UMAP Dimension 2', 'UMAP Dimension 3'])\n",
        "df['Topic'] = topic_model.get_topic_info()['Topic'].values\n",
        "\n",
        "# Crear la gráfica 3D interactiva usando Plotly\n",
        "fig = px.scatter_3d(df, x='UMAP Dimension 1', y='UMAP Dimension 2', z='UMAP Dimension 3',\n",
        "                    text='Topic', title='Embeddings de los Tópicos Reducidos a 3D con UMAP',\n",
        "                    width=1000, height=800)  # Ajustar el tamaño del gráfico\n",
        "\n",
        "# Mostrar la gráfica\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eliminamos el topico -1\n",
        "new_topic_embeddings = topic_embeddings[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la similitud del coseno entre los embeddings de los tópicos\n",
        "similarities = cosine_similarity(new_topic_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear una matriz de similitud excluyendo la diagonal\n",
        "np.fill_diagonal(similarities, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encontrar los pares de tópicos más cercanos\n",
        "topic_pairs = np.dstack(np.unravel_index(np.argsort(similarities.ravel())[::-1], similarities.shape))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar los 5 pares de tópicos más cercanos\n",
        "for i in range(10):\n",
        "    topic_id_1, topic_id_2 = topic_pairs[i]\n",
        "    similarity_score = similarities[topic_id_1, topic_id_2]\n",
        "    print(f\"Topico {topic_id_1} y Topico {topic_id_2} tienen una similitud de: {similarity_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics_to_merge = [16, 17]\n",
        "\n",
        "id_docs_to_merge = []\n",
        "for topic in db_topics:\n",
        "    if topic['index'] in topics_to_merge:\n",
        "        id_docs_to_merge.append(topic['docs'].keys())\n",
        "\n",
        "list_id_docs_to_merge = [ item for sublist in id_docs_to_merge for item in sublist ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name = 'news'\n",
        "\n",
        "# Construir el cuerpo de la solicitud para `mget`\n",
        "body = {\n",
        "    \"docs\": [{\"_index\": \"news\", \"_id\": int(doc_id)} for doc_id in list_id_docs_to_merge]\n",
        "}\n",
        "\n",
        "# Realizar la solicitud `mget`\n",
        "response = os_client.mget(body=body)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx_relativo = []\n",
        "docs_input = []\n",
        "for i, doc in enumerate(response['docs']):\n",
        "    idx_relativo.append(doc['_id'])\n",
        "    docs_input.append(doc['_source']['news'])\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(docs_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.merge_topics(docs_input, topics_to_merge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar la busqueda de todas las noticias no procesadas ( False ) en la base (al menos 10.000)\n",
        "index_name = 'news'\n",
        "search_query = {\n",
        "    'query': {\n",
        "        'match': {\n",
        "            'process': True\n",
        "        }\n",
        "    },\n",
        "    'size': 10000\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda\n",
        "response = os_client.search( body=search_query, index=index_name )\n",
        "\n",
        "db_news = []\n",
        "for reg in response['hits']['hits']:\n",
        "    _id =  reg['_id']\n",
        "    title =  reg['_source']['title']\n",
        "    news =  reg['_source']['news']\n",
        "    try:\n",
        "        keywords =  reg['_source']['keywords'] \n",
        "    except:\n",
        "        keywords = ['']\n",
        "    try:\n",
        "        entities =  reg['_source']['entities'] \n",
        "    except:\n",
        "        entities = ['']\n",
        "    created_at =  reg['_source']['created_at'] \n",
        "    \n",
        "    db_news.append([_id, title, news, keywords, entities, created_at])\n",
        "\n",
        "df_news = pd.DataFrame(db_news , columns=[\"indice\", \"titulo\", \"noticia\", \"keywords\", \"entidades\", \"creado\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T = topic_model.get_document_info(data)\n",
        "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
        "\n",
        "# Obtener los IDs de los documentos y sus probabilidades \n",
        "docs_IDs = {}\n",
        "doc_probs_x_topic = []\n",
        "for doc_idx in docs_per_topics[topic]:\n",
        "    \n",
        "    docs_IDs[df_news.indice[doc_idx]] = probs[doc_idx]\n",
        "    doc_probs_x_topic.append(probs[doc_idx])\n",
        "\n",
        "# Calcular la media, el desvío estándar\n",
        "mean = np.mean(doc_probs_x_topic)\n",
        "std_dev = np.std(doc_probs_x_topic)\n",
        "threshold = mean - std_dev\n",
        "\n",
        "# Filtra los docs que superan o igualan al valor del umbral calculado\n",
        "filter = {}\n",
        "for k,v in docs_IDs.items():\n",
        "    if v >= threshold:\n",
        "        filter[k] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ordeno de mayor a menor\n",
        "ids_filter_sort = dict(sorted(filter.items(), key=lambda item: item[1], reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids_filter_sort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "title_filter_sort = [ df_news.loc[df_news['indice'] == idx].values[0][1] for idx in ids_filter_sort.keys() ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "title_filter_sort "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "' '.join(title_filter_sort)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "x_ids, x_titles, umbral = topic_documents(topic, topic_model, probs, df_news, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGxTNO4yTgb9k2r4ffbTN0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
