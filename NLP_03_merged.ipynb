{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielfernandorey/ITBA-NLP/blob/main/ITBA_nlp01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merged models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "1e5d8d12-903f-4a10-ddd3-6d7d9ae83cc7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, date\n",
        "from dateutil.parser import parse\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from NLP_tools import Cleaning_text, top_keywords, top_entities, get_topic_name, best_document, clean_all, topic_documents\n",
        "from core.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from bertopic.vectorizers import ClassTfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from opensearch_data_model import Topic, TopicKeyword, News, os_client, TOPIC_INDEX_NAME, NEWS_INDEX_NAME\n",
        "from opensearch_io import init_opensearch, get_news\n",
        "from opensearchpy import helpers\n",
        "\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializamos la base vectorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PATH == os.environ.get('PATH_LOCAL'):\n",
        "    client = OpenAI(api_key= os.environ.get('OPENAI_API_KEY'))\n",
        "else:\n",
        "    from google.colab import userdata\n",
        "    client = OpenAI(api_key= userdata.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cargamos noticias de dos chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the parquet file | ( lotes de prueba )\n",
        "\n",
        "df_params = {'0_1000':'0_1000_data.parquet',\n",
        "             '1000_2000':'1000_2000_data.parquet',\n",
        "             '2000_3000':'2000_3000_data.parquet',\n",
        "             'df_joined':'df_joined_2024-04-01 00_00_00.parquet'\n",
        "            }\n",
        "\n",
        "chunk_1 = '0_1000'\n",
        "df_parquet_1 = pd.read_parquet(PATH+df_params[chunk_1])\n",
        "data_1 = list(df_parquet_1['in__text'])\n",
        "\n",
        "chunk_2 = '1000_2000'\n",
        "df_parquet_2 = pd.read_parquet(PATH+df_params[chunk_2])\n",
        "data_2 = list(df_parquet_2['in__text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unificamos datos\n",
        "df_parquet = pd.concat([df_parquet_1, df_parquet_2], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Modelo 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar modelo 1 \n",
        "\n",
        "topic_model_1 = BERTopic.load(PATH+f\"modelos/bertopic_model_{chunk_1}\")\n",
        "#topics_1 = np.load(PATH+f\"modelos/topics_{chunk}.npy\")\n",
        "#probs_1 = np.load(PATH+f\"modelos/probs_{chunk}.npy\")\n",
        "\n",
        "# Cargar los embeddings \n",
        "docs_embedding_1 = np.load(PATH+f\"modelos/docs_embeddings_{chunk_1}.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar modelo 2\n",
        "chunk = '1000_2000'\n",
        "\n",
        "topic_model_2 = BERTopic.load(PATH+f\"modelos/bertopic_model_{chunk_2}\")\n",
        "#topics_2 = np.load(PATH+f\"modelos/topics_{chunk}.npy\")\n",
        "#probs_2 = np.load(PATH+f\"modelos/probs_{chunk}.npy\")\n",
        "\n",
        "# Cargar los embeddings \n",
        "docs_embedding_2 = np.load(PATH+f\"modelos/docs_embeddings_{chunk_2}.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all models into one\n",
        "merged_model = BERTopic.merge_models([topic_model_1, topic_model_2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Cantidad de tópicos modelo 1: {len(topic_model_1.get_topic_info())} -- del 0 al {len(topic_model_1.get_topic_info())-1} (incluye topico -1)\")\n",
        "print(f\"Cantidad de tópicos modelo 2: {len(topic_model_2.get_topic_info())} -- del 0 al {len(topic_model_2.get_topic_info())-1} (incluye topico -1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Cantidad de tópicos modelo merge: {len(merged_model.get_topic_info())} -- del 0 al {len(merged_model.get_topic_info())-1} (incluye topico -1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Topicos de ambos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topicos modelo 1\n",
        "topic_model_1.topic_labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topicos modelo 2\n",
        "topic_model_2.topic_labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nota: Cada modelo puede representar ordenes distintos de los topicos, pero al fusionarlos, el orden del modelo 1 se mantiene en el fusionado y se agregan los nuevos al final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topicos modelo merged\n",
        "merged_model.topic_labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparamos modelos ordenados por topicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_freq_1 = topic_model_1.get_topic_freq()\n",
        "topic_freq_m = merged_model.get_topic_freq()\n",
        "\n",
        "df1 = topic_freq_1.sort_values(by='Topic').reset_index(drop=True)\n",
        "dfm = topic_freq_m.sort_values(by='Topic').reset_index(drop=True)\n",
        "\n",
        "# Renombrar las columnas 'Count' para diferenciar DataFrames\n",
        "df1 = df1.rename(columns={'Count': 'Count1'})\n",
        "dfm = dfm.rename(columns={'Count': 'Merged'})\n",
        "\n",
        "df_combined = pd.merge(df1, dfm, on='Topic', how='outer')\n",
        "\n",
        "# Calcular la nueva columna 'Count2' como la resta de 'Merged' y 'Count1'\n",
        "# Asegurarse de manejar NaN correctamente\n",
        "df_combined['Count2'] = df_combined['Merged'].fillna(0) - df_combined['Count1'].fillna(0)\n",
        "\n",
        "# Reordenar las columnas en el orden deseado\n",
        "df_combined = df_combined[['Topic', 'Count1', 'Count2', 'Merged']]\n",
        "df_combined\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nota: Podemos verificar que topicos se agregaron y cuales de los existentes incorporaron nuevos docs o no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_news = get_news()\n",
        "\n",
        "# Crear un diccionario para agrupar los registros por fecha (solo día, mes y año)\n",
        "fechas_dict = defaultdict(list)\n",
        "\n",
        "# Agrupar registros por fecha\n",
        "for registro in db_news:\n",
        "    fecha_completa = registro[-1]\n",
        "    fecha_solo_dia = fecha_completa.split('T')[0]  # Tomar solo el día, mes y año\n",
        "    if fecha_solo_dia not in fechas_dict:\n",
        "        fechas_dict[fecha_solo_dia] = 1\n",
        "    else:\n",
        "        fechas_dict[fecha_solo_dia] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from_date  = [ '2024-04-03' for _ in range(len(topic_model_1.get_topics().keys())) ] \n",
        "to_date = [ datetime.strptime('2024-04-03', '%Y-%m-%d') + timedelta(days=1) for _ in range(len(from_date)) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analisis de documentos fusionados en un topico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# modelo 1\n",
        "topic = 12\n",
        "docs_per_topics_1 = [i for i, x in enumerate(topic_model_1.topics_) if x == topic]\n",
        "probs_1 = topic_model_1.probabilities_[docs_per_topics_1]\n",
        "A = np.array(docs_per_topics_1).reshape(-1,1)\n",
        "B = np.array([ df_parquet_1.iloc[row][\"in__title\"] for row in docs_per_topics_1 ]).reshape(-1,1)\n",
        "C = np.array(probs_1).reshape(-1,1)\n",
        "combined_array_1 = np.hstack((A, B, C))\n",
        "pd.DataFrame(combined_array_1, columns=['ID doc', 'title', 'probs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# modelo fusionado\n",
        "topic = 12\n",
        "docs_per_topics_m = [i for i, x in enumerate(merged_model.topics_) if x == topic]\n",
        "A = np.array(docs_per_topics_m).reshape(-1,1)\n",
        "B = np.array([ df_parquet.iloc[row][\"in__title\"] for row in docs_per_topics_m ]).reshape(-1,1)\n",
        "combined_array_m = np.hstack((A, B))\n",
        "pd.DataFrame(combined_array_m, columns=['ID doc', 'title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# modelo 2\n",
        "topic = 12\n",
        "docs_per_topics_2 = [i for i, x in enumerate(topic_model_2.topics_) if x == topic]\n",
        "probs_2 = topic_model_1.probabilities_[docs_per_topics_2]\n",
        "docs_per_topics_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_parquet_2.iloc[58][\"in__title\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_parquet.iloc[1058][\"in__title\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recuperar todos los topicos y sus etiquetas generadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# docs de 0-1000 primer chunk\n",
        "# docs de 1000-2000 segundo chunk\n",
        "docs = [i for i, x in enumerate(merged_model.topics_) if x == 10]\n",
        "for i in docs:\n",
        "    print(i, df_parquet.iloc[i]['in__title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Buscar topicos ingresando un texto en el modelo merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_res = merged_model.find_topics(\"cambio climatico\")\n",
        "topic_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verificacion de documentos con topico -1, recuperados en topicos positivos luego de fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = [i for i, x in enumerate(topic_model_1.topics_) if x == -1]\n",
        "print(f\"Cantidad de docs sin topico: {len(docs)} \\n\")\n",
        "a = []\n",
        "for i in docs:\n",
        "    a.append(i)\n",
        "    print(i, df_parquet_1.iloc[i]['in__title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = [i for i, x in enumerate(merged_model.topics_) if x == -1]\n",
        "m = []\n",
        "print(f\"Cantidad de docs sin topico: {len(docs)} \\n\")\n",
        "for i in docs:\n",
        "    m.append(i)\n",
        "    print(i, df_parquet.iloc[i]['in__title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = np.array(a).reshape(-1,1)\n",
        "m = np.array(m).reshape(-1,1)\n",
        "\n",
        "for i in range(len(a)):\n",
        "    if a[i] not in m:\n",
        "        print(a[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = [i for i, x in enumerate(topic_model_1.topics_) if x == -1]\n",
        "print(f\"Cantidad de docs sin topico: {len(docs)} \\n\")\n",
        "a = []\n",
        "for i in docs:\n",
        "    a.append(i)\n",
        "    print(i, df_parquet_1.iloc[i]['in__title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(topic_model_1.probabilities_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics_1 = topic_model_1.get_topics().keys()\n",
        "topics_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics_m = merged_model.get_topics().keys()\n",
        "topics_m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modificar news "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name='topic'\n",
        "query = {   \"size\": 1000,\n",
        "            \"query\": {\n",
        "                \"match_all\": {}\n",
        "            }\n",
        "        }\n",
        "\n",
        "response = os_client.search(index=index_name, body=query)\n",
        "topics = [ (hit['_source']['index'], hit['_source']['from_date'], hit['_source']['to_date'])  for hit in response['hits']['hits']]\n",
        "topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtenemos los topicos del indice topic de Opensearch filtrados por fecha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener los tópicos filtrados por la fecha seleccionada\n",
        "date_str = '2024-04-02'\n",
        "topics = get_topics_opensearch(date_filter=date_str)\n",
        "sorted([  (row['index'], row['name']) for row in topics if row['to_date'][:10] <= date_str  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_topics = []\n",
        "data_topics = {} \n",
        "for reg in topics:\n",
        "    index = reg['index']\n",
        "    name = reg['name']\n",
        "    similarity_threshold = reg['similarity_threshold']\n",
        "    create_at = format_date(reg['created_at'])\n",
        "    from_date = reg['from_date'][:10]\n",
        "    to_date = reg['to_date'][:10]\n",
        "    title_best_doc = reg['title_best_doc']\n",
        "    id_best_doc = reg['id_best_doc']\n",
        "\n",
        "    db_topics.append([index, name, round(similarity_threshold, 4), create_at, from_date, to_date, title_best_doc, id_best_doc])\n",
        "    data_topics[index] = [name,\n",
        "                        title_best_doc,\n",
        "                        reg['best_doc'],\n",
        "                        reg['entities'],\n",
        "                        reg['keywords'],\n",
        "                        similarity_threshold,\n",
        "                        reg['vector']\n",
        "                        ] \n",
        "\n",
        "db_topics = sorted(db_topics)\n",
        "# Convertir a DataFrame\n",
        "topics_df = pd.DataFrame(db_topics, columns=[\"indice\", \"nombre\", \"umbral\", \"creado\", \"desde\", \"hasta\", \"titulo noticia mas cercana\", \"id noticia\"]) \n",
        "topics_df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Buscar en news opensearch las 20 noticias mas cercanas a un topico dado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic = 0\n",
        "query = {\n",
        "        \"size\": 1000,\n",
        "        \"query\": {\n",
        "            \"bool\": {\n",
        "                \"must\": [\n",
        "                    {   \"term\": {\n",
        "                            \"index\": topic\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "                    \n",
        "response = os_client.search(index='topic', body=query)\n",
        "\n",
        "name = [hit['_source']['name'] for hit in response['hits']['hits']]\n",
        "embedding = [hit['_source']['vector'] for hit in response['hits']['hits']]\n",
        "\n",
        "# Construir la consulta de OpenSearch para búsqueda por vector\n",
        "query = {\n",
        "    \"size\": 20,  # Número de resultados \n",
        "    \"_source\": [\"title\", \"vector\"],  # Campos a recuperar\n",
        "    \"query\": {\n",
        "        \"knn\": {\n",
        "            \"vector\": {  # Campo que contiene los embeddings\n",
        "                \"vector\": embedding[0],  # Embedding del tópico\n",
        "            \"k\": 50  \n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Ejecutar la consulta\n",
        "response = os_client.search(index='news', body=query)\n",
        "\n",
        "# Procesar la respuesta\n",
        "results = response['hits']['hits']\n",
        "embeddings = []\n",
        "for result in results:\n",
        "    print(f\"ID: {result['_id']}, Score: {result['_score']}, {result['_source']['title']}\")\n",
        "    embeddings.append(result['_source']['vector'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs_embedding = np.load(PATH+f\"modelos/docs_embeddings_0_1000.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matrix = cosine_similarity(docs_embedding, embedding)\n",
        "similarities = matrix.flatten()\n",
        "# Obtenemos los índices de los 10 valores más altos\n",
        "top_10_indices = similarities.argsort()[-10:][::-1]\n",
        "\n",
        "print(\"Los 10 índices de embeddings más cercanos son:\", top_10_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in top_10_indices:\n",
        "    print(df_parquet_1.iloc[i][\"in__title\"][:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics = get_topics_opensearch(date_filter='2024-04-02')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for reg in topics:\n",
        "    index = reg['index']\n",
        "    name = reg['name']\n",
        "    similarity_threshold = reg['similarity_threshold']\n",
        "    create_at = format_date(reg['created_at'])\n",
        "    from_date = reg['from_date'][:10]\n",
        "    to_date = reg['to_date'][:10]\n",
        "    title_best_doc = reg['title_best_doc']\n",
        "    id_best_doc = reg['id_best_doc']\n",
        "\n",
        "    db_topics.append([index, name, round(similarity_threshold, 4), create_at, from_date, to_date, title_best_doc, id_best_doc])\n",
        "    data_topics[index] = [name,\n",
        "                        title_best_doc,\n",
        "                        reg['best_doc'],\n",
        "                        reg['entities'],\n",
        "                        reg['keywords'],\n",
        "                        similarity_threshold,\n",
        "                        reg['vector']\n",
        "                        ] \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = get_topics_opensearch()\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model = BERTopic.load(PATH+f\"modelos/bertopic_model_0_1000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs_embedding = np.load(PATH+f\"modelos/docs_embeddings_0_1000.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_news = get_news( '2024-04-01' )\n",
        "db_news[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_news = pd.DataFrame(db_news , columns=[\"indice\", \"titulo\", \"noticia\", \"keywords\", \"entidades\", \"creado\"])\n",
        "\n",
        "data         = list(df_news['noticia'])\n",
        "entities     = list(df_news['entidades'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#topics, probs = topic_model.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk='0_1000'\n",
        "#np.save(PATH+f\"modelos/topics_{chunk}.npy\", topics)\n",
        "#np.save(PATH+f\"modelos/probs_{chunk}.npy\", probs)\n",
        "topics = np.load(PATH+f\"modelos/topics_{chunk}.npy\")\n",
        "probs = np.load(PATH+f\"modelos/probs_{chunk}.npy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Desarrollo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_id = 4\n",
        "n_entities=10\n",
        "\n",
        "# Obtener todos los documentos de un topico\n",
        "topic_docs_idx = [i for i, (_, topic) in enumerate(zip(list(df_news.index), topics)) if topic == topic_id]\n",
        "df_data = pd.DataFrame(np.array(topic_docs_idx).reshape(-1,1), columns=[\"idx\"])\n",
        "\n",
        "# Similitud coseno entre el topico y los documentos del topico\n",
        "s_coseno = []\n",
        "for i in topic_docs_idx:\n",
        "    s_coseno.append(cosine_similarity([topic_model.topic_embeddings_[topic_id + 1]], [docs_embedding[i]])[0][0])\n",
        "\n",
        "df_data['similitud'] = s_coseno\n",
        "threshold = df_data['similitud'].mean()\n",
        "\n",
        "# Ordenado por mayor similitud\n",
        "df_filtered = df_data[df_data[\"similitud\"] > threshold].sort_values(\"similitud\", ascending=False)\n",
        "\n",
        "# Entidades de documentos ordenados para el topico elelgido (cantidad por documento=n_entities)\n",
        "entities_topic = []\n",
        "for doc in list(df_filtered[\"idx\"]):\n",
        "    entities_topic.append(entities[doc][:n_entities])\n",
        "\n",
        "# Crear un diccionario para contar en cuántos documentos aparece cada palabra\n",
        "document_frequencies = defaultdict(int)\n",
        "\n",
        "# Crear un conjunto para cada documento y contar las palabras únicas\n",
        "for lista in entities_topic:\n",
        "    unique_words = set(lista)\n",
        "    for palabra in unique_words:\n",
        "        document_frequencies[palabra] += 1\n",
        "\n",
        "# Ordenar las palabras por la frecuencia de documentos de mayor a menor\n",
        "sorted_frequencies = sorted(document_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Calcular el umbral\n",
        "freq_e = [item[1] for item in sorted_frequencies]\n",
        "umbral_e = np.mean(freq_e)\n",
        "\n",
        "# Obtener el resultado ordenado de las primeras 10 entidades segun criterio de corte\n",
        "topic_entities_top = {}\n",
        "c=0\n",
        "for idx in range(len(sorted_frequencies)):\n",
        "    if sorted_frequencies[idx][1] >= umbral_e:\n",
        "        if c != 10:\n",
        "            topic_entities_top[sorted_frequencies[idx][0]] = sorted_frequencies[idx][1]\n",
        "        else:\n",
        "            break\n",
        "        c += 1\n",
        "\n",
        "\n",
        "#--------------------------\n",
        "print(topic_model.topic_labels_[topic_id])\n",
        "print(umbral_e)\n",
        "print(topic_entities_top)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic = 0\n",
        "data_news = list(df_news.index)\n",
        "topic_entities_top = top_entities_(topic, topic_model, topics, docs_embedding, data_news, entities)\n",
        "topic_entities_top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_data['titulo'] = [df_news.iloc[idx]['titulo'] for idx in topic_docs_idx]\n",
        "df_data.sort_values(\"similitud\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_data['similitud'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_docs_idx = [i for i, (doc, topic) in enumerate(zip(list(df_news[\"indice\"]), topics)) if topic == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_entities_(topic_id: int, topic_model: object, topics: list, docs_embedding, data_news, entities: list, n_entities=10):\n",
        "    \"\"\"\n",
        "    Las entidades mas representativas del topico se extraen de las entidades de las noticias mas similares al topico\n",
        "    filtradas por el umbral del tópico\n",
        "    topic_id        : id del topico\n",
        "    topic_model     : modelo de topicos\n",
        "    topics          : lista de los indices de posicion de los documentos del conjunto de documentos de entrenamiento\n",
        "    n_entities      : cant. de entidades extraidas por cada documento del topico\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Obtener todos los documentos de un topic\n",
        "        #topic_docs_idx = [i for i, (doc, topic) in enumerate(zip(list(df_news[\"indice\"]), topics)) if topic == topic_id]\n",
        "        topic_docs_idx = [i for i, (doc, topic) in enumerate(zip(data_news, topics)) if topic == topic_id]\n",
        "        df_data = pd.DataFrame(np.array(topic_docs_idx).reshape(-1,1), columns=[\"idx\"])\n",
        "\n",
        "        # Similitud coseno entre el topico y los documentos del topico\n",
        "        s_coseno = []\n",
        "        for i in topic_docs_idx:\n",
        "            s_coseno.append(cosine_similarity([topic_model.topic_embeddings_[topic_id + 1]], [docs_embedding[i]])[0][0])\n",
        "\n",
        "        df_data['similitud'] = s_coseno\n",
        "        \n",
        "        # umbral\n",
        "        threshold = df_data['similitud'].mean()\n",
        "\n",
        "        # Ordenado por mayor similitud\n",
        "        df_filtered = df_data[df_data[\"similitud\"] > threshold].sort_values(\"similitud\", ascending=False)\n",
        "\n",
        "        # Entidades de documentos ordenados para el topico elelgido (cantidad por documento=n_entities)\n",
        "        entities_topic = []\n",
        "        for doc in list(df_filtered[\"idx\"]):\n",
        "            entities_topic.append(entities[doc][:n_entities])\n",
        "\n",
        "        # Crear un diccionario para contar en cuántos documentos aparece cada palabra\n",
        "        document_frequencies = defaultdict(int)\n",
        "\n",
        "        # Crear un conjunto para cada documento y contar las palabras únicas\n",
        "        for lista in entities_topic:\n",
        "            unique_words = set(lista)\n",
        "            for palabra in unique_words:\n",
        "                document_frequencies[palabra] += 1\n",
        "        \n",
        "        # Ordenar las palabras por la frecuencia de documentos de mayor a menor\n",
        "        sorted_frequencies = sorted(document_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        # Calcular el umbral\n",
        "        freq_e = [item[1] for item in sorted_frequencies]\n",
        "        umbral_e = np.mean(freq_e)\n",
        "\n",
        "        # Obtener el resultado ordenado de las primeras 10 entidades segun criterio de corte\n",
        "        topic_entities_top = {}\n",
        "        c=0\n",
        "        for idx in range(len(sorted_frequencies)):\n",
        "            if sorted_frequencies[idx][1] >= umbral_e:\n",
        "                if c != 10:\n",
        "                    topic_entities_top[sorted_frequencies[idx][0]] = sorted_frequencies[idx][1]\n",
        "                else:\n",
        "                    break\n",
        "                c += 1 \n",
        "\n",
        "        print(type(topic_entities_top))\n",
        "        print(topic_entities_top)\n",
        "        \n",
        "        return topic_entities_top\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Ha ocurrido un error: {e}\")\n",
        "        return False  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_entities_2(topic_id: int, topic_model: object, topics: list, docs_embedding, xx, entities: list, n_entities=10):\n",
        "    \"\"\"\n",
        "    Las entidades mas representativas del topico se extraen de las entidades de las noticias mas similares al topico\n",
        "    filtradas por el umbral del tópico\n",
        "    topic_id        : id del topico\n",
        "    topic_model     : modelo de topicos\n",
        "    topics          : lista de los indices de posicion de los documentos del conjunto de documentos de entrenamiento\n",
        "    n_entities      : cant. de entidades extraidas por cada documento del topico\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    # Obtener todos los documentos de un topic\n",
        "    topic_docs_idx = [i for i, (doc, topic) in enumerate(zip(xx, topics)) if topic == topic_id]\n",
        "\n",
        "    return topic_docs_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xx = list(df_news.index)\n",
        "xx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_entities_top = top_entities_2(topic, topic_model, topics, docs_embedding, xx , entities)\n",
        "topic_entities_top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_entities_top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_news.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.get_topics().keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGxTNO4yTgb9k2r4ffbTN0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
