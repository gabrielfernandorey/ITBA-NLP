{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielfernandorey/ITBA-NLP/blob/main/ITBA_nlp01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumen del problema\n",
        "\n",
        "- Calcular los tópicos de portales de noticias que se reciben \n",
        "- Frecuencia del cálculo de tópicos: diaria\n",
        "- Colección de noticias: diariamente, en lotes o de a un texto.\n",
        "- Identificar tópicos, entidades, keywords y análisis de sentimiento.\n",
        "\n",
        "### Datos\n",
        "- Se reciben las noticias con formato: Titulo, Texto, Fecha, Entidades, Keywords\n",
        "\n",
        "### Tareas\n",
        "- Modelo de detección de tópicos diario utilizando embeddings\n",
        "- Definir un criterio de agrupación de tópicos aplicado al mismo día y entre distintos días (merging)\n",
        "- Almacenar los embeddings de tópicos en una base de datos vectorial\n",
        "- Modelo de datos dado: \n",
        "    - Id del tópico\n",
        "    - Nombre del tópico\n",
        "    - Keywords\n",
        "    - Embbeding\n",
        "    - Fecha de creación\n",
        "    - Fecha de entrenamiento inicial\n",
        "    - Fecha de entrenamiento actualizada\n",
        "    - Umbral de detección\n",
        "    - Documento mas cercano\n",
        "---\n",
        "Tareas en esta notebook:\n",
        "- Inicializar la base de datos vectorial\n",
        "- Ingestar data\n",
        "- NER: Encontrar las entidades de cada documento\n",
        "- Limpiar data\n",
        "- Modelo: Armado del modelo BERTopic\n",
        "- Entrenamiento\n",
        "- Almacenamiento en base de datos vectorial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "1e5d8d12-903f-4a10-ddd3-6d7d9ae83cc7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from NLP_tools import clean_all\n",
        "from core.functions import *\n",
        "\n",
        "# levantar la base antes de ejecutar\n",
        "from opensearch_data_model import os_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializamos la base vectorial\n",
        "Se modifica la indice de la base \"Topic\" agregando referencias del documento mas cercano y un campo para los 100 documentos mas cercanos al tópico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-10 15:24:02.882 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El índice Topic ya existe. Saltando inicialización de base de datos.\n",
            "El índice News ya existe. Saltando inicialización de base de datos.\n"
          ]
        }
      ],
      "source": [
        "# Inicialización de indices\n",
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:/Users/gabri/OneDrive/Machine Learning/Github/ITBA-NLP/data/'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "cmp3cLLv28-T",
        "outputId": "2d83d8fc-9241-448a-98a1-6230eb29ce2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_joined\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Asset Name</th>\n",
              "      <th>Author Id</th>\n",
              "      <th>Author Name</th>\n",
              "      <th>Keyword Id</th>\n",
              "      <th>Keyword Name</th>\n",
              "      <th>Entity Id</th>\n",
              "      <th>Entity Name</th>\n",
              "      <th>Media Group Id</th>\n",
              "      <th>Media Group Name</th>\n",
              "      <th>Impact</th>\n",
              "      <th>...</th>\n",
              "      <th>in__text</th>\n",
              "      <th>out__entities</th>\n",
              "      <th>out__potential_entities</th>\n",
              "      <th>predicted_at_entities</th>\n",
              "      <th>out__keywords_sorted</th>\n",
              "      <th>predicted_at_keywords</th>\n",
              "      <th>start_time_utc</th>\n",
              "      <th>start_time_local</th>\n",
              "      <th>truncated_text</th>\n",
              "      <th>title_and_text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asset Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>105628101</th>\n",
              "      <td>Elecciones en Venezuela: María Corina Machado ...</td>\n",
              "      <td>36192</td>\n",
              "      <td>Infobae</td>\n",
              "      <td>1932002 | 417739 | 1687638 | 36187 | 7476 | 50...</td>\n",
              "      <td>[falsas conspiraciones armadas, sustituta, det...</td>\n",
              "      <td>219925 | 210613 | 219770 | 36424 | 1129437</td>\n",
              "      <td>[Nicolás Maduro, Jorge Rodríguez, Marcelo Ebra...</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>7406333</td>\n",
              "      <td>...</td>\n",
              "      <td>Fotografía de archivo de la líder antichavista...</td>\n",
              "      <td>[Nicolás Maduro, Marcelo Ebrard, Jorge Rodrígu...</td>\n",
              "      <td>[Jorge Rodríguez, Nicolás Maduro, Rayner Peña ...</td>\n",
              "      <td>2024-04-02 08:11:57.825777</td>\n",
              "      <td>[elecciones presidenciales, candidatura presid...</td>\n",
              "      <td>2024-04-02 08:17:44.372891+00:00</td>\n",
              "      <td>2024-04-02</td>\n",
              "      <td>2024-04-01 21:00:00</td>\n",
              "      <td>Fotografía de archivo de la líder antichavista...</td>\n",
              "      <td>Elecciones en Venezuela: María Corina Machado ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Asset Name  Author Id  \\\n",
              "Asset Id                                                                  \n",
              "105628101  Elecciones en Venezuela: María Corina Machado ...      36192   \n",
              "\n",
              "          Author Name                                         Keyword Id  \\\n",
              "Asset Id                                                                   \n",
              "105628101     Infobae  1932002 | 417739 | 1687638 | 36187 | 7476 | 50...   \n",
              "\n",
              "                                                Keyword Name  \\\n",
              "Asset Id                                                       \n",
              "105628101  [falsas conspiraciones armadas, sustituta, det...   \n",
              "\n",
              "                                            Entity Id  \\\n",
              "Asset Id                                                \n",
              "105628101  219925 | 210613 | 219770 | 36424 | 1129437   \n",
              "\n",
              "                                                 Entity Name  Media Group Id  \\\n",
              "Asset Id                                                                       \n",
              "105628101  [Nicolás Maduro, Jorge Rodríguez, Marcelo Ebra...               0   \n",
              "\n",
              "          Media Group Name   Impact  ...  \\\n",
              "Asset Id                             ...   \n",
              "105628101                   7406333  ...   \n",
              "\n",
              "                                                    in__text  \\\n",
              "Asset Id                                                       \n",
              "105628101  Fotografía de archivo de la líder antichavista...   \n",
              "\n",
              "                                               out__entities  \\\n",
              "Asset Id                                                       \n",
              "105628101  [Nicolás Maduro, Marcelo Ebrard, Jorge Rodrígu...   \n",
              "\n",
              "                                     out__potential_entities  \\\n",
              "Asset Id                                                       \n",
              "105628101  [Jorge Rodríguez, Nicolás Maduro, Rayner Peña ...   \n",
              "\n",
              "               predicted_at_entities  \\\n",
              "Asset Id                               \n",
              "105628101 2024-04-02 08:11:57.825777   \n",
              "\n",
              "                                        out__keywords_sorted  \\\n",
              "Asset Id                                                       \n",
              "105628101  [elecciones presidenciales, candidatura presid...   \n",
              "\n",
              "                     predicted_at_keywords start_time_utc    start_time_local  \\\n",
              "Asset Id                                                                        \n",
              "105628101 2024-04-02 08:17:44.372891+00:00     2024-04-02 2024-04-01 21:00:00   \n",
              "\n",
              "                                              truncated_text  \\\n",
              "Asset Id                                                       \n",
              "105628101  Fotografía de archivo de la líder antichavista...   \n",
              "\n",
              "                                              title_and_text  \n",
              "Asset Id                                                      \n",
              "105628101  Elecciones en Venezuela: María Corina Machado ...  \n",
              "\n",
              "[1 rows x 21 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read the parquet file | ( lotes de prueba )\n",
        "\n",
        "df_params = {'0_1000':'0_1000_data.parquet',\n",
        "             '1000_2000':'1000_2000_data.parquet',\n",
        "             '2000_3000':'2000_3000_data.parquet',\n",
        "             'df_joined':'df_joined_2024-04-01 00_00_00.parquet'\n",
        "            }\n",
        "\n",
        "chunk = os.environ.get('CHUNK')\n",
        "print(chunk)\n",
        "\n",
        "df_parquet = pd.read_parquet(PATH+df_params[chunk])\n",
        "df_parquet.head(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codigo para fraccionar el dataset (pruebas)\n",
        "#df_parquet[:1000].to_parquet(PATH+'0_1000_data.parquet', engine='pyarrow')\n",
        "\n",
        "#df_1000_2000 = df_parquet[1000:2000]\n",
        "\n",
        "#df_1000_2000['start_time_local'] = '2024-04-03 00:00:00'\n",
        "#df_1000_2000.to_parquet(PATH+'1000_2000_data.parquet', engine='pyarrow')\n",
        "\n",
        "#df_2000 = df_parquet[2000:]\n",
        "\n",
        "#df_2000['start_time_local'] = '2024-04-05 00:00:00'\n",
        "#df_2000.to_parquet(PATH+'2000_3000_data.parquet', engine='pyarrow')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3104"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = list(df_parquet['in__text'])\n",
        "\n",
        "# Cantidad total de documentos\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StopWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "SPANISH_STOPWORDS = list(pd.read_csv(PATH+'spanish_stop_words.csv' )['stopwords'].values)\n",
        "SPANISH_STOPWORDS_SPECIAL = list(pd.read_csv(PATH+'spanish_stop_words_spec.csv' )['stopwords'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" import csv\n",
        "# Guardar la lista de stopwords especial en un archivo CSV\n",
        "with open(PATH+\"spanish_stop_words_spec.csv\", mode='w', newline='', encoding='utf-8') as archivo:\n",
        "    escritor = csv.writer(archivo)\n",
        "    escritor.writerow(['stopwords'])\n",
        "    for stopword in SPANISH_STOPWORDS_SPECIAL:\n",
        "        escritor.writerow([stopword]) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER - Named Entity Recognition\n",
        "Obtener entidades de las noticias "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el modelo de spaCy para español\n",
        "spa = spacy.load(\"es_core_news_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" # Cargar o saltar carga y procesar celda inferior\n",
        "with open(PATH+f'modelos/entities{chunk}.json', 'r') as json_file:\n",
        "    entities = json.load(json_file)\n",
        "\n",
        "with open(PATH+f'modelos/entities_spa{chunk}.json', 'r') as json_file:\n",
        "    entities_spa = json.load(json_file)\n",
        "\n",
        "with open(PATH+f'modelos/keywords_spa{chunk}.json', 'r') as json_file:\n",
        "    keywords_spa = json.load(json_file) \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "171it [00:18,  9.48it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar entidades para todos los documentos usando spaCy\n",
        "\n",
        "original_entities = []\n",
        "for data_in in tqdm(data):\n",
        "\n",
        "    # Contabilizar palabras en doc\n",
        "    normalized_text = re.sub(r'\\W+', ' ', data_in.lower())\n",
        "    words_txt_without_stopwords = [word for word in normalized_text.split() if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL]\n",
        "    words_txt_counter = Counter(words_txt_without_stopwords)\n",
        "    words_counter = {elemento: cuenta for elemento, cuenta in sorted(words_txt_counter.items(), key=lambda item:item[1], reverse=True) if cuenta > 1}\n",
        "\n",
        "    # Extraer entidades del doc segun atributos\n",
        "    extract = spa(data_in)\n",
        "    entidades_spacy = [(ent.text, ent.label_) for ent in extract.ents]\n",
        "    ent_select = [ent for ent in entidades_spacy if ent[1] == 'PER' or ent[1] == 'ORG' or ent[1] == 'LOC' ]\n",
        "\n",
        "    # Extraer entidades maximo 3 palabras \n",
        "    entidades = [ent[0] for ent in ent_select if len(ent[0].split()) <= 3]\n",
        "    ent_clean = clean_all(entidades, accents=False)\n",
        "    ent_unique = list(set([ word for word in ent_clean if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL] ))\n",
        "\n",
        "    ents_proc = {}\n",
        "    \n",
        "    pre_original_entities = []\n",
        "    for ent in ent_unique:\n",
        "        \n",
        "        # Criterio de selección \n",
        "        weight = 0\n",
        "        for word in ent.split():\n",
        "            if word in words_counter:\n",
        "                weight += 1 /len(ent.split()) * words_counter[word]\n",
        "        \n",
        "        ents_proc[ent] = round(weight,4)\n",
        "\n",
        "    ents_proc = {k: v for k, v in sorted(ents_proc.items(), key=lambda item: item[1], reverse=True) if v > 0}\n",
        "\n",
        "    # Crear la lista de entidades procesadas por noticia \n",
        "    pre_entities = [key for key, _ in ents_proc.items()] \n",
        "\n",
        "    # Obtener las última palabra de cada entidad que tenga mas de una palabra por entidad\n",
        "    ult_palabras = list(set([ent.split()[-1] for ent in pre_entities if len(ent.split()) > 1 ]))\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al final de una compuesta\n",
        "    pre_entities_aux = []\n",
        "    for idx, ent in enumerate(pre_entities):\n",
        "        if not (len(ent.split()) == 1 and ent in ult_palabras):\n",
        "            pre_entities_aux.append(ent)\n",
        "\n",
        "    # Obtener las palabras únicas\n",
        "    unicas_palabras = [ ent.split()[0] for ent in pre_entities_aux if len(ent.split()) > 1 ]\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al comienzo de una compuesta\n",
        "    pre_entities = []\n",
        "    for idx, ent in enumerate(pre_entities_aux):\n",
        "        if not (len(ent.split()) == 1 and ent in unicas_palabras):\n",
        "            pre_entities.append(ent)\n",
        "\n",
        "    # obtener entidades filtradas\n",
        "    if len(pre_entities) > 10:\n",
        "        umbral = 10 + (len(pre_entities)-10) // 2\n",
        "        entities = pre_entities[:umbral] \n",
        "    else:\n",
        "        entities = pre_entities[:10]\n",
        "\n",
        "    # capturar las entidades en formato original\n",
        "    for ent in entities:\n",
        "        pre_original_entities.append([elemento for elemento in entidades if elemento.lower() == ent.lower()])\n",
        "\n",
        "    sort_original_entities = sorted(pre_original_entities, key=len, reverse=True)\n",
        "    \n",
        "    try:\n",
        "        original_entities.append( [ent[0] for ent in sort_original_entities if ent] ) \n",
        "    except Exception as e:\n",
        "        original_entities.append([])\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detectar entidades para todos los documentos usando spaCy\n",
        "entities_ = []\n",
        "data_in =  data[4]\n",
        "\n",
        "# Contabilizar palabras en doc\n",
        "normalized_text = re.sub(r'\\W+', ' ', data_in.lower())\n",
        "words_txt_without_stopwords = [word for word in normalized_text.split() if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL]\n",
        "words_txt_counter = Counter(words_txt_without_stopwords)\n",
        "words_counter = {elemento: cuenta for elemento, cuenta in sorted(words_txt_counter.items(), key=lambda item:item[1], reverse=True) }\n",
        "\n",
        "# Extraer entidades del doc segun atributos\n",
        "extract = spa(data_in)\n",
        "entidades_spacy = [(ent.text, ent.label_) for ent in extract.ents]\n",
        "ent_select = [ent for ent in entidades_spacy if ent[1] == 'PER' or ent[1] == 'ORG' or ent[1] == 'LOC' ]\n",
        "\n",
        "# Extraer entidades maximo 3 palabras \n",
        "entidades = [ent[0] for ent in ent_select if len(ent[0].split()) <= 3]\n",
        "ent_clean = clean_all(entidades, accents=False)\n",
        "ent_unique = list(set([ word for word in ent_clean if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL] ))\n",
        "\n",
        "ents_proc = {}\n",
        "\n",
        "pre_original_entities = []\n",
        "for ent in ent_unique:\n",
        "    \n",
        "    # Criterio de selección \n",
        "    weight = 0\n",
        "    for word in ent.split():\n",
        "        if word in words_counter:\n",
        "            weight += 1 /len(ent.split()) * words_counter[word]\n",
        "    \n",
        "    ents_proc[ent] = round(weight,4)\n",
        "\n",
        "ents_proc = {k: v for k, v in sorted(ents_proc.items(), key=lambda item: item[1], reverse=True) if v > 0}\n",
        "\n",
        "# Crear la lista de entidades procesadas por noticia \n",
        "pre_entities = [key for key, _ in ents_proc.items()] \n",
        "\n",
        "# Obtener las última palabra de cada entidad que tenga mas de una palabra por entidad\n",
        "ult_palabras = list(set([ent.split()[-1] for ent in pre_entities if len(ent.split()) > 1 ]))\n",
        "\n",
        "# Eliminar palabra única si la encuentra al final de una compuesta\n",
        "pre_entities_aux = []\n",
        "for idx, ent in enumerate(pre_entities):\n",
        "    if not (len(ent.split()) == 1 and ent in ult_palabras):\n",
        "        pre_entities_aux.append(ent)\n",
        "\n",
        "# Obtener las palabras únicas\n",
        "unicas_palabras = [ ent.split()[0] for ent in pre_entities_aux if len(ent.split()) > 1 ]\n",
        "\n",
        "# Eliminar palabra única si la encuentra al comienzo de una compuesta\n",
        "pre_entities = []\n",
        "for idx, ent in enumerate(pre_entities_aux):\n",
        "    if not (len(ent.split()) == 1 and ent in unicas_palabras):\n",
        "        pre_entities.append(ent)\n",
        "\n",
        "# obtener entidades filtradas\n",
        "if len(pre_entities) > 10:\n",
        "    umbral = 10 + (len(pre_entities)-10) // 2\n",
        "    entities_f = pre_entities[:umbral] \n",
        "else:\n",
        "    entities_f = pre_entities[:10]\n",
        "\n",
        "# capturar las entidades en formato original\n",
        "for ent in entities_f:\n",
        "    pre_original_entities.append([elemento for elemento in entidades if elemento.lower() == ent.lower()])\n",
        "\n",
        "sort_original_entities = sorted(pre_original_entities, key=len, reverse=True)\n",
        "\n",
        "try:\n",
        "    entities_.append([ent[0] for ent in sort_original_entities if ent] ) \n",
        "except Exception as e:\n",
        "    entities_.append([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Mariana',\n",
              "  'Diego Maradona',\n",
              "  'Diez',\n",
              "  'Juanito Belmonte',\n",
              "  'Enrique Pinti',\n",
              "  'Sembró',\n",
              "  'Edelweiss',\n",
              "  'calle Libertad',\n",
              "  'Cuba',\n",
              "  'Cocodrilo',\n",
              "  'Salsa Criolla']]"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entities_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'lectura': 17,\n",
              " 'voz': 12,\n",
              " 'alta': 12,\n",
              " 'práctica': 6,\n",
              " 'niños': 5,\n",
              " 'escuela': 5,\n",
              " 'desarrollo': 4,\n",
              " 'leer': 4,\n",
              " 'social': 4,\n",
              " 'libros': 4,\n",
              " 'lingüístico': 3,\n",
              " 'integral': 3,\n",
              " 'investigación': 3,\n",
              " 'hogar': 3,\n",
              " 'vocabulario': 3,\n",
              " 'iniciativas': 3,\n",
              " 'presenta': 2,\n",
              " 'herramienta': 2,\n",
              " 'esencial': 2,\n",
              " 'niñas': 2,\n",
              " 'educativo': 2,\n",
              " 'familiar': 2,\n",
              " 'artículo': 2,\n",
              " 'universidad': 2,\n",
              " 'capacidad': 2,\n",
              " 'palabras': 2,\n",
              " 'actividad': 2,\n",
              " 'texto': 2,\n",
              " 'experiencia': 2,\n",
              " 'conexión': 2,\n",
              " 'beneficios': 2,\n",
              " 'ofrece': 2,\n",
              " 'mejora': 2,\n",
              " 'escritura': 2,\n",
              " 'fomentar': 2,\n",
              " 'lazos': 2,\n",
              " 'afectivos': 2,\n",
              " 'padres': 2,\n",
              " 'resalta': 2,\n",
              " 'importancia': 2,\n",
              " 'emocional': 2,\n",
              " 'visual': 2,\n",
              " 'sugieren': 2,\n",
              " 'turnos': 2,\n",
              " 'creación': 2,\n",
              " 'atmósfera': 2,\n",
              " 'propicia': 2,\n",
              " 'genere': 2,\n",
              " 'expectativa': 2}"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'modelos/entities{chunk}.json', 'w') as file:\n",
        "    json.dump(entities, file)\n",
        "\n",
        "# Grabar\n",
        "with open(PATH+f'modelos/entities_spa{chunk}.json', 'w') as file:\n",
        "    json.dump(entities_spa, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keywords\n",
        "Obtener palabras clave de las noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3104/3104 [04:43<00:00, 10.96it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar keywords para todos los documentos usando spaCy\n",
        "\n",
        "keywords_spa = []\n",
        "for doc in tqdm(data):\n",
        "    extract = spa(doc)\n",
        "    keywords_spa.append([(ext.text, ext.pos_) for ext in extract])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keyboards with neighboards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encontrar la posicion en el df segun su indice\n",
        "df_parquet.index.get_loc(105640350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prueba ejemplo\n",
        "doc = 211\n",
        "\n",
        "# Obtenemos las keywords 'NOUN' mas frecuentes\n",
        "nouns = []\n",
        "for token in keywords_spa[doc]:\n",
        "    if token[1] == 'NOUN':\n",
        "        nouns.append(token[0])\n",
        "\n",
        "count_nouns = Counter(nouns)\n",
        "\n",
        "count_nouns.most_common()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos las keywords 'VERB' mas frecuentes\n",
        "verbs = []\n",
        "for token in keywords_spa[doc]:\n",
        "    if token[1] == 'VERB':\n",
        "        verbs.append(token[0])\n",
        "\n",
        "count_verbs = Counter(verbs)\n",
        "\n",
        "count_verbs.most_common()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pobar un documento ( resultados lematizados )\n",
        "keywords_spa_n = []\n",
        "extract = spa(data[doc])\n",
        "keywords_spa_n.append([(ext.lemma_, ext.pos_) for ext in extract])\n",
        "keywords_spa_n[0][:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resultados sin lematizar\n",
        "extract = spa(data[doc])\n",
        "tokens_and_labels = [(token.text, token.pos_) for token in extract if token.is_alpha]\n",
        "tokens_and_labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a function to get all two-word combinations\n",
        "def get_bigrams(word_list, number_consecutive_words=2):\n",
        "    \n",
        "    ngrams = []\n",
        "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
        "    \n",
        "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
        "    for word_index in range(adj_length_of_word_list):\n",
        "        \n",
        "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
        "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
        "        \n",
        "        #Append this word combo to the master list \"ngrams\"\n",
        "        ngrams.append(ngram)\n",
        "        \n",
        "    return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bigrams = get_bigrams(tokens_and_labels)\n",
        "bigrams[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# return the most frequent words that appear next to a particular keyword\n",
        "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
        "    \n",
        "    neighbor_words = []\n",
        "    keyword = keyword.lower()\n",
        "    \n",
        "    for bigram in bigrams:\n",
        "        \n",
        "        #Extract just the lowercased words (not the labels) for each bigram\n",
        "        words = [word.lower() for word, label in bigram]        \n",
        "        \n",
        "        #Check to see if keyword is in the bigram\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            for word, label in bigram:\n",
        "                \n",
        "                #Now focus on the neighbor word, not the keyword\n",
        "                if word.lower() != keyword:\n",
        "                    #If the neighbor word matches the right pos_label, append it to the master list\n",
        "                    if label == pos_label or pos_label == None:\n",
        "                        if idx == 0:\n",
        "                            neighbor_words.append(\" \".join([keyword, word.lower()]))\n",
        "                        else:\n",
        "                            neighbor_words.append(\" \".join([word.lower(), keyword]))\n",
        "                    \n",
        "    return Counter(neighbor_words).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for word in count_nouns.most_common():\n",
        "    print(get_neighbor_words(word[0], bigrams, pos_label='ADJ'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Funcion completa para keywords with neighboards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keywords_with_neighboards(keywords_spa, POS_1='NOUN', POS_2='ADJ'):\n",
        "    \"\"\"\n",
        "    Funcion que devuelve dos listas:\n",
        "    - lista de keywords with neighboards (segun argumentos POS_1 y POS_2)\n",
        "    - lista de keywords mas frecuentes (segun argumentos POS_1 y POS_2)\n",
        "    \"\"\"\n",
        "\n",
        "    doc_kwn = []\n",
        "    commons = []\n",
        "    for keywords in keywords_spa:\n",
        "    \n",
        "        # Obtenemos las keywords del tipo (Universal Dependences) mas frecuentes de cada doc (spaCy format)\n",
        "        words = []\n",
        "        for k_spa in keywords:\n",
        "            if k_spa[1] == POS_1:\n",
        "                words.append(k_spa[0])\n",
        "\n",
        "        cont_words = Counter(words)\n",
        "\n",
        "        common = cont_words.most_common()\n",
        "        commons.append( [com for com in common if com[1] > 1] )\n",
        "\n",
        "        # Calcular un umbral de corte (en repeticiones) para los keywords obtenidos\n",
        "            ## suma de todos los valores\n",
        "        valores = [valor for _, valor in common]\n",
        "\n",
        "            ## Calcular los pesos como proporcionales a los valores mismos\n",
        "        pesos = np.array(valores) / np.sum(valores)\n",
        "\n",
        "            ## Calcular el umbral ponderado, valor 2 o superior ( debe repetirse la keyword al menos una vez )\n",
        "        threshold = max(2, round(np.sum(np.array(valores) * pesos),4))\n",
        "\n",
        "\n",
        "        # Obtenemos los bigramas del doc        \n",
        "        tokens_and_labels = [(token[0], token[1]) for token in keywords if token[0].isalpha()]\n",
        "\n",
        "        bigrams = get_bigrams(tokens_and_labels)\n",
        "\n",
        "        keywords_neighbor = []\n",
        "        for item_common in common:\n",
        "            if item_common[1] >= threshold or len(keywords_neighbor) < 6: # corte por umbral o menor a 6\n",
        "                \n",
        "                kwn = get_neighbor_words(item_common[0], bigrams, pos_label=POS_2)\n",
        "                if kwn != []:\n",
        "                    keywords_neighbor.append( kwn )\n",
        "\n",
        "        sorted_keywords_neighbor = sorted([item for sublist in keywords_neighbor for item in sublist ], key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        doc_kwn.append(sorted_keywords_neighbor)\n",
        "\n",
        "    return doc_kwn, commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_w_n, common = keywords_with_neighboards(keywords_spa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# filtramos que al menos se repitan una vez\n",
        "filtered_k_w_n = [ [tupla[0] for tupla in sublista if tupla[1] > 1] for sublista in k_w_n ]\n",
        "filtered_k_w_n[4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "common[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_common = [ [tupla[0] for i, tupla in enumerate(sublista) if i < 6] for sublista in common ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_common[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KeyBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keybert import KeyBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kw_model = KeyBERT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords = kw_model.extract_keywords(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BOW - Armado del vocabulario con las entidades y keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unificar Entities + Keywords + Keywords with neighboards\n",
        "vocab = list(set().union(*entities, *keywords, *filtered_k_w_n, *common[:10]))\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab[211]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar vocabulario\n",
        "with open(PATH+f'modelos/vocabulary{chunk}.json', 'w') as file:\n",
        "    json.dump(vocab, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar noticias en la base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configurar  batch_size = ( ej.: 5000 ) si se supera el limite 100MB en elasticsearch por operacion\n",
        "index_name = 'news'\n",
        "bulk_data = []\n",
        "\n",
        "# Unificar Keywords + Keywords with neighboards\n",
        "keywords_plus = [ list(set(keywords[i]+filtered_k_w_n[i])) for i in range(len(entities)) ]\n",
        "\n",
        "for idx, text_news in tqdm(enumerate(data)):\n",
        "    doc = {\n",
        "        'index': {\n",
        "            '_index': index_name,\n",
        "            '_id': int(df_parquet.index[idx])\n",
        "        }\n",
        "    }\n",
        "    reg = {\n",
        "        'title': str(df_parquet.iloc[idx].in__title),\n",
        "        'news' : str(text_news), \n",
        "        'author': str(df_parquet.iloc[idx]['Author Name']),\n",
        "        'topics': {},\n",
        "        'vector': None,\n",
        "        'keywords' : keywords_plus[idx],\n",
        "        'entities' : original_entities[idx],\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'process': False\n",
        "    }\n",
        "    bulk_data.append(json.dumps(doc))\n",
        "    bulk_data.append(json.dumps(reg))\n",
        "\n",
        "# Convertir la lista en un solo string separado por saltos de línea\n",
        "bulk_request_body = '\\n'.join(bulk_data) + '\\n'\n",
        "\n",
        "# Enviar la solicitud bulk\n",
        "response = os_client.bulk(body=bulk_request_body)\n",
        "\n",
        "if response['errors']:\n",
        "    print(\"Errores encontrados al insertar los documentos\")\n",
        "else:\n",
        "    print(\"Documentos insertados correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nota:\n",
        "- por cada documento se van a guardar las entidades que al menos se repitan una vez (mayor frecuencia)\n",
        "- se utilizarán todas las entidades guardadas de todos los documentos como vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keywords de dataframe: [array(['vocabulario', 'desarrollo lingüístico', 'libros', 'hogar',\n",
            "        'lectura'], dtype=object)                                  ]\n",
            "Entities de dataframe: [array([''], dtype=object)]\n",
            "--------------------------------------------------------------------------------\n",
            "Fila: 7\n",
            "Entities calculadas: []\n",
            "Keywords neighboards calculadas: ([['voz alta', 'desarrollo lingüístico']], [[('lectura', 16), ('voz', 11), ('práctica', 6), ('niños', 5), ('escuela', 5), ('desarrollo', 4), ('libros', 4), ('hogar', 3), ('vocabulario', 3), ('manera', 3), ('iniciativas', 3), ('herramienta', 2), ('artículo', 2), ('través', 2), ('capacidad', 2), ('palabras', 2), ('actividad', 2), ('uso', 2), ('texto', 2), ('experiencia', 2), ('conexión', 2), ('beneficios', 2), ('escritura', 2), ('lazos', 2), ('padres', 2), ('investigación', 2), ('importancia', 2), ('turnos', 2), ('creación', 2), ('atmósfera', 2), ('expectativa', 2), ('apertura', 2), ('tiempo', 2)]])\n"
          ]
        }
      ],
      "source": [
        "def funcion_aux(ID):\n",
        "    keywords_df = df_parquet[df_parquet.index==ID]['Keyword Name'].values\n",
        "    entities_df = df_parquet[df_parquet.index==ID]['Entity Name'].values\n",
        "    fila = df_parquet.index.get_loc(ID)\n",
        "    print(f\"Keywords de dataframe: {keywords_df}\")\n",
        "    print(f\"Entities de dataframe: {entities_df}\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Fila: {fila}\")\n",
        "    print(f\"Entities calculadas: {original_entities[fila]}\")\n",
        "    k_w_n, common = keywords_with_neighboards([keywords_spa[fila]])\n",
        "    filtered_k_w_n = [ [tupla[0] for tupla in sublista if tupla[1] > 1] for sublista in k_w_n ]\n",
        "    print(f\"Keywords neighboards calculadas: {filtered_k_w_n, common}\")\n",
        "\n",
        "funcion_aux(105641111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener las última palabra de cada entidad que tenga mas de una palabra por entidad\n",
        "ult_palabras = [ent.split()[-1] for ent in pre_entities if len(ent.split()) > 1 ]\n",
        "ult_palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_entities_aux = pre_entities\n",
        "for idx, ent in enumerate(pre_entities_aux):\n",
        "    if len(ent.split()) == 1 and ent in ult_palabras:\n",
        "        del pre_entities[idx]\n",
        "        del pre_original_entities[idx]\n",
        "\n",
        "pre_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener las palabras únicas\n",
        "unicas_palabras = [ ent.split()[0] for ent in pre_entities if len(ent.split()) > 1 ]\n",
        "unicas_palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_entities_aux = pre_entities\n",
        "for idx, ent in enumerate(pre_entities_aux):\n",
        "    if len(ent.split()) == 1 and ent in unicas_palabras:\n",
        "        del pre_entities[idx]\n",
        "        del pre_original_entities[idx]\n",
        "\n",
        "pre_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_original_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGxTNO4yTgb9k2r4ffbTN0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
