{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielfernandorey/ITBA-NLP/blob/main/ITBA_nlp01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumen del problema\n",
        "\n",
        "- Calcular los tópicos de portales de noticias que se reciben \n",
        "- Frecuencia del cálculo de tópicos: diaria\n",
        "- Colección de noticias: diariamente, en lotes o de a un texto.\n",
        "- Identificar tópicos, entidades, keywords y análisis de sentimiento.\n",
        "\n",
        "### Datos\n",
        "- Se reciben las noticias con formato: Titulo, Texto, Fecha, Entidades, Keywords\n",
        "\n",
        "### Tareas\n",
        "- Modelo de detección de tópicos diario utilizando embeddings\n",
        "- Definir un criterio de agrupación de tópicos aplicado al mismo día y entre distintos días (merging)\n",
        "- Almacenar los embeddings de tópicos en una base de datos vectorial\n",
        "- Modelo de datos dado: \n",
        "    - Id del tópico\n",
        "    - Nombre del tópico\n",
        "    - Keywords\n",
        "    - Embbeding\n",
        "    - Fecha de creación\n",
        "    - Fecha de entrenamiento inicial\n",
        "    - Fecha de entrenamiento actualizada\n",
        "    - Umbral de detección\n",
        "    - Documento mas cercano\n",
        "---\n",
        "Tareas en esta notebook:\n",
        "- Inicializar la base de datos vectorial\n",
        "- Ingestar data\n",
        "- NER: Encontrar las entidades de cada documento\n",
        "- Limpiar data\n",
        "- Modelo: Armado del modelo BERTopic\n",
        "- Entrenamiento\n",
        "- Almacenamiento en base de datos vectorial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "1e5d8d12-903f-4a10-ddd3-6d7d9ae83cc7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "from NLP_tools import clean_all\n",
        "from core.functions import *\n",
        "\n",
        "# -->> levantar la base antes de ejecutar\n",
        "from opensearch_data_model import os_client\n",
        "from opensearch_io import init_opensearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializamos la base vectorial\n",
        "Se modifica la indice de la base \"Topic\" agregando referencias del documento mas cercano y un campo para los 100 documentos mas cercanos al tópico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-21 19:57:37.739 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El índice Topic ya existe. Saltando inicialización de base de datos.\n",
            "El índice News ya existe. Saltando inicialización de base de datos.\n"
          ]
        }
      ],
      "source": [
        "# Inicialización de indices\n",
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:/Users/gabri/OneDrive/Machine Learning/Github/ITBA-NLP/data/'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "cmp3cLLv28-T",
        "outputId": "2d83d8fc-9241-448a-98a1-6230eb29ce2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0_1000\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Asset Name</th>\n",
              "      <th>Author Id</th>\n",
              "      <th>Author Name</th>\n",
              "      <th>Keyword Id</th>\n",
              "      <th>Keyword Name</th>\n",
              "      <th>Entity Id</th>\n",
              "      <th>Entity Name</th>\n",
              "      <th>Media Group Id</th>\n",
              "      <th>Media Group Name</th>\n",
              "      <th>Impact</th>\n",
              "      <th>...</th>\n",
              "      <th>in__text</th>\n",
              "      <th>out__entities</th>\n",
              "      <th>out__potential_entities</th>\n",
              "      <th>predicted_at_entities</th>\n",
              "      <th>out__keywords_sorted</th>\n",
              "      <th>predicted_at_keywords</th>\n",
              "      <th>start_time_utc</th>\n",
              "      <th>start_time_local</th>\n",
              "      <th>truncated_text</th>\n",
              "      <th>title_and_text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asset Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>105628101</th>\n",
              "      <td>Elecciones en Venezuela: María Corina Machado ...</td>\n",
              "      <td>36192</td>\n",
              "      <td>Infobae</td>\n",
              "      <td>1932002 | 417739 | 1687638 | 36187 | 7476 | 50...</td>\n",
              "      <td>[falsas conspiraciones armadas, sustituta, det...</td>\n",
              "      <td>219925 | 210613 | 219770 | 36424 | 1129437</td>\n",
              "      <td>[Nicolás Maduro, Jorge Rodríguez, Marcelo Ebra...</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>7406333</td>\n",
              "      <td>...</td>\n",
              "      <td>Fotografía de archivo de la líder antichavista...</td>\n",
              "      <td>[Nicolás Maduro, Marcelo Ebrard, Jorge Rodrígu...</td>\n",
              "      <td>[Jorge Rodríguez, Nicolás Maduro, Rayner Peña ...</td>\n",
              "      <td>2024-04-02 08:11:57.825777</td>\n",
              "      <td>[elecciones presidenciales, candidatura presid...</td>\n",
              "      <td>2024-04-02 08:17:44.372891+00:00</td>\n",
              "      <td>2024-04-02</td>\n",
              "      <td>2024-04-01 21:00:00</td>\n",
              "      <td>Fotografía de archivo de la líder antichavista...</td>\n",
              "      <td>Elecciones en Venezuela: María Corina Machado ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Asset Name  Author Id  \\\n",
              "Asset Id                                                                  \n",
              "105628101  Elecciones en Venezuela: María Corina Machado ...      36192   \n",
              "\n",
              "          Author Name                                         Keyword Id  \\\n",
              "Asset Id                                                                   \n",
              "105628101     Infobae  1932002 | 417739 | 1687638 | 36187 | 7476 | 50...   \n",
              "\n",
              "                                                Keyword Name  \\\n",
              "Asset Id                                                       \n",
              "105628101  [falsas conspiraciones armadas, sustituta, det...   \n",
              "\n",
              "                                            Entity Id  \\\n",
              "Asset Id                                                \n",
              "105628101  219925 | 210613 | 219770 | 36424 | 1129437   \n",
              "\n",
              "                                                 Entity Name  Media Group Id  \\\n",
              "Asset Id                                                                       \n",
              "105628101  [Nicolás Maduro, Jorge Rodríguez, Marcelo Ebra...               0   \n",
              "\n",
              "          Media Group Name   Impact  ...  \\\n",
              "Asset Id                             ...   \n",
              "105628101                   7406333  ...   \n",
              "\n",
              "                                                    in__text  \\\n",
              "Asset Id                                                       \n",
              "105628101  Fotografía de archivo de la líder antichavista...   \n",
              "\n",
              "                                               out__entities  \\\n",
              "Asset Id                                                       \n",
              "105628101  [Nicolás Maduro, Marcelo Ebrard, Jorge Rodrígu...   \n",
              "\n",
              "                                     out__potential_entities  \\\n",
              "Asset Id                                                       \n",
              "105628101  [Jorge Rodríguez, Nicolás Maduro, Rayner Peña ...   \n",
              "\n",
              "               predicted_at_entities  \\\n",
              "Asset Id                               \n",
              "105628101 2024-04-02 08:11:57.825777   \n",
              "\n",
              "                                        out__keywords_sorted  \\\n",
              "Asset Id                                                       \n",
              "105628101  [elecciones presidenciales, candidatura presid...   \n",
              "\n",
              "                     predicted_at_keywords start_time_utc    start_time_local  \\\n",
              "Asset Id                                                                        \n",
              "105628101 2024-04-02 08:17:44.372891+00:00     2024-04-02 2024-04-01 21:00:00   \n",
              "\n",
              "                                              truncated_text  \\\n",
              "Asset Id                                                       \n",
              "105628101  Fotografía de archivo de la líder antichavista...   \n",
              "\n",
              "                                              title_and_text  \n",
              "Asset Id                                                      \n",
              "105628101  Elecciones en Venezuela: María Corina Machado ...  \n",
              "\n",
              "[1 rows x 21 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read the parquet file | ( lotes de prueba )\n",
        "\n",
        "df_params = {'0_1000':'0_1000_data.parquet',\n",
        "             '1000_2000':'1000_2000_data.parquet',\n",
        "             '2000_3000':'2000_3000_data.parquet',\n",
        "             'df_joined':'df_joined_2024-04-01 00_00_00.parquet'\n",
        "            }\n",
        "\n",
        "#chunk = os.environ.get('CHUNK')\n",
        "chunk = '0_1000'\n",
        "print(chunk)\n",
        "\n",
        "df_parquet = pd.read_parquet(PATH+df_params[chunk])\n",
        "\n",
        "data = list(df_parquet['in__text'])\n",
        "\n",
        "df_parquet.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cantidad total de documentos\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StopWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "SPANISH_STOPWORDS = list(pd.read_csv(PATH+'spanish_stop_words.csv' )['stopwords'].values)\n",
        "SPANISH_STOPWORDS_SPECIAL = list(pd.read_csv(PATH+'spanish_stop_words_spec.csv' )['stopwords'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" import csv\n",
        "# Guardar la lista de stopwords especial en un archivo CSV\n",
        "with open(PATH+\"spanish_stop_words_spec.csv\", mode='w', newline='', encoding='utf-8') as archivo:\n",
        "    escritor = csv.writer(archivo)\n",
        "    escritor.writerow(['stopwords'])\n",
        "    for stopword in SPANISH_STOPWORDS_SPECIAL:\n",
        "        escritor.writerow([stopword]) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER - Named Entity Recognition\n",
        "Obtener entidades de las noticias "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el modelo de spaCy para español\n",
        "spa = spacy.load(\"es_core_news_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        " \n",
        "# Cargar o saltar carga y procesar celda inferior\n",
        "with open(PATH+f'modelos/entities_{chunk}.json', 'r') as json_file:\n",
        "    entities = json.load(json_file)\n",
        "\n",
        "with open(PATH+f'modelos/keywords_{chunk}.json', 'r') as json_file:\n",
        "    keywords = json.load(json_file)\n",
        "\n",
        "with open(PATH+f'modelos/vocabulary_{chunk}.json', 'r') as json_file:\n",
        "    vocab = json.load(json_file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:54<00:00,  8.75it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar entidades para todos los documentos usando spaCy\n",
        "\n",
        "entities = []\n",
        "for data_in in tqdm(data):\n",
        "\n",
        "    # Contabilizar palabras en doc original\n",
        "    normalized_text = re.sub(r'\\W+', ' ', data_in.lower())\n",
        "    words_txt_without_stopwords = [word for word in normalized_text.split() if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL]\n",
        "    words_txt_counter = Counter(words_txt_without_stopwords)\n",
        "    words_counter = {elemento: cuenta for elemento, cuenta in sorted(words_txt_counter.items(), key=lambda item:item[1], reverse=True) if cuenta > 1}\n",
        "\n",
        "    # Extraer entidades del doc segun atributos\n",
        "    extract = spa(data_in)\n",
        "    entidades_spacy = [(ent.text, ent.label_) for ent in extract.ents]\n",
        "    ent_select = [ent for ent in entidades_spacy if ent[1] == 'PER' or ent[1] == 'ORG' or ent[1] == 'LOC' ]\n",
        "\n",
        "    # Extraer entidades de \"maximo 3 palabras\"\n",
        "    ent_max_3 = [ent[0] for ent in ent_select if len(ent[0].split()) <= 3]\n",
        "    ent_clean = clean_all(ent_max_3, accents=False)\n",
        "    ent_unique = list(set([ word for word in ent_clean if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL] ))\n",
        "\n",
        "    ents_proc = {}\n",
        "    for ent in ent_unique:\n",
        "        \n",
        "        # Criterio de selección \n",
        "        weight = 0\n",
        "        for word in ent.split():\n",
        "            if word in words_counter:\n",
        "                weight += 1 /len(ent.split()) * words_counter[word]\n",
        "        \n",
        "        ents_proc[ent] = round(weight,4)\n",
        "\n",
        "    ents_proc_sorted = {k: v for k, v in sorted(ents_proc.items(), key=lambda item: item[1], reverse=True) if v > 0}\n",
        "\n",
        "    # Crear la lista preliminar de entidades procesadas por noticia \n",
        "    pre_entities = [key for key, _ in ents_proc_sorted.items()] \n",
        "\n",
        "    # Obtener las últimas palabras de cada entidad que tenga mas de una palabra por entidad\n",
        "    last_words = list(set([ent.split()[-1] for ent in pre_entities if len(ent.split()) > 1 ]))\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al final de una compuesta\n",
        "    pre_entities_without_last_word_equal = []\n",
        "    for idx, ent in enumerate(pre_entities):\n",
        "        if not (len(ent.split()) == 1 and ent in last_words):\n",
        "            pre_entities_without_last_word_equal.append(ent)\n",
        "\n",
        "    # Obtener las palabras únicas\n",
        "    unique_words = [ ent.split()[0] for ent in pre_entities_without_last_word_equal if len(ent.split()) > 1 ]\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al comienzo de una compuesta\n",
        "    pre_entities_without_first_word_equal = []\n",
        "    for idx, ent in enumerate(pre_entities_without_last_word_equal):\n",
        "        if not (len(ent.split()) == 1 and ent in unique_words):\n",
        "            pre_entities_without_first_word_equal.append(ent)\n",
        "\n",
        "    # obtener entidades filtradas\n",
        "    if len(pre_entities_without_first_word_equal) > 10:\n",
        "        umbral = 10 + (len(pre_entities_without_first_word_equal)-10) // 2\n",
        "        filter_entities = pre_entities_without_first_word_equal[:umbral] \n",
        "    else:\n",
        "        filter_entities = pre_entities_without_first_word_equal[:10]\n",
        "\n",
        "    pre_original_entities = []\n",
        "    # capturar las entidades en formato original\n",
        "    for ent in filter_entities:\n",
        "        pre_original_entities.append([elemento for elemento in ent_max_3 if elemento.lower() == ent.lower()])\n",
        "\n",
        "    sort_original_entities = sorted(pre_original_entities, key=len, reverse=True)\n",
        "    \n",
        "    try:\n",
        "        entities.append( [ent[0] for ent in sort_original_entities if ent] ) \n",
        "    except Exception as e:\n",
        "        entities.append([])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'modelos/entities_{chunk}.json', 'w') as file:\n",
        "    json.dump(entities, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keywords\n",
        "Obtener palabras clave de las noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:44<00:00,  9.53it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar keywords para todos los documentos usando spaCy\n",
        "\n",
        "keywords_spa = []\n",
        "for doc in tqdm(data):\n",
        "    extract = spa(doc)\n",
        "    keywords_spa.append([(ext.text, ext.pos_) for ext in extract])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keyboards with neighboards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funcion para obtener keywords con combinaciones de bigramas\n",
        "def get_bigrams(word_list, number_consecutive_words=2):\n",
        "    \n",
        "    ngrams = []\n",
        "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
        "    \n",
        "    for word_index in range(adj_length_of_word_list):\n",
        "        \n",
        "        # Indexar la lista \n",
        "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
        "        \n",
        "        # Agregar a la lista de \"ngrams\"\n",
        "        ngrams.append(ngram)\n",
        "        \n",
        "    return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# devolver las palabras más frecuentes que aparecen junto a una palabra clave en particular\n",
        "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
        "    \n",
        "    neighbor_words = []\n",
        "    keyword = keyword.lower()\n",
        "    \n",
        "    for bigram in bigrams:\n",
        "        \n",
        "        # Extrae solo las palabras en minúsculas (no las etiquetas) para cada bigrama\n",
        "        words = [word.lower() for word, label in bigram]        \n",
        "        \n",
        "        # Comprueba si la palabra clave está en el bigram\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            for word, label in bigram:\n",
        "                \n",
        "                #Ahora nos centramos en la palabra vecina, no en la palabra clave\n",
        "                if word.lower() != keyword:\n",
        "                    #Si la palabra vecina coincide con la pos_label correcta, agregarla a la lista maestra\n",
        "                    if label == pos_label or pos_label == None:\n",
        "                        if idx == 0:\n",
        "                            neighbor_words.append(\" \".join([keyword, word.lower()]))\n",
        "                        else:\n",
        "                            neighbor_words.append(\" \".join([word.lower(), keyword]))\n",
        "                    \n",
        "    return Counter(neighbor_words).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keywords_with_neighboards(keywords_spa, POS_1='NOUN', POS_2='ADJ'):\n",
        "    \"\"\"\n",
        "    Funcion que devuelve dos listas:\n",
        "    - lista de keywords with neighboards (segun argumentos POS_1 y POS_2)\n",
        "    - lista de keywords mas frecuentes (segun argumentos POS_1 y POS_2)\n",
        "    \"\"\"\n",
        "\n",
        "    doc_kwn = []\n",
        "    commons = []\n",
        "    for keywords in keywords_spa:\n",
        "    \n",
        "        # Obtenemos las keywords del tipo (Universal Dependences) mas frecuentes de cada doc (spaCy format)\n",
        "        words = []\n",
        "        for k_spa in keywords:\n",
        "            if k_spa[1] == POS_1:\n",
        "                words.append(k_spa[0])\n",
        "\n",
        "        cont_words = Counter(words)\n",
        "\n",
        "        common = cont_words.most_common()\n",
        "        commons.append( [com for com in common if com[1] > 1] )\n",
        "\n",
        "        # Calcular un umbral de corte (en repeticiones) para los keywords obtenidos\n",
        "            ## suma de todos los valores\n",
        "        valores = [valor for _, valor in common]\n",
        "\n",
        "            ## Calcular los pesos como proporcionales a los valores mismos\n",
        "        pesos = np.array(valores) / np.sum(valores)\n",
        "\n",
        "            ## Calcular el umbral ponderado, valor 2 o superior ( debe repetirse la keyword al menos una vez )\n",
        "        threshold = max(2, round(np.sum(np.array(valores) * pesos),4))\n",
        "\n",
        "\n",
        "        # Obtenemos los bigramas del doc        \n",
        "        tokens_and_labels = [(token[0], token[1]) for token in keywords if token[0].isalpha()]\n",
        "\n",
        "        bigrams = get_bigrams(tokens_and_labels)\n",
        "\n",
        "        keywords_neighbor = []\n",
        "        for item_common in common:\n",
        "            if item_common[1] >= threshold or len(keywords_neighbor) < 6: # corte por umbral o menor a 6\n",
        "                \n",
        "                kwn = get_neighbor_words(item_common[0], bigrams, pos_label=POS_2)\n",
        "                if kwn != []:\n",
        "                    keywords_neighbor.append( kwn )\n",
        "\n",
        "        sorted_keywords_neighbor = sorted([item for sublist in keywords_neighbor for item in sublist ], key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        doc_kwn.append(sorted_keywords_neighbor)\n",
        "\n",
        "    return doc_kwn, commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obtenemos keywords with neighboards y keywords mas frecuentes\n",
        "k_w_n, keyword_single = keywords_with_neighboards(keywords_spa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_w_n[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ahorro interno',\n",
              " 'sistema bancario',\n",
              " 'descomunal tasa',\n",
              " 'impuestos confiscatorios']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# filtramos las que al menos se repiten una vez\n",
        "filtered_k_w_n = [ [tupla[0] for tupla in sublista if tupla[1] > 1] for sublista in k_w_n ]\n",
        "filtered_k_w_n[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizamos los keywords unigrama\n",
        "keyword_single[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Si un keyword unigrama coincide en los bigramas elegidos se descarta\n",
        "# la cantidad de keywords se obtiene utilizando la media como umbral de corte\n",
        "\n",
        "# Umbral\n",
        "values = [value for sublist in keyword_single for _, value in sublist]\n",
        "threshold = np.mean(values)\n",
        "\n",
        "for i, sublist in enumerate(keyword_single):\n",
        "    lista_k_w_n = list(set([word for sentence in filtered_k_w_n[i] for word in sentence.split()]))\n",
        "    for tupla in sublist:\n",
        "        if tupla[1] >= threshold and tupla[0] not in lista_k_w_n:\n",
        "            filtered_k_w_n[i].append(tupla[0])\n",
        "\n",
        "keywords = filtered_k_w_n      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'modelos/keywords_{chunk}.json', 'w') as file:\n",
        "    json.dump(keywords, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BOW - Armado del vocabulario con las entidades y keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6826"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Unificar Entities + Keywords + Keywords with neighboards\n",
        "vocab = list(set().union(*entities, *keywords))\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar vocabulario\n",
        "with open(PATH+f'modelos/vocabulary_{chunk}.json', 'w') as file:\n",
        "    json.dump(vocab, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocesar las noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 1175.35it/s]\n"
          ]
        }
      ],
      "source": [
        "clean_data = Cleaning_text()\n",
        "\n",
        "proc_data = []\n",
        "for data_in in tqdm(data):\n",
        "    aux = clean_data.unicode(data_in)\n",
        "    aux = clean_data.urls(aux)\n",
        "    aux = clean_data.simbols(aux)\n",
        "    aux = clean_data.escape_sequence(aux)\n",
        "    aux = \" \".join([ word for word in aux.split() if word.lower() not in SPANISH_STOPWORDS_SPECIAL])\n",
        "    proc_data.append(aux)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'modelos/proc_data_{chunk}.json', 'w') as file:\n",
        "    json.dump(proc_data, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'punto mira preocupantes informaciones salud tiempos Bertín Osborne reaparecido Semana Santa confirmando recuperación marcha camino comenzar tratamiendo inyección vitaminas superar secuelas arrastra contagió Covid principios enero entorno reveló enfermedad dejado defensas sistema inmune debilitado reposo seguido finca Sevilla semanas frutos días dejaba comiendo cochinillo familiares popular venta localidad gaditana Medina Sidonia Semana Santa residencia sevillana puntos Andalucía preguntado hija Claudia Osborne llegada aeropuerto Madrid disfrutar vacaciones marido José Entrecanales pequeñas Micaela Violeta nació 22 enero lejos país rostro cansado llevando bebé pegado pecho mochila porteadora hijo presidente Acciona llevaba primogénita brazos coach mostrado discreta cámaras revelado sonrisa padre gracias'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "proc_data[60]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar noticias en el indice news de la base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [00:00, 2004.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos insertados correctamente\n"
          ]
        }
      ],
      "source": [
        "# configurar  batch_size = ( ej.: 5000 ) si se supera el limite 100MB en elasticsearch por operacion\n",
        "index_name = 'news'\n",
        "bulk_data = []\n",
        "\n",
        "for idx, text_news in tqdm(enumerate(data)):\n",
        "    doc = {\n",
        "        'index': {\n",
        "            '_index': index_name,\n",
        "            '_id': int(df_parquet.index[idx])\n",
        "        }\n",
        "    }\n",
        "    reg = {\n",
        "        'title': str(df_parquet.iloc[idx].in__title),\n",
        "        'news' : str(text_news), \n",
        "        'author': str(df_parquet.iloc[idx]['Author Name']),\n",
        "        'vector': None,\n",
        "        'keywords' : keywords[idx],\n",
        "        'entities' : entities[idx],\n",
        "        'created_at': parse(str(df_parquet.iloc[idx]['start_time_local'])).isoformat(),\n",
        "        'process': False\n",
        "    }\n",
        "    bulk_data.append(json.dumps(doc))\n",
        "    bulk_data.append(json.dumps(reg))\n",
        "\n",
        "# Convertir la lista en un solo string separado por saltos de línea\n",
        "bulk_request_body = '\\n'.join(bulk_data) + '\\n'\n",
        "\n",
        "# Enviar la solicitud bulk\n",
        "response = os_client.bulk(body=bulk_request_body)\n",
        "\n",
        "if response['errors']:\n",
        "    print(\"Errores encontrados al insertar los documentos\")\n",
        "else:\n",
        "    print(\"Documentos insertados correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Funciones de pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "211"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encontrar la posicion en el df segun su ID\n",
        "df_parquet.index.get_loc(105640350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noticia ID: 105638862 ['Bitcoin: cómo se ha movido en el mercado este 2 de abril']\n",
            "\n",
            "Entities de dataframe: ['Elon Musk' 'Bloomberg']\n",
            "Keywords de dataframe: ['flujo' 'precio' 'moneda legal' 'mercado' 'bitcoin']\n",
            "--------------------------------------------------------------------------------\n",
            "Fila: 15\n",
            "Entities calculadas: ['El Salvador', 'San Salvador']\n",
            "Keywords calculadas: ['bitcoin', 'criptomonedas', 'mercado', 'criptomoneda', 'valor']\n"
          ]
        }
      ],
      "source": [
        "def funcion_aux(ID):\n",
        "    keywords_df = df_parquet[df_parquet.index==ID]['Keyword Name'].values[0]\n",
        "    entities_df = df_parquet[df_parquet.index==ID]['Entity Name'].values[0]\n",
        "    fila = df_parquet.index.get_loc(ID)\n",
        "    print(f\"Noticia ID: {ID} {df_parquet[df_parquet.index==ID]['in__title'].values}\\n\")\n",
        "    print(f\"Entities de dataframe: {entities_df}\")\n",
        "    print(f\"Keywords de dataframe: {keywords_df}\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Fila: {fila}\")\n",
        "    print(f\"Entities calculadas: {entities[fila]}\")\n",
        "    print(f\"Keywords calculadas: {filtered_k_w_n[fila]}\")\n",
        "\n",
        "funcion_aux(105638862)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGxTNO4yTgb9k2r4ffbTN0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
