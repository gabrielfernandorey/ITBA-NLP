{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielfernandorey/ITBA-NLP/blob/main/ITBA_nlp01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumen del problema\n",
        "\n",
        "- Calcular los tópicos de portales de noticias que se reciben \n",
        "- Frecuencia del cálculo de tópicos: diaria\n",
        "- Colección de noticias: diariamente, en lotes o de a un texto.\n",
        "- Identificar tópicos, entidades, keywords y análisis de sentimiento.\n",
        "\n",
        "### Datos\n",
        "- Se reciben las noticias con formato: Titulo, Texto, Fecha, Entidades, Keywords\n",
        "\n",
        "### Tareas\n",
        "- Modelo de detección de tópicos diario utilizando embeddings\n",
        "- Definir un criterio de agrupación de tópicos aplicado al mismo día y entre distintos días (merging)\n",
        "- Almacenar los embeddings de tópicos en una base de datos vectorial\n",
        "- Modelo de datos: \n",
        "    - Id del tópico\n",
        "    - Nombre del tópico\n",
        "    - Keywords\n",
        "    - Embbeding\n",
        "    - Fecha de creación\n",
        "    - Fecha de entrenamiento inicial\n",
        "    - Fecha de entrenamiento actualizada\n",
        "    - Umbral de detección\n",
        "    - Documento mas cercano\n",
        "---\n",
        "Tareas en esta notebook:\n",
        "- Inicializar la base de datos vectorial\n",
        "- Ingestar data\n",
        "- NER: Encontrar las entidades de cada documento\n",
        "- Limpiar data\n",
        "- Modelo: Armado del modelo BERTopic\n",
        "- Entrenamiento\n",
        "- Almacenamiento en base de datos vectorial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "1e5d8d12-903f-4a10-ddd3-6d7d9ae83cc7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from NLP_tools import clean_all\n",
        "from core.functions import *\n",
        "\n",
        "# levantar la base antes de ejecutar\n",
        "from opensearch_data_model import os_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializamos la base vectorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-01 19:51:27.539 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El índice Topic ya existe. Saltando inicialización de base de datos.\n",
            "El índice News ya existe. Saltando inicialización de base de datos.\n"
          ]
        }
      ],
      "source": [
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:/Users/gabri/OneDrive/Machine Learning/Github/ITBA-TP/data/'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "cmp3cLLv28-T",
        "outputId": "2d83d8fc-9241-448a-98a1-6230eb29ce2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Asset Name</th>\n",
              "      <th>Author Id</th>\n",
              "      <th>Author Name</th>\n",
              "      <th>Keyword Id</th>\n",
              "      <th>Keyword Name</th>\n",
              "      <th>Entity Id</th>\n",
              "      <th>Entity Name</th>\n",
              "      <th>Media Group Id</th>\n",
              "      <th>Media Group Name</th>\n",
              "      <th>Impact</th>\n",
              "      <th>...</th>\n",
              "      <th>in__text</th>\n",
              "      <th>out__entities</th>\n",
              "      <th>out__potential_entities</th>\n",
              "      <th>predicted_at_entities</th>\n",
              "      <th>out__keywords_sorted</th>\n",
              "      <th>predicted_at_keywords</th>\n",
              "      <th>start_time_utc</th>\n",
              "      <th>start_time_local</th>\n",
              "      <th>truncated_text</th>\n",
              "      <th>title_and_text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asset Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>105628101</th>\n",
              "      <td>Elecciones en Venezuela: María Corina Machado ...</td>\n",
              "      <td>36192</td>\n",
              "      <td>Infobae</td>\n",
              "      <td>1932002 | 417739 | 1687638 | 36187 | 7476 | 50...</td>\n",
              "      <td>[falsas conspiraciones armadas, sustituta, det...</td>\n",
              "      <td>219925 | 210613 | 219770 | 36424 | 1129437</td>\n",
              "      <td>[Nicolás Maduro, Jorge Rodríguez, Marcelo Ebra...</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>7406333</td>\n",
              "      <td>...</td>\n",
              "      <td>Fotografía de archivo de la líder antichavista...</td>\n",
              "      <td>[Nicolás Maduro, Marcelo Ebrard, Jorge Rodrígu...</td>\n",
              "      <td>[Jorge Rodríguez, Nicolás Maduro, Rayner Peña ...</td>\n",
              "      <td>2024-04-02 08:11:57.825777</td>\n",
              "      <td>[elecciones presidenciales, candidatura presid...</td>\n",
              "      <td>2024-04-02 08:17:44.372891+00:00</td>\n",
              "      <td>2024-04-02</td>\n",
              "      <td>2024-04-01 21:00:00</td>\n",
              "      <td>Fotografía de archivo de la líder antichavista...</td>\n",
              "      <td>Elecciones en Venezuela: María Corina Machado ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Asset Name  Author Id  \\\n",
              "Asset Id                                                                  \n",
              "105628101  Elecciones en Venezuela: María Corina Machado ...      36192   \n",
              "\n",
              "          Author Name                                         Keyword Id  \\\n",
              "Asset Id                                                                   \n",
              "105628101     Infobae  1932002 | 417739 | 1687638 | 36187 | 7476 | 50...   \n",
              "\n",
              "                                                Keyword Name  \\\n",
              "Asset Id                                                       \n",
              "105628101  [falsas conspiraciones armadas, sustituta, det...   \n",
              "\n",
              "                                            Entity Id  \\\n",
              "Asset Id                                                \n",
              "105628101  219925 | 210613 | 219770 | 36424 | 1129437   \n",
              "\n",
              "                                                 Entity Name  Media Group Id  \\\n",
              "Asset Id                                                                       \n",
              "105628101  [Nicolás Maduro, Jorge Rodríguez, Marcelo Ebra...               0   \n",
              "\n",
              "          Media Group Name   Impact  ...  \\\n",
              "Asset Id                             ...   \n",
              "105628101                   7406333  ...   \n",
              "\n",
              "                                                    in__text  \\\n",
              "Asset Id                                                       \n",
              "105628101  Fotografía de archivo de la líder antichavista...   \n",
              "\n",
              "                                               out__entities  \\\n",
              "Asset Id                                                       \n",
              "105628101  [Nicolás Maduro, Marcelo Ebrard, Jorge Rodrígu...   \n",
              "\n",
              "                                     out__potential_entities  \\\n",
              "Asset Id                                                       \n",
              "105628101  [Jorge Rodríguez, Nicolás Maduro, Rayner Peña ...   \n",
              "\n",
              "               predicted_at_entities  \\\n",
              "Asset Id                               \n",
              "105628101 2024-04-02 08:11:57.825777   \n",
              "\n",
              "                                        out__keywords_sorted  \\\n",
              "Asset Id                                                       \n",
              "105628101  [elecciones presidenciales, candidatura presid...   \n",
              "\n",
              "                     predicted_at_keywords start_time_utc    start_time_local  \\\n",
              "Asset Id                                                                        \n",
              "105628101 2024-04-02 08:17:44.372891+00:00     2024-04-02 2024-04-01 21:00:00   \n",
              "\n",
              "                                              truncated_text  \\\n",
              "Asset Id                                                       \n",
              "105628101  Fotografía de archivo de la líder antichavista...   \n",
              "\n",
              "                                              title_and_text  \n",
              "Asset Id                                                      \n",
              "105628101  Elecciones en Venezuela: María Corina Machado ...  \n",
              "\n",
              "[1 rows x 21 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read the parquet file\n",
        "\n",
        "df_params = {'0_1000':'0_1000_data.parquet',\n",
        "             '1000_2000':'1000_2000_data.parquet',\n",
        "             '2000_3000':'2000_3000_data.parquet',\n",
        "             'df_joined':'df_joined_2024-04-01 00_00_00.parquet'\n",
        "            }\n",
        "\n",
        "chunk = os.environ.get('CHUNK')\n",
        "\n",
        "df_parquet = pd.read_parquet(PATH+df_params[chunk])\n",
        "df_parquet.head(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codigo para fraccionar el dataset (pruebas)\n",
        "#df_parquet[:1000].to_parquet(PATH+'0_1000_data.parquet', engine='pyarrow')\n",
        "\n",
        "#df_1000_2000 = df_parquet[1000:2000]\n",
        "\n",
        "#df_1000_2000['start_time_local'] = '2024-04-03 00:00:00'\n",
        "#df_1000_2000.to_parquet(PATH+'1000_2000_data.parquet', engine='pyarrow')\n",
        "\n",
        "#df_2000 = df_parquet[2000:]\n",
        "\n",
        "#df_2000['start_time_local'] = '2024-04-05 00:00:00'\n",
        "#df_2000.to_parquet(PATH+'2000_3000_data.parquet', engine='pyarrow')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = list(df_parquet['in__text'])\n",
        "\n",
        "# Cantidad total de documentos\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StopWords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "SPANISH_STOPWORDS = list(pd.read_csv(PATH+'spanish_stop_words.csv' )['stopwords'].values)\n",
        "SPANISH_STOPWORDS_SPECIAL = list(pd.read_csv(PATH+'spanish_stop_words_spec.csv' )['stopwords'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" import csv\n",
        "# Guardar la lista en un archivo CSV\n",
        "with open(PATH+\"spanish_stop_words_spec.csv\", mode='w', newline='', encoding='utf-8') as archivo:\n",
        "    escritor = csv.writer(archivo)\n",
        "    escritor.writerow(['stopwords'])\n",
        "    for stopword in SPANISH_STOPWORDS_SPECIAL:\n",
        "        escritor.writerow([stopword]) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER - Named Entity Recognition\n",
        "Obtener entidades de las noticias "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el modelo de spaCy para español\n",
        "spa = spacy.load(\"es_core_news_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" # Cargar o saltar carga y procesar celda inferior\n",
        "with open(PATH+f'modelos/entities{chunk}.json', 'r') as json_file:\n",
        "    entities = json.load(json_file)\n",
        "\n",
        "with open(PATH+f'modelos/entities_spa{chunk}.json', 'r') as json_file:\n",
        "    entities_spa = json.load(json_file)\n",
        "\n",
        "with open(PATH+f'modelos/keywords_spa{chunk}.json', 'r') as json_file:\n",
        "    keywords_spa = json.load(json_file) \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:36<00:00, 10.39it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar entidades y keywords para todos los documentos usando spaCy\n",
        "entities_spa = []\n",
        "keywords_spa = []\n",
        "for doc in tqdm(data):\n",
        "    extract = spa(doc)\n",
        "    entities_spa.append([(ent.text, ent.label_) for ent in extract.ents])\n",
        "    keywords_spa.append([(ext.text, ext.pos_) for ext in extract])    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 1132.50it/s]\n"
          ]
        }
      ],
      "source": [
        "# Procesamiento de entidades encontradas\n",
        "entities = []\n",
        "original_entities = []\n",
        "word_count = {}\n",
        "for item in tqdm(entities_spa):\n",
        "    for ent in item:\n",
        "        if ent[1] == 'PER' or ent[1] == 'ORG' or ent[1] == 'LOC':\n",
        "            words = str(ent[0]).lower()\n",
        "            words = clean_all([words], accents=False)[0]\n",
        "            words = \" \".join(words.split())\n",
        "            if len(words) > 2 and len(words.split()) <= 3:   # valida mas de una letra cada palabra & maximo 3 palabras por token\n",
        "                add = True\n",
        "                \n",
        "                for token in words.split():\n",
        "                    if token.isalpha():\n",
        "                        if token in SPANISH_STOPWORDS or token in SPANISH_STOPWORDS_SPECIAL:\n",
        "                            add = False\n",
        "                        \n",
        "                    elif token.isnumeric():\n",
        "                        if len(token) > 5:\n",
        "                            add = False\n",
        "                    else:\n",
        "                        if token in SPANISH_STOPWORDS_SPECIAL:\n",
        "                            add = False\n",
        "\n",
        "                    if add:\n",
        "                        if words not in word_count:\n",
        "                            word_count[words] = {'count': 0, 'original': ent[0]}\n",
        "                        else:\n",
        "                            word_count[words]['count'] += 1      \n",
        "\n",
        "    # Ordenar el diccionario por el valor del conteo en orden descendente\n",
        "    sorted_word_count = dict(sorted(word_count.items(), key=lambda item: item[1]['count'], reverse=True))  \n",
        "\n",
        "    word_count = {}\n",
        "\n",
        "    pre_original_entities = [value['original'] for _, value in sorted_word_count.items()]\n",
        "                             \n",
        "    # Crear la lista de entidades procesadas por noticia (entrenamiento)\n",
        "    pre_entities = [key for key, _ in sorted_word_count.items()] # if _['count'] > 1]\n",
        "\n",
        "    # Obtener las últimas palabras de las entidades con más de una palabra\n",
        "    ultimas_palabras = [ent.split()[-1] for ent in pre_entities if len(ent.split()) > 1]\n",
        "\n",
        "    # Filtrar si las últimas palabras coinciden con alguna unica palabra\n",
        "    filtro_ulp = [ent for ent in pre_entities if not (len(ent.split()) == 1 and ent in ultimas_palabras)]\n",
        "\n",
        "    # Obtener las palabras únicas\n",
        "    unicas_palabras = [ ent for ent in filtro_ulp if len(ent.split()) == 1]\n",
        "\n",
        "    # Filtrar si las palabras únicas coinciden con las una entidad con más de una palabra\n",
        "    filtro_unp = [ ent for ent in filtro_ulp if not ent in unicas_palabras ]\n",
        "\n",
        "    umbral=10\n",
        "    # entidades para entrenar\n",
        "    entities.append( filtro_unp[:umbral] )\n",
        "    original_entities.append([pre for pre in pre_original_entities if pre.lower() in filtro_unp[:umbral]])\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "211"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encontrar la posicion en el df segun su indice\n",
        "df_parquet.index.get_loc(105640350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>maría corina machado</td>\n",
              "      <td>nicolás maduro</td>\n",
              "      <td>corina yoris</td>\n",
              "      <td>gerardo blyde pérez</td>\n",
              "      <td>consejo nacional electoral</td>\n",
              "      <td>edmundo gonzález urrutia</td>\n",
              "      <td>plataforma unitaria democrática</td>\n",
              "      <td>jonas gahr</td>\n",
              "      <td>embajada argentina</td>\n",
              "      <td>vente venezuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ptsd tept</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>marcos brindicci</td>\n",
              "      <td>reino unido</td>\n",
              "      <td>erica sandberg</td>\n",
              "      <td>courtney alev</td>\n",
              "      <td>teri williams</td>\n",
              "      <td>oneunited bank</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>diego maradona</td>\n",
              "      <td>guillermo coppola</td>\n",
              "      <td>enrique pinti</td>\n",
              "      <td>juanito belmonte</td>\n",
              "      <td>calle libertad</td>\n",
              "      <td>salsa criolla</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>río negro</td>\n",
              "      <td>alberto weretilneck</td>\n",
              "      <td>gustavo melella</td>\n",
              "      <td>rolando figueroa</td>\n",
              "      <td>puerto madryn</td>\n",
              "      <td>congreso extraordinario</td>\n",
              "      <td>tierra del fuego</td>\n",
              "      <td>ate y upcn</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>javier milei</td>\n",
              "      <td>gustavo petro</td>\n",
              "      <td>gobierno colombiano</td>\n",
              "      <td>camilo romero</td>\n",
              "      <td>gobierno argentino</td>\n",
              "      <td>diana mondino</td>\n",
              "      <td>república argentina</td>\n",
              "      <td>cancillerías de colombia</td>\n",
              "      <td>república de colombia</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>luis caputo</td>\n",
              "      <td>emmanuel álvarez agis</td>\n",
              "      <td>marina dal poggetto</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>javier milei</td>\n",
              "      <td>marcelo hacklander</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>mari rantanen</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        0                      1                    2  \\\n",
              "0    maría corina machado         nicolás maduro         corina yoris   \n",
              "1                    None                   None                 None   \n",
              "2               ptsd tept                   None                 None   \n",
              "3        marcos brindicci            reino unido       erica sandberg   \n",
              "4          diego maradona      guillermo coppola        enrique pinti   \n",
              "..                    ...                    ...                  ...   \n",
              "995             río negro    alberto weretilneck      gustavo melella   \n",
              "996          javier milei          gustavo petro  gobierno colombiano   \n",
              "997           luis caputo  emmanuel álvarez agis  marina dal poggetto   \n",
              "998          javier milei     marcelo hacklander                 None   \n",
              "999         mari rantanen                   None                 None   \n",
              "\n",
              "                       3                           4  \\\n",
              "0    gerardo blyde pérez  consejo nacional electoral   \n",
              "1                   None                        None   \n",
              "2                   None                        None   \n",
              "3          courtney alev               teri williams   \n",
              "4       juanito belmonte              calle libertad   \n",
              "..                   ...                         ...   \n",
              "995     rolando figueroa               puerto madryn   \n",
              "996        camilo romero          gobierno argentino   \n",
              "997                 None                        None   \n",
              "998                 None                        None   \n",
              "999                 None                        None   \n",
              "\n",
              "                            5                                6  \\\n",
              "0    edmundo gonzález urrutia  plataforma unitaria democrática   \n",
              "1                        None                             None   \n",
              "2                        None                             None   \n",
              "3              oneunited bank                             None   \n",
              "4               salsa criolla                             None   \n",
              "..                        ...                              ...   \n",
              "995   congreso extraordinario                 tierra del fuego   \n",
              "996             diana mondino              república argentina   \n",
              "997                      None                             None   \n",
              "998                      None                             None   \n",
              "999                      None                             None   \n",
              "\n",
              "                            7                      8                9  \n",
              "0                  jonas gahr     embajada argentina  vente venezuela  \n",
              "1                        None                   None             None  \n",
              "2                        None                   None             None  \n",
              "3                        None                   None             None  \n",
              "4                        None                   None             None  \n",
              "..                        ...                    ...              ...  \n",
              "995                ate y upcn                   None             None  \n",
              "996  cancillerías de colombia  república de colombia             None  \n",
              "997                      None                   None             None  \n",
              "998                      None                   None             None  \n",
              "999                      None                   None             None  \n",
              "\n",
              "[1000 rows x 10 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_entities = pd.DataFrame(entities)\n",
        "df_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'modelos/entities{chunk}.json', 'w') as file:\n",
        "    json.dump(entities, file)\n",
        "\n",
        "# Grabar\n",
        "with open(PATH+f'modelos/entities_spa{chunk}.json', 'w') as file:\n",
        "    json.dump(entities_spa, file)\n",
        "\n",
        "# Grabar\n",
        "with open(PATH+f'modelos/keywords_spa{chunk}.json', 'w') as file:\n",
        "    json.dump(keywords_spa, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keywords\n",
        "Obtener palabras clave de las noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [00:00, 3538.88it/s]\n"
          ]
        }
      ],
      "source": [
        "# Procesamiento de keywords encontradas como 'MISC'\n",
        "keywords = []\n",
        "original_keywords = []\n",
        "word_count = {}\n",
        "for i, item in tqdm(enumerate(entities_spa)):\n",
        "    for ent in item:\n",
        "        if ent[1] == 'MISC':\n",
        "            words = str(ent[0]).lower()\n",
        "            words = clean_all([words], accents=False)[0]\n",
        "            #if len(entities[i]) < 5: # Si se encontraron menos de 5 entidades, obtenemos keywords.\n",
        "            if len(words) > 2 and len(words.split()) < 2:\n",
        "                add = True\n",
        "                for token in words.split():\n",
        "                    if token.isalpha():\n",
        "                        if token in SPANISH_STOPWORDS or token in SPANISH_STOPWORDS_SPECIAL:\n",
        "                            add = False\n",
        "                    elif token.isnumeric():\n",
        "                        if len(token) > 5:\n",
        "                            add = False\n",
        "                    else:\n",
        "                        if token in SPANISH_STOPWORDS_SPECIAL:\n",
        "                            add = False\n",
        "\n",
        "                    if add:\n",
        "                        if words not in word_count:\n",
        "                            word_count[words] = {'count': 0, 'original': ent[0]}\n",
        "                        else:\n",
        "                            word_count[words]['count'] += 1   \n",
        "\n",
        "\n",
        "    # Ordenar el diccionario por el valor del conteo en orden descendente\n",
        "    \n",
        "    sorted_word_count = dict(sorted(word_count.items(), key=lambda item: item[1]['count'], reverse=True))  \n",
        "    word_count = {}\n",
        "\n",
        "    # Crear la lista de entidades procesadas por noticia (para guardar en DB)\n",
        "    original_keywords.append([value['original'] for _, value in sorted_word_count.items()]) # and value['count'] > 1] )\n",
        "                                \n",
        "    # Crear la lista de entidades procesadas por noticia (entrenamiento)\n",
        "    pre_keywords = [key for key, _ in sorted_word_count.items()]   # and _['count'] > 1]\n",
        "                        \n",
        "    # entidades para entrenar (seleccionamos hasta 5 primeras)\n",
        "    keywords.append( pre_keywords[:5] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['adentro']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords[211]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>newsweek</td>\n",
              "      <td>salidas”</td>\n",
              "      <td>ropa”</td>\n",
              "      <td>tarjetas</td>\n",
              "      <td>tiktok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tumberos</td>\n",
              "      <td>sorprendidos</td>\n",
              "      <td>disputas</td>\n",
              "      <td>quédense</td>\n",
              "      <td>diez</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>inflación</td>\n",
              "      <td>iprofesional</td>\n",
              "      <td>precios</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>dnu</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>registrar</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0             1         2         3       4\n",
              "0         None          None      None      None    None\n",
              "1         None          None      None      None    None\n",
              "2         None          None      None      None    None\n",
              "3     newsweek      salidas”     ropa”  tarjetas  tiktok\n",
              "4     tumberos  sorprendidos  disputas  quédense    diez\n",
              "..         ...           ...       ...       ...     ...\n",
              "995       None          None      None      None    None\n",
              "996       None          None      None      None    None\n",
              "997  inflación  iprofesional   precios      None    None\n",
              "998        dnu          None      None      None    None\n",
              "999  registrar          None      None      None    None\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_keywords = pd.DataFrame(keywords)\n",
        "df_keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keyboards with neighboards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('fotógrafo', 9),\n",
              " ('agua', 5),\n",
              " ('playa', 4),\n",
              " ('costa', 4),\n",
              " ('olas', 4),\n",
              " ('mar', 4),\n",
              " ('años', 4),\n",
              " ('revista', 4),\n",
              " ('día', 3),\n",
              " ('amiga', 3)]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prueba ejemplo\n",
        "doc = 211\n",
        "\n",
        "# Obtenemos las keywords 'NOUN' mas frecuentes\n",
        "nouns = []\n",
        "for token in keywords_spa[doc]:\n",
        "    if token[1] == 'NOUN':\n",
        "        nouns.append(token[0])\n",
        "\n",
        "count_nouns = Counter(nouns)\n",
        "\n",
        "count_nouns.most_common()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('volver', 3),\n",
              " ('tiró', 3),\n",
              " ('llegar', 3),\n",
              " ('rescatarla', 2),\n",
              " ('desapareció', 2),\n",
              " ('tomar', 2),\n",
              " ('empezaron', 2),\n",
              " ('convirtió', 2),\n",
              " ('contó', 2),\n",
              " ('ocurrió', 1)]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prueba ejemplo\n",
        "doc = 211\n",
        "\n",
        "# Obtenemos las keywords 'VERB' mas frecuentes\n",
        "verbs = []\n",
        "for token in keywords_spa[doc]:\n",
        "    if token[1] == 'VERB':\n",
        "        verbs.append(token[0])\n",
        "\n",
        "count_verbs = Counter(verbs)\n",
        "\n",
        "count_verbs.most_common()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('el', 'DET'),\n",
              " ('tragedia', 'NOUN'),\n",
              " ('ocurrir', 'VERB'),\n",
              " ('el', 'DET'),\n",
              " ('primero', 'ADJ'),\n",
              " ('día', 'NOUN'),\n",
              " ('del', 'ADP'),\n",
              " ('año', 'NOUN'),\n",
              " ('en', 'ADP'),\n",
              " ('el', 'DET')]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pobar un documento ( resultados lematizados )\n",
        "keywords_spa_n = []\n",
        "extract = spa(data[doc])\n",
        "keywords_spa_n.append([(ext.lemma_, ext.pos_) for ext in extract])\n",
        "keywords_spa_n[0][:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('La', 'DET'),\n",
              " ('tragedia', 'NOUN'),\n",
              " ('ocurrió', 'VERB'),\n",
              " ('el', 'DET'),\n",
              " ('primer', 'ADJ'),\n",
              " ('día', 'NOUN'),\n",
              " ('del', 'ADP'),\n",
              " ('año', 'NOUN'),\n",
              " ('en', 'ADP'),\n",
              " ('la', 'DET')]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Resultados sin lematizar\n",
        "extract = spa(data[doc])\n",
        "tokens_and_labels = [(token.text, token.pos_) for token in extract if token.is_alpha]\n",
        "tokens_and_labels[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a function to get all two-word combinations\n",
        "def get_bigrams(word_list, number_consecutive_words=2):\n",
        "    \n",
        "    ngrams = []\n",
        "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
        "    \n",
        "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
        "    for word_index in range(adj_length_of_word_list):\n",
        "        \n",
        "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
        "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
        "        \n",
        "        #Append this word combo to the master list \"ngrams\"\n",
        "        ngrams.append(ngram)\n",
        "        \n",
        "    return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('La', 'DET'), ('tragedia', 'NOUN')],\n",
              " [('tragedia', 'NOUN'), ('ocurrió', 'VERB')],\n",
              " [('ocurrió', 'VERB'), ('el', 'DET')],\n",
              " [('el', 'DET'), ('primer', 'ADJ')],\n",
              " [('primer', 'ADJ'), ('día', 'NOUN')],\n",
              " [('día', 'NOUN'), ('del', 'ADP')],\n",
              " [('del', 'ADP'), ('año', 'NOUN')],\n",
              " [('año', 'NOUN'), ('en', 'ADP')],\n",
              " [('en', 'ADP'), ('la', 'DET')],\n",
              " [('la', 'DET'), ('playa', 'NOUN')]]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigrams = get_bigrams(tokens_and_labels)\n",
        "bigrams[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# return the most frequent words that appear next to a particular keyword\n",
        "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
        "    \n",
        "    neighbor_words = []\n",
        "    keyword = keyword.lower()\n",
        "    \n",
        "    for bigram in bigrams:\n",
        "        \n",
        "        #Extract just the lowercased words (not the labels) for each bigram\n",
        "        words = [word.lower() for word, label in bigram]        \n",
        "        \n",
        "        #Check to see if keyword is in the bigram\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            for word, label in bigram:\n",
        "                \n",
        "                #Now focus on the neighbor word, not the keyword\n",
        "                if word.lower() != keyword:\n",
        "                    #If the neighbor word matches the right pos_label, append it to the master list\n",
        "                    if label == pos_label or pos_label == None:\n",
        "                        if idx == 0:\n",
        "                            neighbor_words.append(\" \".join([keyword, word.lower()]))\n",
        "                        else:\n",
        "                            neighbor_words.append(\" \".join([word.lower(), keyword]))\n",
        "                    \n",
        "    return Counter(neighbor_words).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "[('agua junto', 1)]\n",
            "[('playa junto', 1)]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('primer día', 1), ('día siguiente', 1)]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('gritos desesperados', 2)]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('grandísimo amigo', 1)]\n",
            "[('año nuevo', 1)]\n",
            "[('chacra marítima', 1)]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('reporteros gráficos', 1)]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('médico forense', 1)]\n",
            "[('paro cardíaco', 1)]\n",
            "[]\n",
            "[('fuerte diferencia', 1), ('diferencia térmica', 1)]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('gran reportero', 1), ('reportero gráfico', 1)]\n",
            "[]\n",
            "[]\n",
            "[('reacción heroica', 1)]\n",
            "[]\n",
            "[]\n",
            "[('condición humana', 1)]\n"
          ]
        }
      ],
      "source": [
        "for word in count_nouns.most_common():\n",
        "    print(get_neighbor_words(word[0], bigrams, pos_label='ADJ'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funcion completa para keywords with neighboards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keywords_with_neighboards(keywords_spa, POS_1='NOUN', POS_2='ADJ'):\n",
        "    \"\"\"\n",
        "    Funcion que devuelve dos listas:\n",
        "    - lista de keywords with neighboards (segun argumentos POS_1 y POS_2)\n",
        "    - lista de keywords mas frecuentes (segun argumentos POS_1 y POS_2)\n",
        "    \"\"\"\n",
        "\n",
        "    doc_kwn = []\n",
        "    commons = []\n",
        "    for keywords in keywords_spa:\n",
        "    \n",
        "        # Obtenemos las keywords del tipo (Universal Dependences) mas frecuentes de cada doc (spaCy format)\n",
        "        words = []\n",
        "        for k_spa in keywords:\n",
        "            if k_spa[1] == POS_1:\n",
        "                words.append(k_spa[0])\n",
        "\n",
        "        cont_words = Counter(words)\n",
        "\n",
        "        common = cont_words.most_common()\n",
        "        commons.append( [com for com in common if com[1] > 1] )\n",
        "\n",
        "        # Calcular un umbral de corte (en repeticiones) para los keywords obtenidos\n",
        "            ## suma de todos los valores\n",
        "        valores = [valor for _, valor in common]\n",
        "\n",
        "            ## Calcular los pesos como proporcionales a los valores mismos\n",
        "        pesos = np.array(valores) / np.sum(valores)\n",
        "\n",
        "            ## Calcular el umbral ponderado, valor 2 o superior ( debe repetirse la keyword al menos una vez )\n",
        "        threshold = max(2, round(np.sum(np.array(valores) * pesos),4))\n",
        "\n",
        "\n",
        "        # Obtenemos los bigramas del doc        \n",
        "        tokens_and_labels = [(token[0], token[1]) for token in keywords if token[0].isalpha()]\n",
        "\n",
        "        bigrams = get_bigrams(tokens_and_labels)\n",
        "\n",
        "        keywords_neighbor = []\n",
        "        for item_common in common:\n",
        "            if item_common[1] >= threshold or len(keywords_neighbor) < 6: # corte por umbral o menor a 6\n",
        "                \n",
        "                kwn = get_neighbor_words(item_common[0], bigrams, pos_label=POS_2)\n",
        "                if kwn != []:\n",
        "                    keywords_neighbor.append( kwn )\n",
        "\n",
        "        sorted_keywords_neighbor = sorted([item for sublist in keywords_neighbor for item in sublist ], key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        doc_kwn.append(sorted_keywords_neighbor)\n",
        "\n",
        "    return doc_kwn, commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "k_w_n, common = keywords_with_neighboards(keywords_spa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['gritos desesperados']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# filtramos que al menos se repitan una vez\n",
        "filtered_k_w_n = [ [tupla[0] for tupla in sublista if tupla[1] > 1] for sublista in k_w_n ]\n",
        "filtered_k_w_n[211]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('fotógrafo', 9),\n",
              " ('agua', 5),\n",
              " ('playa', 4),\n",
              " ('costa', 4),\n",
              " ('olas', 4),\n",
              " ('mar', 4),\n",
              " ('años', 4),\n",
              " ('revista', 4),\n",
              " ('día', 3),\n",
              " ('amiga', 3),\n",
              " ('ayuda', 3),\n",
              " ('conductor', 3),\n",
              " ('héroe', 3),\n",
              " ('tragedia', 2),\n",
              " ('hija', 2),\n",
              " ('guardia', 2),\n",
              " ('gritos', 2),\n",
              " ('equipos', 2),\n",
              " ('arena', 2),\n",
              " ('chica', 2),\n",
              " ('cuatriciclo', 2),\n",
              " ('personas', 2),\n",
              " ('hombre', 2),\n",
              " ('amigo', 2)]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common[211]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_common = [ [tupla[0] for i, tupla in enumerate(sublista) if i < 6] for sublista in common ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['fotógrafo', 'agua', 'playa', 'costa', 'olas', 'mar']"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_common[211]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BOW - Armado del vocabulario con las entidades y keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6251"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Unificar Entities + Keywords + Keywords with neighboards\n",
        "vocab = list(set().union(*entities, *keywords, *filtered_k_w_n, *common[:10]))\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'emmanuel soliño'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab[211]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar vocabulario\n",
        "with open(PATH+f'modelos/vocabulary{chunk}.json', 'w') as file:\n",
        "    json.dump(vocab, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar noticias en la base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configurar  batch_size = ( ej.: 5000 ) si se supera el limite 100MB en elasticsearch por operacion\n",
        "index_name = 'news'\n",
        "bulk_data = []\n",
        "\n",
        "# Unificar Keywords + Keywords with neighboards\n",
        "keywords_plus = [ list(set(keywords[i]+filtered_k_w_n[i])) for i in range(len(entities)) ]\n",
        "\n",
        "for idx, text_news in tqdm(enumerate(data)):\n",
        "    doc = {\n",
        "        'index': {\n",
        "            '_index': index_name,\n",
        "            '_id': int(df_parquet.index[idx])\n",
        "        }\n",
        "    }\n",
        "    reg = {\n",
        "        'title': str(df_parquet.iloc[idx].in__title),\n",
        "        'news' : str(text_news), \n",
        "        'author': str(df_parquet.iloc[idx]['Author Name']),\n",
        "        'topics': {},\n",
        "        'vector': None,\n",
        "        'keywords' : keywords_plus[idx],\n",
        "        'entities' : original_entities[idx],\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'process': False\n",
        "    }\n",
        "    bulk_data.append(json.dumps(doc))\n",
        "    bulk_data.append(json.dumps(reg))\n",
        "\n",
        "# Convertir la lista en un solo string separado por saltos de línea\n",
        "bulk_request_body = '\\n'.join(bulk_data) + '\\n'\n",
        "\n",
        "# Enviar la solicitud bulk\n",
        "response = os_client.bulk(body=bulk_request_body)\n",
        "\n",
        "if response['errors']:\n",
        "    print(\"Errores encontrados al insertar los documentos\")\n",
        "else:\n",
        "    print(\"Documentos insertados correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nota:\n",
        "- por cada documento se van a guardar las entidades que al menos se repitan una vez (mayor frecuencia)\n",
        "- se utilizarán todas las entidades guardadas de todos los documentos como vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keywords de dataframe: [array(['sitio oficial', 'membresía', 'suscripción', 'dólar',\n",
            "        'plan deseado'], dtype=object)                       ]\n",
            "Entities de dataframe: [array(['Netflix'], dtype=object)]\n",
            "--------------------------------------------------------------------------------\n",
            "Fila: 886\n",
            "Entities calculadas: ['netflix', 'gobierno nacional', 'pais', 'ganancias', '―celular', 'nacion']\n",
            "Keywords calculadas: []\n",
            "Keywords neighboards calculadas: ['dólar oficial', 'sitio oficial']\n"
          ]
        }
      ],
      "source": [
        "def funcion_aux(ID):\n",
        "    keywords_df = df_parquet[df_parquet.index==ID]['Keyword Name'].values\n",
        "    entities_df = df_parquet[df_parquet.index==ID]['Entity Name'].values\n",
        "    fila = df_parquet.index.get_loc(ID)\n",
        "    print(f\"Keywords de dataframe: {keywords_df}\")\n",
        "    print(f\"Entities de dataframe: {entities_df}\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Fila: {fila}\")\n",
        "    print(f\"Entities calculadas: {entities[fila]}\")\n",
        "    print(f\"Keywords calculadas: {keywords[fila]}\")\n",
        "    print(f\"Keywords neighboards calculadas: {filtered_k_w_n[fila]}\")\n",
        "\n",
        "funcion_aux(105579385)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGxTNO4yTgb9k2r4ffbTN0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
