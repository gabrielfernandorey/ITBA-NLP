{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielfernandorey/ITBA-NLP/blob/main/ITBA_nlp01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumen del problema\n",
        "\n",
        "- Calcular los tópicos de portales de noticias que se reciben \n",
        "- Frecuencia del cálculo de tópicos: diaria\n",
        "- Colección de noticias: diariamente, en lotes o de a un texto.\n",
        "- Identificar tópicos, entidades, keywords y análisis de sentimiento.\n",
        "\n",
        "### Datos\n",
        "- Se reciben las noticias con formato: Titulo, Texto, Fecha, Entidades, Keywords\n",
        "\n",
        "### Tareas\n",
        "- Modelo de detección de tópicos diario utilizando embeddings\n",
        "- Definir un criterio de agrupación de tópicos aplicado al mismo día y entre distintos días (merging)\n",
        "- Almacenar los embeddings de tópicos en una base de datos vectorial\n",
        "- Modelo de datos dado: \n",
        "    - Id del tópico\n",
        "    - Nombre del tópico\n",
        "    - Keywords\n",
        "    - Embbeding\n",
        "    - Fecha de creación\n",
        "    - Fecha de entrenamiento inicial\n",
        "    - Fecha de entrenamiento actualizada\n",
        "    - Umbral de detección\n",
        "    - Documento mas cercano\n",
        "---\n",
        "Tareas en esta notebook:\n",
        "- Inicializar la base de datos vectorial\n",
        "- Ingestar data\n",
        "- NER: Encontrar las entidades de cada documento\n",
        "- Limpiar data\n",
        "- Modelo: Armado del modelo BERTopic\n",
        "- Entrenamiento\n",
        "- Almacenamiento en base de datos vectorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NLP_01_data\n",
        "Esta notebook se utiliza para:\n",
        "- obtener un lote predefinido de las noticias de una fecha dada\n",
        "- se realiza el ejercicio de obtención de entities y keywords, mas allá de tenerlas provistas en el set de datos de origen.\n",
        "- se obtiene el vocabulario para utilizar en BERTopic\n",
        "\n",
        "Esta y las consecuentes notebooks son el desarrollo de base de procesos y funciones para la web app provista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "1e5d8d12-903f-4a10-ddd3-6d7d9ae83cc7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "from NLP_tools import clean_all\n",
        "from core.functions import *\n",
        "\n",
        "# -->> levantar la base antes de ejecutar\n",
        "from opensearch_data_model import os_client\n",
        "from opensearch_io import init_opensearch\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializamos la base vectorial\n",
        "- Se modifica el indice de la base \"Topic\" agregando referencias del documento mas cercano como el ID y el titulo\n",
        "- Se crea un nuevo indice para las noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indice Topic creado\n",
            "Indice News creado\n"
          ]
        }
      ],
      "source": [
        "# Inicialización de indices\n",
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:/Users/gabri/OneDrive/Machine Learning/Github/ITBA-NLP/data/'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Obtenemos los datos "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "cmp3cLLv28-T",
        "outputId": "2d83d8fc-9241-448a-98a1-6230eb29ce2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>title_ch</th>\n",
              "      <th>Asset Destination</th>\n",
              "      <th>media</th>\n",
              "      <th>impact</th>\n",
              "      <th>start_time_utc</th>\n",
              "      <th>start_time_local</th>\n",
              "      <th>entities_curated</th>\n",
              "      <th>entities</th>\n",
              "      <th>predicted_at_entities</th>\n",
              "      <th>entities_raw_transformers</th>\n",
              "      <th>entities_transformers</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>keywords</th>\n",
              "      <th>predicted_at_keywords</th>\n",
              "      <th>truncated_text</th>\n",
              "      <th>title_and_text</th>\n",
              "      <th>prediction_delay_predictions</th>\n",
              "      <th>prediction_delay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>114894323</td>\n",
              "      <td>Supremo recibe expediente de la Fiscalía colom...</td>\n",
              "      <td>http://infobae.com/america/agencias/2024/07/16...</td>\n",
              "      <td>Infobae</td>\n",
              "      <td>9642</td>\n",
              "      <td>2024-07-17 00:20:49</td>\n",
              "      <td>2024-07-16 21:20:49</td>\n",
              "      <td>[Fiscalía General de la Nación Colombia, Alian...</td>\n",
              "      <td>[Iván Name, Julián Peinado Ramírez, Andrés Cal...</td>\n",
              "      <td>2024-07-17 00:59:39.937401</td>\n",
              "      <td>[{'entities': [{'end': 7, 'entity_group': 'ORG...</td>\n",
              "      <td>[Supremo, Fiscalía colo, Newsroom Infobae Nuev...</td>\n",
              "      <td>Supremo recibe expediente de la Fiscalía colom...</td>\n",
              "      <td>16 Jul, 2024 Por  Newsroom Infobae Nuevo Bogot...</td>\n",
              "      <td>[los congresistas, fiscalía colombiana, falsed...</td>\n",
              "      <td>2024-07-17 01:05:59.174600</td>\n",
              "      <td>16 Jul, 2024 Por  Newsroom Infobae Nuevo Bogot...</td>\n",
              "      <td>Supremo recibe expediente de la Fiscalía colom...</td>\n",
              "      <td>0.105344</td>\n",
              "      <td>0.752826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    asset_id                                           title_ch  \\\n",
              "0  114894323  Supremo recibe expediente de la Fiscalía colom...   \n",
              "\n",
              "                                   Asset Destination    media  impact  \\\n",
              "0  http://infobae.com/america/agencias/2024/07/16...  Infobae    9642   \n",
              "\n",
              "       start_time_utc    start_time_local  \\\n",
              "0 2024-07-17 00:20:49 2024-07-16 21:20:49   \n",
              "\n",
              "                                    entities_curated  \\\n",
              "0  [Fiscalía General de la Nación Colombia, Alian...   \n",
              "\n",
              "                                            entities  \\\n",
              "0  [Iván Name, Julián Peinado Ramírez, Andrés Cal...   \n",
              "\n",
              "       predicted_at_entities  \\\n",
              "0 2024-07-17 00:59:39.937401   \n",
              "\n",
              "                           entities_raw_transformers  \\\n",
              "0  [{'entities': [{'end': 7, 'entity_group': 'ORG...   \n",
              "\n",
              "                               entities_transformers  \\\n",
              "0  [Supremo, Fiscalía colo, Newsroom Infobae Nuev...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Supremo recibe expediente de la Fiscalía colom...   \n",
              "\n",
              "                                                text  \\\n",
              "0  16 Jul, 2024 Por  Newsroom Infobae Nuevo Bogot...   \n",
              "\n",
              "                                            keywords  \\\n",
              "0  [los congresistas, fiscalía colombiana, falsed...   \n",
              "\n",
              "       predicted_at_keywords  \\\n",
              "0 2024-07-17 01:05:59.174600   \n",
              "\n",
              "                                      truncated_text  \\\n",
              "0  16 Jul, 2024 Por  Newsroom Infobae Nuevo Bogot...   \n",
              "\n",
              "                                      title_and_text  \\\n",
              "0  Supremo recibe expediente de la Fiscalía colom...   \n",
              "\n",
              "   prediction_delay_predictions  prediction_delay  \n",
              "0                      0.105344          0.752826  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read the parquet file \n",
        "\n",
        "# Desde Hugginface\n",
        "date_choice = '2024-07-17'  # formato aaaa-mm-dd\n",
        "path_file = f\"jganzabalseenka/news_{date_choice}_24hs\"\n",
        "dataset = load_dataset(path_file)\n",
        "df_parquet = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Local \"\"\" el formato del archivo parque debe ser fecha aaaammdd \"\n",
        "# date_choice = '2024-07-15' \n",
        "# path_file = f\"{date_choice}.parquet\"\n",
        "# df_parquet = pd.read_parquet(PATH+path_file)\n",
        "\n",
        "df_parquet.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16037"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cantidad total de documentos\n",
        "len(df_parquet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validar datos para la fecha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registros para la fecha 20240717 -> 11663 de un total de 16037\n"
          ]
        }
      ],
      "source": [
        "choice = \"\".join(date_choice.split('-'))\n",
        "df_parquet.sort_values(\"start_time_local\", ascending=True, inplace=True)\n",
        "df_date_filtered = df_parquet[df_parquet['start_time_local'].dt.date == pd.to_datetime(choice).date()]\n",
        "print(f\"Registros para la fecha {choice} -> {len(df_date_filtered)} de un total de {len(df_parquet)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtenemos un lote de N noticias por dia (para agilizar el procesamiento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>title_ch</th>\n",
              "      <th>Asset Destination</th>\n",
              "      <th>media</th>\n",
              "      <th>impact</th>\n",
              "      <th>start_time_utc</th>\n",
              "      <th>start_time_local</th>\n",
              "      <th>entities_curated</th>\n",
              "      <th>entities</th>\n",
              "      <th>predicted_at_entities</th>\n",
              "      <th>entities_raw_transformers</th>\n",
              "      <th>entities_transformers</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>keywords</th>\n",
              "      <th>predicted_at_keywords</th>\n",
              "      <th>truncated_text</th>\n",
              "      <th>title_and_text</th>\n",
              "      <th>prediction_delay_predictions</th>\n",
              "      <th>prediction_delay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12619</th>\n",
              "      <td>114944575</td>\n",
              "      <td>Estudiantes de La Plata y Central Córdoba van ...</td>\n",
              "      <td>http://dobleamarilla.com.ar/liga-/estudiantes-...</td>\n",
              "      <td>Doble Amarilla</td>\n",
              "      <td>1022</td>\n",
              "      <td>2024-07-17 12:42:29</td>\n",
              "      <td>2024-07-17 09:42:29</td>\n",
              "      <td>[Club Estudiantes de La Plata, Eduardo Domíngu...</td>\n",
              "      <td>[Eros Mancuso, Luciano Lollo, Belgrano, Gastón...</td>\n",
              "      <td>2024-07-17 12:49:00.219674</td>\n",
              "      <td>[{'entities': [{'end': 23, 'entity_group': 'OR...</td>\n",
              "      <td>[Estudiantes de La Plata, Central Córdoba, Est...</td>\n",
              "      <td>Estudiantes de La Plata y Central Córdoba van ...</td>\n",
              "      <td>Estudiantes de La Plata y Central Córdoba de S...</td>\n",
              "      <td>[octavos, eros, plata, estadio, monte maíz, pu...</td>\n",
              "      <td>2024-07-17 12:49:36.524530</td>\n",
              "      <td>Estudiantes de La Plata y Central Córdoba de S...</td>\n",
              "      <td>Estudiantes de La Plata y Central Córdoba van ...</td>\n",
              "      <td>0.010085</td>\n",
              "      <td>0.118757</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        asset_id                                           title_ch  \\\n",
              "12619  114944575  Estudiantes de La Plata y Central Córdoba van ...   \n",
              "\n",
              "                                       Asset Destination           media  \\\n",
              "12619  http://dobleamarilla.com.ar/liga-/estudiantes-...  Doble Amarilla   \n",
              "\n",
              "       impact      start_time_utc    start_time_local  \\\n",
              "12619    1022 2024-07-17 12:42:29 2024-07-17 09:42:29   \n",
              "\n",
              "                                        entities_curated  \\\n",
              "12619  [Club Estudiantes de La Plata, Eduardo Domíngu...   \n",
              "\n",
              "                                                entities  \\\n",
              "12619  [Eros Mancuso, Luciano Lollo, Belgrano, Gastón...   \n",
              "\n",
              "           predicted_at_entities  \\\n",
              "12619 2024-07-17 12:49:00.219674   \n",
              "\n",
              "                               entities_raw_transformers  \\\n",
              "12619  [{'entities': [{'end': 23, 'entity_group': 'OR...   \n",
              "\n",
              "                                   entities_transformers  \\\n",
              "12619  [Estudiantes de La Plata, Central Córdoba, Est...   \n",
              "\n",
              "                                                   title  \\\n",
              "12619  Estudiantes de La Plata y Central Córdoba van ...   \n",
              "\n",
              "                                                    text  \\\n",
              "12619  Estudiantes de La Plata y Central Córdoba de S...   \n",
              "\n",
              "                                                keywords  \\\n",
              "12619  [octavos, eros, plata, estadio, monte maíz, pu...   \n",
              "\n",
              "           predicted_at_keywords  \\\n",
              "12619 2024-07-17 12:49:36.524530   \n",
              "\n",
              "                                          truncated_text  \\\n",
              "12619  Estudiantes de La Plata y Central Córdoba de S...   \n",
              "\n",
              "                                          title_and_text  \\\n",
              "12619  Estudiantes de La Plata y Central Córdoba van ...   \n",
              "\n",
              "       prediction_delay_predictions  prediction_delay  \n",
              "12619                      0.010085          0.118757  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_news=os.environ.get('BATCH_NEWS', 1000)\n",
        "\n",
        "df_batch = df_date_filtered.sample(n=int(batch_news))\n",
        "data = list(df_batch['text'])\n",
        "df_batch.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar a disco el dataframe filtrado \n",
        "df_batch.to_parquet(PATH+choice+\".parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StopWords\n",
        "Se genera una lista especial de stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "SPANISH_STOPWORDS = list(pd.read_csv(PATH+'spanish_stop_words.csv' )['stopwords'].values)\n",
        "SPANISH_STOPWORDS_SPECIAL = list(pd.read_csv(PATH+'spanish_stop_words_spec.csv' )['stopwords'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' import csv\\n# Guardar la lista de stopwords especial en un archivo CSV\\nwith open(PATH+\"spanish_stop_words_spec.csv\", mode=\\'w\\', newline=\\'\\', encoding=\\'utf-8\\') as archivo:\\n    escritor = csv.writer(archivo)\\n    escritor.writerow([\\'stopwords\\'])\\n    for stopword in SPANISH_STOPWORDS_SPECIAL:\\n        escritor.writerow([stopword]) '"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" import csv\n",
        "# Guardar la lista de stopwords especial en un archivo CSV\n",
        "with open(PATH+\"spanish_stop_words_spec.csv\", mode='w', newline='', encoding='utf-8') as archivo:\n",
        "    escritor = csv.writer(archivo)\n",
        "    escritor.writerow(['stopwords'])\n",
        "    for stopword in SPANISH_STOPWORDS_SPECIAL:\n",
        "        escritor.writerow([stopword]) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER - Named Entity Recognition\n",
        "Obtener entidades de las noticias \n",
        "\n",
        "-   Nota: Aunque el dataset original provee keywords y entities, se realiza el proceso de generacion propia de keywords y entites y se utilizan para modelar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el modelo de spaCy para español\n",
        "spa = spacy.load(\"es_core_news_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar o saltar carga y procesar celda inferior\n",
        "#with open(PATH+f'modelos/entities_{str(choice)}.json', 'r') as json_file:\n",
        "#    entities = json.load(json_file)\n",
        "\n",
        "#with open(PATH+f'modelos/keywords_{str(choice)}.json', 'r') as json_file:\n",
        "#    keywords = json.load(json_file)\n",
        "\n",
        "#with open(PATH+f'modelos/vocabulary_{str(choice)}.json', 'r') as json_file:\n",
        "#    vocab = json.load(json_file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:34<00:00, 14.36it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar entidades para todos los documentos usando spaCy\n",
        "# se procesa utilizando un criterio de seleccion reflejado en el codigo\n",
        "\n",
        "entities = []\n",
        "for data_in in tqdm(data):\n",
        "\n",
        "    # Contabilizar palabras en doc original\n",
        "    normalized_text = re.sub(r'\\W+', ' ', data_in.lower())\n",
        "    words_txt_without_stopwords = [word for word in normalized_text.split() if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL]\n",
        "    words_txt_counter = Counter(words_txt_without_stopwords)\n",
        "    words_counter = {elemento: cuenta for elemento, cuenta in sorted(words_txt_counter.items(), key=lambda item:item[1], reverse=True) if cuenta > 1}\n",
        "\n",
        "    # Extraer entidades del doc segun atributos\n",
        "    extract = spa(data_in)\n",
        "    entidades_spacy = [(ent.text, ent.label_) for ent in extract.ents]\n",
        "    ent_select = [ent for ent in entidades_spacy if ent[1] == 'PER' or ent[1] == 'ORG' or ent[1] == 'LOC' ]\n",
        "\n",
        "    # Extraer entidades de \"maximo 3 palabras\"\n",
        "    ent_max_3 = [ent[0] for ent in ent_select if len(ent[0].split()) <= 3]\n",
        "    ent_clean = clean_all(ent_max_3, accents=False)\n",
        "    ent_unique = list(set([ word for word in ent_clean if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL] ))\n",
        "\n",
        "    ents_proc = {}\n",
        "    for ent in ent_unique:\n",
        "        \n",
        "        # Criterio de selección \n",
        "        weight = 0\n",
        "        for word in ent.split():\n",
        "            if word in words_counter:\n",
        "                weight += 1 /len(ent.split()) * words_counter[word]\n",
        "        \n",
        "        ents_proc[ent] = round(weight,4)\n",
        "\n",
        "    ents_proc_sorted = {k: v for k, v in sorted(ents_proc.items(), key=lambda item: item[1], reverse=True) if v > 0}\n",
        "\n",
        "    # Crear la lista preliminar de entidades procesadas por noticia \n",
        "    pre_entities = [key for key, _ in ents_proc_sorted.items()] \n",
        "\n",
        "    # Obtener las últimas palabras de cada entidad que tenga mas de una palabra por entidad\n",
        "    last_words = list(set([ent.split()[-1] for ent in pre_entities if len(ent.split()) > 1 ]))\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al final de una compuesta\n",
        "    pre_entities_without_last_word_equal = []\n",
        "    for idx, ent in enumerate(pre_entities):\n",
        "        if not (len(ent.split()) == 1 and ent in last_words):\n",
        "            pre_entities_without_last_word_equal.append(ent)\n",
        "\n",
        "    # Obtener las palabras únicas\n",
        "    unique_words = [ ent.split()[0] for ent in pre_entities_without_last_word_equal if len(ent.split()) > 1 ]\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al comienzo de una compuesta\n",
        "    pre_entities_without_first_word_equal = []\n",
        "    for idx, ent in enumerate(pre_entities_without_last_word_equal):\n",
        "        if not (len(ent.split()) == 1 and ent in unique_words):\n",
        "            pre_entities_without_first_word_equal.append(ent)\n",
        "\n",
        "    # obtener entidades filtradas\n",
        "    if len(pre_entities_without_first_word_equal) > 10:\n",
        "        umbral = 10 + (len(pre_entities_without_first_word_equal)-10) // 2\n",
        "        filter_entities = pre_entities_without_first_word_equal[:umbral] \n",
        "    else:\n",
        "        filter_entities = pre_entities_without_first_word_equal[:10]\n",
        "\n",
        "    pre_original_entities = []\n",
        "    # capturar las entidades en formato original\n",
        "    for ent in filter_entities:\n",
        "        pre_original_entities.append([elemento for elemento in ent_max_3 if elemento.lower() == ent.lower()])\n",
        "\n",
        "    sort_original_entities = sorted(pre_original_entities, key=len, reverse=True)\n",
        "    \n",
        "    try:\n",
        "        entities.append( [ent[0] for ent in sort_original_entities if ent] ) \n",
        "    except Exception as e:\n",
        "        entities.append([])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'preproc_notebook/entities_{str(choice)}.json', 'w') as file:\n",
        "    json.dump(entities, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keywords\n",
        "Obtener palabras clave de las noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:30<00:00, 16.65it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar keywords para todos los documentos usando spaCy\n",
        "\n",
        "keywords_spa = []\n",
        "for doc in tqdm(data):\n",
        "    extract = spa(doc)\n",
        "    keywords_spa.append([(ext.text, ext.pos_) for ext in extract])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keyboards with neighboards\n",
        "- Se seleccionan keywords unigrama y bigrama mediante la funcion keywords_with_neighboards(), que a su vez llama a las funciones get_bigrams() y get_neighbor_words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funcion para obtener keywords con combinaciones de bigramas\n",
        "def get_bigrams(word_list, number_consecutive_words=2):\n",
        "    \n",
        "    ngrams = []\n",
        "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
        "    \n",
        "    for word_index in range(adj_length_of_word_list):\n",
        "        \n",
        "        # Indexar la lista \n",
        "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
        "        \n",
        "        # Agregar a la lista de \"ngrams\"\n",
        "        ngrams.append(ngram)\n",
        "        \n",
        "    return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# devolver las palabras más frecuentes que aparecen junto a una palabra clave en particular\n",
        "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
        "    \n",
        "    neighbor_words = []\n",
        "    keyword = keyword.lower()\n",
        "    \n",
        "    for bigram in bigrams:\n",
        "        \n",
        "        # Extrae solo las palabras en minúsculas (no las etiquetas) para cada bigrama\n",
        "        words = [word.lower() for word, label in bigram]        \n",
        "        \n",
        "        # Comprueba si la palabra clave está en el bigram\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            for word, label in bigram:\n",
        "                \n",
        "                #Ahora nos centramos en la palabra vecina, no en la palabra clave\n",
        "                if word.lower() != keyword:\n",
        "                    #Si la palabra vecina coincide con la pos_label correcta, agregarla a la lista maestra\n",
        "                    if label == pos_label or pos_label == None:\n",
        "                        if idx == 0:\n",
        "                            neighbor_words.append(\" \".join([keyword, word.lower()]))\n",
        "                        else:\n",
        "                            neighbor_words.append(\" \".join([word.lower(), keyword]))\n",
        "                    \n",
        "    return Counter(neighbor_words).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keywords_with_neighboards(keywords_spa, POS_1='NOUN', POS_2='ADJ'):\n",
        "    \"\"\"\n",
        "    Funcion que devuelve dos listas:\n",
        "    - lista de keywords with neighboards (segun argumentos POS_1 y POS_2)\n",
        "    - lista de keywords mas frecuentes (segun argumentos POS_1 y POS_2)\n",
        "    \"\"\"\n",
        "\n",
        "    doc_kwn = []\n",
        "    commons = []\n",
        "    for keywords in keywords_spa:\n",
        "    \n",
        "        # Obtenemos las keywords del tipo (Universal Dependences) mas frecuentes de cada doc (spaCy format)\n",
        "        words = []\n",
        "        for k_spa in keywords:\n",
        "            if k_spa[1] == POS_1:\n",
        "                words.append(k_spa[0])\n",
        "\n",
        "        cont_words = Counter(words)\n",
        "\n",
        "        common = cont_words.most_common()\n",
        "        commons.append( [com for com in common if com[1] > 1] )\n",
        "\n",
        "        # Calcular un umbral de corte (en repeticiones) para los keywords obtenidos\n",
        "            ## suma de todos los valores\n",
        "        valores = [valor for _, valor in common]\n",
        "\n",
        "            ## Calcular los pesos como proporcionales a los valores mismos\n",
        "        pesos = np.array(valores) / np.sum(valores)\n",
        "\n",
        "            ## Calcular el umbral ponderado, valor 2 o superior ( debe repetirse la keyword al menos una vez )\n",
        "        threshold = max(2, round(np.sum(np.array(valores) * pesos),4))\n",
        "\n",
        "\n",
        "        # Obtenemos los bigramas del doc        \n",
        "        tokens_and_labels = [(token[0], token[1]) for token in keywords if token[0].isalpha()]\n",
        "\n",
        "        bigrams = get_bigrams(tokens_and_labels)\n",
        "\n",
        "        keywords_neighbor = []\n",
        "        for item_common in common:\n",
        "            if item_common[1] >= threshold or len(keywords_neighbor) < 6: # corte por umbral o menor a 6\n",
        "                \n",
        "                kwn = get_neighbor_words(item_common[0], bigrams, pos_label=POS_2)\n",
        "                if kwn != []:\n",
        "                    keywords_neighbor.append( kwn )\n",
        "\n",
        "        sorted_keywords_neighbor = sorted([item for sublist in keywords_neighbor for item in sublist ], key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        doc_kwn.append(sorted_keywords_neighbor)\n",
        "\n",
        "    return doc_kwn, commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obtenemos keywords with neighboards y keywords mas frecuentes\n",
        "k_w_n, keyword_single = keywords_with_neighboards(keywords_spa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('redes sociales', 2),\n",
              " ('contrato legal', 1),\n",
              " ('contrato ligado', 1),\n",
              " ('nuevo contrato', 1),\n",
              " ('momento ideal', 1),\n",
              " ('manera rápida', 1),\n",
              " ('importante marca', 1),\n",
              " ('contenido erótico', 1),\n",
              " ('alto contenido', 1),\n",
              " ('segunda vez', 1),\n",
              " ('reglas incumplidas', 1)]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# muestra de keywords with neighboards\n",
        "k_w_n[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filtramos las que al menos se repiten una vez\n",
        "filtered_k_w_n = [ [tupla[0] for tupla in sublista if tupla[1] > 1] for sublista in k_w_n ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['empresariado sanmartinense',\n",
              " 'sector laboral',\n",
              " 'verano pasado',\n",
              " 'empresarios hoteleros',\n",
              " 'reclamo justo',\n",
              " 'trabajadores hoteleros']"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# muestra\n",
        "filtered_k_w_n[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('familias', 11),\n",
              " ('verano', 10),\n",
              " ('empresarios', 9),\n",
              " ('reclamo', 7),\n",
              " ('trabajadores', 7),\n",
              " ('julio', 6),\n",
              " ('gastronómicos', 6),\n",
              " ('oportunidad', 6),\n",
              " ('empresariado', 6),\n",
              " ('par', 6),\n",
              " ('negativa', 6),\n",
              " ('beneficio', 6),\n",
              " ('sector', 6),\n",
              " ('pago', 6),\n",
              " ('necesidad', 6),\n",
              " ('cámara', 5),\n",
              " ('postura', 5),\n",
              " ('acuerdo', 5),\n",
              " ('intereses', 5),\n",
              " ('adelanto', 5),\n",
              " ('aumento', 5),\n",
              " ('temporada', 5),\n",
              " ('inflación', 5),\n",
              " ('lleno', 5),\n",
              " ('salarios', 5),\n",
              " ('meses', 5),\n",
              " ('invierno', 5),\n",
              " ('agosto', 5),\n",
              " ('enero', 5),\n",
              " ('febrero', 5),\n",
              " ('escenario', 5),\n",
              " ('referentes', 5),\n",
              " ('respuesta', 5),\n",
              " ('estrategia', 5),\n",
              " ('tiempo', 5),\n",
              " ('razón', 4),\n",
              " ('vez', 4),\n",
              " ('reflexión', 4),\n",
              " ('localidad', 4),\n",
              " ('actitud', 4),\n",
              " ('fe', 4),\n",
              " ('hora', 4),\n",
              " ('relación', 4),\n",
              " ('esfuerzo', 3),\n",
              " ('dirigentes', 3),\n",
              " ('cuerpo', 3),\n",
              " ('delegados', 3),\n",
              " ('secretarios', 3),\n",
              " ('punto', 3),\n",
              " ('vista', 3),\n",
              " ('condiciones', 3),\n",
              " ('clima', 3),\n",
              " ('región', 3),\n",
              " ('caso', 3),\n",
              " ('vidas', 3),\n",
              " ('compañeros', 3),\n",
              " ('salvo', 3),\n",
              " ('representantes', 2),\n",
              " ('embargo', 2),\n",
              " ('situación', 2),\n",
              " ('naturaleza', 2),\n",
              " ('peligro', 2),\n",
              " ('vida', 2),\n",
              " ('conductores', 2),\n",
              " ('organización', 2)]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Analizamos los keywords unigrama\n",
        "keyword_single[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Si un keyword unigrama coincide en los bigramas elegidos se descarta\n",
        "# la cantidad de keywords se obtiene utilizando la media como umbral de corte\n",
        "\n",
        "# Umbral\n",
        "values = [value for sublist in keyword_single for _, value in sublist]\n",
        "threshold = np.mean(values)\n",
        "\n",
        "for i, sublist in enumerate(keyword_single):\n",
        "    lista_k_w_n = list(set([word for sentence in filtered_k_w_n[i] for word in sentence.split()]))\n",
        "    for tupla in sublist:\n",
        "        if tupla[1] >= threshold and tupla[0] not in lista_k_w_n:\n",
        "            filtered_k_w_n[i].append(tupla[0])\n",
        "\n",
        "keywords = filtered_k_w_n      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['empresariado sanmartinense',\n",
              " 'sector laboral',\n",
              " 'verano pasado',\n",
              " 'empresarios hoteleros',\n",
              " 'reclamo justo',\n",
              " 'trabajadores hoteleros',\n",
              " 'familias',\n",
              " 'julio',\n",
              " 'gastronómicos',\n",
              " 'oportunidad',\n",
              " 'par',\n",
              " 'negativa',\n",
              " 'beneficio',\n",
              " 'pago',\n",
              " 'necesidad',\n",
              " 'cámara',\n",
              " 'postura',\n",
              " 'acuerdo',\n",
              " 'intereses',\n",
              " 'adelanto',\n",
              " 'aumento',\n",
              " 'temporada',\n",
              " 'inflación',\n",
              " 'lleno',\n",
              " 'salarios',\n",
              " 'meses',\n",
              " 'invierno',\n",
              " 'agosto',\n",
              " 'enero',\n",
              " 'febrero',\n",
              " 'escenario',\n",
              " 'referentes',\n",
              " 'respuesta',\n",
              " 'estrategia',\n",
              " 'tiempo',\n",
              " 'razón',\n",
              " 'vez',\n",
              " 'reflexión',\n",
              " 'localidad',\n",
              " 'actitud',\n",
              " 'fe',\n",
              " 'hora',\n",
              " 'relación',\n",
              " 'esfuerzo',\n",
              " 'dirigentes',\n",
              " 'cuerpo',\n",
              " 'delegados',\n",
              " 'secretarios',\n",
              " 'punto',\n",
              " 'vista',\n",
              " 'condiciones',\n",
              " 'clima',\n",
              " 'región',\n",
              " 'caso',\n",
              " 'vidas',\n",
              " 'compañeros',\n",
              " 'salvo']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'preproc_notebook/keywords_{choice}.json', 'w') as file:\n",
        "    json.dump(keywords, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BOW - Armado del vocabulario con las entidades y keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3646"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Unificar Entities + Keywords + Keywords with neighboards\n",
        "vocab = list(set().union(*entities, *keywords))\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar vocabulario\n",
        "with open(PATH+f'preproc_notebook/vocabulary_{choice}.json', 'w') as file:\n",
        "    json.dump(vocab, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar noticias en el indice news de la base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "500it [00:00, 887.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos insertados correctamente\n"
          ]
        }
      ],
      "source": [
        "# configurar  batch_size = ( ej.: 5000 ) si se supera el limite 100MB en elasticsearch por operacion\n",
        "index_name = 'news'\n",
        "bulk_data = []\n",
        "\n",
        "for idx, text_news in tqdm(enumerate(data)):\n",
        "    doc = {\n",
        "        'index': {\n",
        "            '_index': index_name,\n",
        "            '_id': int(df_batch.iloc[idx]['asset_id'])\n",
        "        }\n",
        "    }\n",
        "    reg = {\n",
        "        'title': str(df_batch.iloc[idx].title),\n",
        "        'news' : str(text_news), \n",
        "        'author': str(df_batch.iloc[idx]['media']),\n",
        "        'vector': None,\n",
        "        'keywords' : keywords[idx],\n",
        "        'entities' : entities[idx],\n",
        "        'created_at': parse(str(df_batch.iloc[idx]['start_time_local'])).isoformat(),\n",
        "        'process': False\n",
        "    }\n",
        "    bulk_data.append(json.dumps(doc))\n",
        "    bulk_data.append(json.dumps(reg))\n",
        "\n",
        "# Convertir la lista en un solo string separado por saltos de línea\n",
        "bulk_request_body = '\\n'.join(bulk_data) + '\\n'\n",
        "\n",
        "# Enviar la solicitud bulk\n",
        "response = os_client.bulk(body=bulk_request_body)\n",
        "\n",
        "if response['errors']:\n",
        "    print(\"Errores encontrados al insertar los documentos\")\n",
        "else:\n",
        "    print(\"Documentos insertados correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Funcion para comparar resultados entre keywords y entities provistas y las generadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noticia ID: 114954707 ['Murió Nerón, uno de los perros que encontró rastros de Loan Danilo Peña en el auto de Caillava y Pérez']\n",
            "\n",
            "Entities de dataframe: ['Loan', 'Carlos Pérez', 'Nerón', 'Brigada K9', 'Federación Entrerriana de Asociaciones de Bomberos Voluntarios']\n",
            "Keywords de dataframe: ['rastros', 'labrador', 'los perros', 'numerosas misiones', 'asociaciones', 'búsqueda', 'color marrón', 'media asta', 'cuartel', 'certificación', 'fiebre', 'amplio historial', 'infección', 'rescate', 'fallecimiento', 'autos', 'homenaje', 'compañero', 'personas', 'registro nacional', 'duelo', 'regreso', 'reconocimiento', 'dudas', 'foto instagram', 'triste noticia', 'diálogo', 'localidad', 'equipos', 'repentina muerte', 'participación', 'servicio']\n",
            "--------------------------------------------------------------------------------\n",
            "Entities generadas: ['Nerón', 'Pabellón Nacional', 'Provincial', 'provincia de Corrientes', 'Loan Danilo Peña', '9 de Julio', 'Canal 9 Litoral']\n",
            "Keywords generadas: ['color marrón', 'búsqueda', 'parte', 'labrador', 'años']\n",
            "\n",
            " ['Nerón era un labrador color marrón de 7 años, que había participado de la búsqueda de Loan Danilo Peña desde su desaparición, el pasado 13 de junio en la localidad de 9 de Julio, Corrientes. En las últimas horas, trascendió la triste noticia de su muerte.\\n\\nEl animal formaba parte del grupo de perros que habían detectado rastros del pequeño de 5 años en uno de los autos de María Victoria Caillava y Carlos Pérez, dos de los detenidos en la causa.\\n\\n“Al regreso (de Corrientes) se sintió mal, estuvo con fiebre muy alta y a los cinco días falleció. Fue una infección en la sangre por lo que serían garrapatas que juntó en la búsqueda”, contó Cristina Basso, la presidenta de la Cooperativa de Bomberos Voluntarios de Chajarí, sobre la repentina muerte del labrador que formaba parte de la Brigada K9 en Entre Ríos.\\n\\n“Estamos todos con una tristeza impresionante. En el cuartel, Nerón era uno más de todos nosotros”, agregó en diálogo con Canal 9 Litoral, y aseguró: “Es un momento muy doloroso y una se siente responsable”.\\n\\nNerón era un labrador color marrón de 7 años que había participado en numerosas misiones de búsqueda y rescate. (Foto: Instagram/@feabv_entrerios)\\n\\nPor su parte, la Federación Entrerriana de Asociaciones de Bomberos Voluntarios publicó un comunicado en sus redes sociales, en homenaje al fallecimiento de Nerón. “Su último operativo fue la búsqueda de Loan, en la provincia de Corrientes”, remarcaron.\\n\\nEl animal contaba con Certificación en el Registro Nacional de Equipos Cinotécnicos de Búsqueda de Personas del Ministerio Nacional de Seguridad y tenía un amplio historial de participación en misiones de búsqueda y rescate.\\n\\nA su vez, en reconocimiento a su servicio, “la Federación Entrerriana de Asociaciones de Bomberos Voluntarios decretó 72 horas de duelo federativo provincial, durante las cuales el Pabellón Nacional se mantendrá a media asta”, informaron.\\n\\n“Sin dudas nuestro sistema Nacional y Provincial ha perdido un gran can de búsqueda de personas, un compañero y para gran parte del país ha sido una esperanza”, concluyeron.']\n"
          ]
        }
      ],
      "source": [
        "def funcion_aux(ID):\n",
        "    keywords_df = df_parquet[df_parquet.asset_id==ID]['keywords'].values[0]\n",
        "    entities_df = df_parquet[df_parquet.asset_id==ID]['entities'].values[0]\n",
        "    print(f\"Noticia ID: {ID} {df_parquet[df_parquet.asset_id==ID]['title'].values}\\n\")\n",
        "    print(f\"Entities de dataframe: {entities_df}\")\n",
        "    print(f\"Keywords de dataframe: {keywords_df}\")\n",
        "    print(\"-\"*80)\n",
        "    condicion = df_batch['asset_id'] == ID\n",
        "    posicion = df_batch.index[condicion].tolist()[0]\n",
        "    posicion_ordinal = df_batch.index.get_loc(posicion)\n",
        "    print(f\"Entities generadas: {entities[posicion_ordinal]}\")\n",
        "    print(f\"Keywords generadas: {filtered_k_w_n[posicion_ordinal]}\")\n",
        "\n",
        "\n",
        "ID = df_batch.iloc[np.random.randint(len(df_batch))]['asset_id']\n",
        "funcion_aux(ID)\n",
        "print(f\"\\n {df_batch[df_batch.asset_id == ID].text.values}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El criterio de selección de entities y keywords se basa en seleccionar solo las que han sido mayormente nombradas, sin embargo es solo un criterio, y podrían obtenerse una mayor cantidad o la totalidad ajustando el criterio de seleccion."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGxTNO4yTgb9k2r4ffbTN0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
