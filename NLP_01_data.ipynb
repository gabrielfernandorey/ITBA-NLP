{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielfernandorey/ITBA-NLP/blob/main/ITBA_nlp01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumen del problema\n",
        "\n",
        "- Calcular los tópicos de portales de noticias que se reciben \n",
        "- Frecuencia del cálculo de tópicos: diaria\n",
        "- Colección de noticias: diariamente, en lotes o de a un texto.\n",
        "- Identificar tópicos, entidades, keywords y análisis de sentimiento.\n",
        "\n",
        "### Datos\n",
        "- Se reciben las noticias con formato: Titulo, Texto, Fecha, Entidades, Keywords\n",
        "\n",
        "### Tareas\n",
        "- Modelo de detección de tópicos diario utilizando embeddings\n",
        "- Definir un criterio de agrupación de tópicos aplicado al mismo día y entre distintos días (merging)\n",
        "- Almacenar los embeddings de tópicos en una base de datos vectorial\n",
        "- Modelo de datos dado: \n",
        "    - Id del tópico\n",
        "    - Nombre del tópico\n",
        "    - Keywords\n",
        "    - Embbeding\n",
        "    - Fecha de creación\n",
        "    - Fecha de entrenamiento inicial\n",
        "    - Fecha de entrenamiento actualizada\n",
        "    - Umbral de detección\n",
        "    - Documento mas cercano\n",
        "---\n",
        "Tareas en esta notebook:\n",
        "- Inicializar la base de datos vectorial\n",
        "- Ingestar data\n",
        "- NER: Encontrar las entidades de cada documento\n",
        "- Limpiar data\n",
        "- Modelo: Armado del modelo BERTopic\n",
        "- Entrenamiento\n",
        "- Almacenamiento en base de datos vectorial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "1e5d8d12-903f-4a10-ddd3-6d7d9ae83cc7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "from NLP_tools import clean_all\n",
        "from core.functions import *\n",
        "\n",
        "# -->> levantar la base antes de ejecutar\n",
        "from opensearch_data_model import os_client\n",
        "from opensearch_io import init_opensearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializamos la base vectorial\n",
        "Se modifica el indice de la base \"Topic\" agregando referencias del documento mas cercano como el ID y el titulo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El índice Topic ya existe. Saltando inicialización de base de datos.\n",
            "El índice News ya existe. Saltando inicialización de base de datos.\n"
          ]
        }
      ],
      "source": [
        "# Inicialización de indices\n",
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:/Users/gabri/OneDrive/Machine Learning/Github/ITBA-NLP/data/'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "cmp3cLLv28-T",
        "outputId": "2d83d8fc-9241-448a-98a1-6230eb29ce2a"
      },
      "outputs": [],
      "source": [
        "# Read the parquet file \n",
        "\n",
        "file = \"train-00000-of-00001.parquet\"\n",
        "df_parquet = pd.read_parquet(PATH+file)\n",
        "\n",
        "data = list(df_parquet['text'])\n",
        "\n",
        "df_parquet.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cantidad total de documentos\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_parquet.sort_values(\"start_time_local\", ascending=True, inplace=True)\n",
        "df_out = df_parquet[df_parquet['start_time_local'].dt.date > pd.to_datetime('2024-05-31').date()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>title_ch</th>\n",
              "      <th>media</th>\n",
              "      <th>impact</th>\n",
              "      <th>start_time_utc</th>\n",
              "      <th>start_time_local</th>\n",
              "      <th>entities_curated</th>\n",
              "      <th>entities</th>\n",
              "      <th>predicted_at_entities</th>\n",
              "      <th>entities_raw_transformers</th>\n",
              "      <th>entities_transformers</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>keywords</th>\n",
              "      <th>predicted_at_keywords</th>\n",
              "      <th>truncated_text</th>\n",
              "      <th>title_and_text</th>\n",
              "      <th>prediction_delay_predictions</th>\n",
              "      <th>prediction_delay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12498</th>\n",
              "      <td>110476946</td>\n",
              "      <td>Los Tipitos vuelven a Mendoza para celebrar 20...</td>\n",
              "      <td>Diario Los Andes</td>\n",
              "      <td>112583</td>\n",
              "      <td>2024-06-01 03:00:00</td>\n",
              "      <td>2024-06-01 00:00:00</td>\n",
              "      <td>[Abel Pintos, Ricky Martin, Paulina Rubio]</td>\n",
              "      <td>[Ricky Martin, Peteco Carabajal, Chaqueño Pala...</td>\n",
              "      <td>2024-06-02 16:01:42.062757</td>\n",
              "      <td>[{'entities': [{'end': 11, 'entity_group': 'OR...</td>\n",
              "      <td>[Tipitos, Mendoza, Los Tipitos, Plaza, Entrada...</td>\n",
              "      <td>Los Tipitos vuelven a Mendoza para celebrar 20...</td>\n",
              "      <td>Los Tipitos son una banda que marcaron su esti...</td>\n",
              "      <td>[séptimo disco, camaleón, canción, banda, músi...</td>\n",
              "      <td>2024-06-02 16:10:29.363180</td>\n",
              "      <td>Los Tipitos son una banda que marcaron su esti...</td>\n",
              "      <td>Los Tipitos vuelven a Mendoza para celebrar 20...</td>\n",
              "      <td>0.146472</td>\n",
              "      <td>37.174823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6941</th>\n",
              "      <td>110467879</td>\n",
              "      <td>El ``RIGI`` afecta al patrimonio nacional y a ...</td>\n",
              "      <td>Diario Los Andes</td>\n",
              "      <td>11258</td>\n",
              "      <td>2024-06-01 03:00:00</td>\n",
              "      <td>2024-06-01 00:00:00</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Cámara de Comercio Internacional, Poder Ejecu...</td>\n",
              "      <td>2024-06-02 12:21:38.590492</td>\n",
              "      <td>[{'entities': [{'end': 8, 'entity_group': 'MIS...</td>\n",
              "      <td>[Na, ción, Estado Nacional, Gran Bretaña, Gell...</td>\n",
              "      <td>El \"RIGI\" afecta al patrimonio nacional y a nu...</td>\n",
              "      <td>El capítulo de la ley Bases: “Régimen de incen...</td>\n",
              "      <td>[régimen, inversiones, patrimonio nacional, tr...</td>\n",
              "      <td>2024-06-02 12:36:30.679520</td>\n",
              "      <td>El capítulo de la ley Bases: “Régimen de incen...</td>\n",
              "      <td>El \"RIGI\" afecta al patrimonio nacional y a nu...</td>\n",
              "      <td>0.247803</td>\n",
              "      <td>33.608522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9820</th>\n",
              "      <td>110468621</td>\n",
              "      <td>El jugador de Talleres que anotó un golazo par...</td>\n",
              "      <td>Vía País</td>\n",
              "      <td>3365</td>\n",
              "      <td>2024-06-01 03:19:12</td>\n",
              "      <td>2024-06-01 00:19:12</td>\n",
              "      <td>[La Voz]</td>\n",
              "      <td>[Colón, Brigadier López, Catriel Sánchez, Iván...</td>\n",
              "      <td>2024-06-02 12:27:59.973850</td>\n",
              "      <td>[{'entities': [{'end': 22, 'entity_group': 'OR...</td>\n",
              "      <td>[Talleres, Colón, Colón de Santa Fe, Newell’s,...</td>\n",
              "      <td>El jugador de Talleres que anotó un golazo par...</td>\n",
              "      <td>Talleres se cruzará con Colón de Santa Fe el j...</td>\n",
              "      <td>[colón, talleres, golazo, préstamo, almirante,...</td>\n",
              "      <td>2024-06-02 12:39:40.038500</td>\n",
              "      <td>Talleres se cruzará con Colón de Santa Fe el j...</td>\n",
              "      <td>El jugador de Talleres que anotó un golazo par...</td>\n",
              "      <td>0.194462</td>\n",
              "      <td>33.341122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4462</th>\n",
              "      <td>110468519</td>\n",
              "      <td>Cortes de luz programados para este sábado en ...</td>\n",
              "      <td>Diario El Litoral</td>\n",
              "      <td>3117</td>\n",
              "      <td>2024-06-01 04:07:14</td>\n",
              "      <td>2024-06-01 01:07:14</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>2024-06-02 12:28:55.717827</td>\n",
              "      <td>[{'entities': [{'end': 54, 'entity_group': 'LO...</td>\n",
              "      <td>[Santa Fe, Carcaraña]</td>\n",
              "      <td>Cortes de luz programados para este sábado en ...</td>\n",
              "      <td>En el sur santafesino Preocupación por el robo...</td>\n",
              "      <td>[cortes, cables, robo, preocupación, responsab...</td>\n",
              "      <td>2024-06-02 12:44:14.984180</td>\n",
              "      <td>En el sur santafesino Preocupación por el robo...</td>\n",
              "      <td>Cortes de luz programados para este sábado en ...</td>\n",
              "      <td>0.255352</td>\n",
              "      <td>32.616940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1781</th>\n",
              "      <td>110473152</td>\n",
              "      <td>Descifrando las calles: un viaje de 20 años po...</td>\n",
              "      <td>Diario El Litoral</td>\n",
              "      <td>3260</td>\n",
              "      <td>2024-06-01 04:25:15</td>\n",
              "      <td>2024-06-01 01:25:15</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>2024-06-02 14:34:01.744379</td>\n",
              "      <td>[{'entities': [{'end': 80, 'entity_group': 'LO...</td>\n",
              "      <td>[Venado Tuerto, Vía y Obras Ven, ado Tuerto, C...</td>\n",
              "      <td>Descifrando las calles: un viaje de 20 años po...</td>\n",
              "      <td>En el edificio de Vía y Obras Venado Tuerto: d...</td>\n",
              "      <td>[nomenclatura, tuerto, viaje, calles, edificio...</td>\n",
              "      <td>2024-06-02 14:47:40.586200</td>\n",
              "      <td>En el edificio de Vía y Obras Venado Tuerto: d...</td>\n",
              "      <td>Descifrando las calles: un viaje de 20 años po...</td>\n",
              "      <td>0.227456</td>\n",
              "      <td>34.373774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10047</th>\n",
              "      <td>110506945</td>\n",
              "      <td>Los Pumas seven lograron un brillante triunfo ...</td>\n",
              "      <td>Cba24n</td>\n",
              "      <td>88</td>\n",
              "      <td>2024-06-02 23:40:11</td>\n",
              "      <td>2024-06-02 20:40:11</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Lucho González, Wade, Schulz, Tobías Wade, Gó...</td>\n",
              "      <td>2024-06-02 23:40:38.844764</td>\n",
              "      <td>[{'entities': [{'end': 9, 'entity_group': 'ORG...</td>\n",
              "      <td>[Pumas, Francia, Madrid, Australia, Gómez Cora...</td>\n",
              "      <td>Los Pumas seven lograron un brillante triunfo ...</td>\n",
              "      <td>------------------ publicidad ----------------...</td>\n",
              "      <td>[pumas seven, corrida, posesión argentina, par...</td>\n",
              "      <td>2024-06-02 23:48:18.920530</td>\n",
              "      <td>------------------ publicidad ----------------...</td>\n",
              "      <td>Los Pumas seven lograron un brillante triunfo ...</td>\n",
              "      <td>0.127799</td>\n",
              "      <td>0.135533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10805</th>\n",
              "      <td>110506987</td>\n",
              "      <td>Reapareció Posse en una fiesta de la Embajada ...</td>\n",
              "      <td>La Nueva</td>\n",
              "      <td>301</td>\n",
              "      <td>2024-06-02 23:41:40</td>\n",
              "      <td>2024-06-02 20:41:40</td>\n",
              "      <td>[La Nueva, Guillermo Francos, La Libertad Avan...</td>\n",
              "      <td>[Billinghurst, Colón, Gabinete, Rodolfo Barra,...</td>\n",
              "      <td>2024-06-02 23:54:30.092739</td>\n",
              "      <td>[{'entities': [{'end': 16, 'entity_group': 'PE...</td>\n",
              "      <td>[Posse, Italia, Bahía Blanca, Gabinete, Guille...</td>\n",
              "      <td>Reapareció Posse en una fiesta de la Embajada ...</td>\n",
              "      <td>Bahía Blanca | Domingo, 02 de junio Bahía Blan...</td>\n",
              "      <td>[temp, festejo, noticias, ex jefe, representac...</td>\n",
              "      <td>2024-06-02 23:54:45.598360</td>\n",
              "      <td>Bahía Blanca | Domingo, 02 de junio Bahía Blan...</td>\n",
              "      <td>Reapareció Posse en una fiesta de la Embajada ...</td>\n",
              "      <td>0.004307</td>\n",
              "      <td>0.218222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2653</th>\n",
              "      <td>110507001</td>\n",
              "      <td>La obra de teatro ``Juana la intensa`` realiza...</td>\n",
              "      <td>Diario La Capital</td>\n",
              "      <td>161</td>\n",
              "      <td>2024-06-02 23:41:49</td>\n",
              "      <td>2024-06-02 20:41:49</td>\n",
              "      <td>[Teatro Auditorium]</td>\n",
              "      <td>[Astor Piazzolla, Juana, Carmen Domínguez, Ju,...</td>\n",
              "      <td>2024-06-02 23:54:35.585690</td>\n",
              "      <td>[{'entities': [{'end': 24, 'entity_group': 'MI...</td>\n",
              "      <td>[Juana Azurduy, Teatro Auditorium, Mar del Pla...</td>\n",
              "      <td>La obra de teatro \"Juana la intensa\" realizará...</td>\n",
              "      <td>“Juana la intensa” es la historia de Juana Azu...</td>\n",
              "      <td>[teatro, funciones gratuitas, obra, instituto ...</td>\n",
              "      <td>2024-06-02 23:54:50.932080</td>\n",
              "      <td>“Juana la intensa” es la historia de Juana Azu...</td>\n",
              "      <td>La obra de teatro \"Juana la intensa\" realizará...</td>\n",
              "      <td>0.004263</td>\n",
              "      <td>0.217203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12827</th>\n",
              "      <td>110506999</td>\n",
              "      <td>Juan Otero Peña se lanza en busca de su sueño ...</td>\n",
              "      <td>AN Roca</td>\n",
              "      <td>192</td>\n",
              "      <td>2024-06-02 23:41:49</td>\n",
              "      <td>2024-06-02 20:41:49</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Flor Peña, Ramiro Ponce de León, Juan, Lee, G...</td>\n",
              "      <td>2024-06-02 23:49:25.213520</td>\n",
              "      <td>[{'entities': [{'end': 15, 'entity_group': 'PE...</td>\n",
              "      <td>[Juan Otero Peña, Nueva York, Flor Peña, Juan,...</td>\n",
              "      <td>Juan Otero Peña se lanza en busca de su sueño ...</td>\n",
              "      <td>Juan Otero Peña, el hijo de la famosa actriz F...</td>\n",
              "      <td>[sueño artístico, flor peña, carrera artística...</td>\n",
              "      <td>2024-06-02 23:58:11.913650</td>\n",
              "      <td>Juan Otero Peña, el hijo de la famosa actriz F...</td>\n",
              "      <td>Juan Otero Peña se lanza en busca de su sueño ...</td>\n",
              "      <td>0.146306</td>\n",
              "      <td>0.273032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9305</th>\n",
              "      <td>110507149</td>\n",
              "      <td>Cómo se puso La Tora al enterarse del romance ...</td>\n",
              "      <td>MDZ Online</td>\n",
              "      <td>7615</td>\n",
              "      <td>2024-06-02 23:45:28</td>\n",
              "      <td>2024-06-02 20:45:28</td>\n",
              "      <td>[Ángel de Brito, MDZ Online]</td>\n",
              "      <td>[Coti, Conejo, Coty Romero, Nacho, La Tora, Fe...</td>\n",
              "      <td>2024-06-02 23:51:55.660181</td>\n",
              "      <td>[{'entities': [{'end': 20, 'entity_group': 'PE...</td>\n",
              "      <td>[La Tora, Coty, Nacho, Coty Romero, Nacho Cast...</td>\n",
              "      <td>Cómo se puso La Tora al enterarse del romance ...</td>\n",
              "      <td>En medio de las pruebas que indicarían que Cot...</td>\n",
              "      <td>[nacho, anteriores romances, circo, cumpleaños...</td>\n",
              "      <td>2024-06-02 23:52:12.608560</td>\n",
              "      <td>En medio de las pruebas que indicarían que Cot...</td>\n",
              "      <td>Cómo se puso La Tora al enterarse del romance ...</td>\n",
              "      <td>0.004708</td>\n",
              "      <td>0.112391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15042 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        asset_id                                           title_ch  \\\n",
              "12498  110476946  Los Tipitos vuelven a Mendoza para celebrar 20...   \n",
              "6941   110467879  El ``RIGI`` afecta al patrimonio nacional y a ...   \n",
              "9820   110468621  El jugador de Talleres que anotó un golazo par...   \n",
              "4462   110468519  Cortes de luz programados para este sábado en ...   \n",
              "1781   110473152  Descifrando las calles: un viaje de 20 años po...   \n",
              "...          ...                                                ...   \n",
              "10047  110506945  Los Pumas seven lograron un brillante triunfo ...   \n",
              "10805  110506987  Reapareció Posse en una fiesta de la Embajada ...   \n",
              "2653   110507001  La obra de teatro ``Juana la intensa`` realiza...   \n",
              "12827  110506999  Juan Otero Peña se lanza en busca de su sueño ...   \n",
              "9305   110507149  Cómo se puso La Tora al enterarse del romance ...   \n",
              "\n",
              "                   media  impact      start_time_utc    start_time_local  \\\n",
              "12498   Diario Los Andes  112583 2024-06-01 03:00:00 2024-06-01 00:00:00   \n",
              "6941    Diario Los Andes   11258 2024-06-01 03:00:00 2024-06-01 00:00:00   \n",
              "9820            Vía País    3365 2024-06-01 03:19:12 2024-06-01 00:19:12   \n",
              "4462   Diario El Litoral    3117 2024-06-01 04:07:14 2024-06-01 01:07:14   \n",
              "1781   Diario El Litoral    3260 2024-06-01 04:25:15 2024-06-01 01:25:15   \n",
              "...                  ...     ...                 ...                 ...   \n",
              "10047             Cba24n      88 2024-06-02 23:40:11 2024-06-02 20:40:11   \n",
              "10805           La Nueva     301 2024-06-02 23:41:40 2024-06-02 20:41:40   \n",
              "2653   Diario La Capital     161 2024-06-02 23:41:49 2024-06-02 20:41:49   \n",
              "12827            AN Roca     192 2024-06-02 23:41:49 2024-06-02 20:41:49   \n",
              "9305          MDZ Online    7615 2024-06-02 23:45:28 2024-06-02 20:45:28   \n",
              "\n",
              "                                        entities_curated  \\\n",
              "12498         [Abel Pintos, Ricky Martin, Paulina Rubio]   \n",
              "6941                                                  []   \n",
              "9820                                            [La Voz]   \n",
              "4462                                                  []   \n",
              "1781                                                  []   \n",
              "...                                                  ...   \n",
              "10047                                                 []   \n",
              "10805  [La Nueva, Guillermo Francos, La Libertad Avan...   \n",
              "2653                                 [Teatro Auditorium]   \n",
              "12827                                                 []   \n",
              "9305                        [Ángel de Brito, MDZ Online]   \n",
              "\n",
              "                                                entities  \\\n",
              "12498  [Ricky Martin, Peteco Carabajal, Chaqueño Pala...   \n",
              "6941   [Cámara de Comercio Internacional, Poder Ejecu...   \n",
              "9820   [Colón, Brigadier López, Catriel Sánchez, Iván...   \n",
              "4462                                                  []   \n",
              "1781                                                  []   \n",
              "...                                                  ...   \n",
              "10047  [Lucho González, Wade, Schulz, Tobías Wade, Gó...   \n",
              "10805  [Billinghurst, Colón, Gabinete, Rodolfo Barra,...   \n",
              "2653   [Astor Piazzolla, Juana, Carmen Domínguez, Ju,...   \n",
              "12827  [Flor Peña, Ramiro Ponce de León, Juan, Lee, G...   \n",
              "9305   [Coti, Conejo, Coty Romero, Nacho, La Tora, Fe...   \n",
              "\n",
              "           predicted_at_entities  \\\n",
              "12498 2024-06-02 16:01:42.062757   \n",
              "6941  2024-06-02 12:21:38.590492   \n",
              "9820  2024-06-02 12:27:59.973850   \n",
              "4462  2024-06-02 12:28:55.717827   \n",
              "1781  2024-06-02 14:34:01.744379   \n",
              "...                          ...   \n",
              "10047 2024-06-02 23:40:38.844764   \n",
              "10805 2024-06-02 23:54:30.092739   \n",
              "2653  2024-06-02 23:54:35.585690   \n",
              "12827 2024-06-02 23:49:25.213520   \n",
              "9305  2024-06-02 23:51:55.660181   \n",
              "\n",
              "                               entities_raw_transformers  \\\n",
              "12498  [{'entities': [{'end': 11, 'entity_group': 'OR...   \n",
              "6941   [{'entities': [{'end': 8, 'entity_group': 'MIS...   \n",
              "9820   [{'entities': [{'end': 22, 'entity_group': 'OR...   \n",
              "4462   [{'entities': [{'end': 54, 'entity_group': 'LO...   \n",
              "1781   [{'entities': [{'end': 80, 'entity_group': 'LO...   \n",
              "...                                                  ...   \n",
              "10047  [{'entities': [{'end': 9, 'entity_group': 'ORG...   \n",
              "10805  [{'entities': [{'end': 16, 'entity_group': 'PE...   \n",
              "2653   [{'entities': [{'end': 24, 'entity_group': 'MI...   \n",
              "12827  [{'entities': [{'end': 15, 'entity_group': 'PE...   \n",
              "9305   [{'entities': [{'end': 20, 'entity_group': 'PE...   \n",
              "\n",
              "                                   entities_transformers  \\\n",
              "12498  [Tipitos, Mendoza, Los Tipitos, Plaza, Entrada...   \n",
              "6941   [Na, ción, Estado Nacional, Gran Bretaña, Gell...   \n",
              "9820   [Talleres, Colón, Colón de Santa Fe, Newell’s,...   \n",
              "4462                               [Santa Fe, Carcaraña]   \n",
              "1781   [Venado Tuerto, Vía y Obras Ven, ado Tuerto, C...   \n",
              "...                                                  ...   \n",
              "10047  [Pumas, Francia, Madrid, Australia, Gómez Cora...   \n",
              "10805  [Posse, Italia, Bahía Blanca, Gabinete, Guille...   \n",
              "2653   [Juana Azurduy, Teatro Auditorium, Mar del Pla...   \n",
              "12827  [Juan Otero Peña, Nueva York, Flor Peña, Juan,...   \n",
              "9305   [La Tora, Coty, Nacho, Coty Romero, Nacho Cast...   \n",
              "\n",
              "                                                   title  \\\n",
              "12498  Los Tipitos vuelven a Mendoza para celebrar 20...   \n",
              "6941   El \"RIGI\" afecta al patrimonio nacional y a nu...   \n",
              "9820   El jugador de Talleres que anotó un golazo par...   \n",
              "4462   Cortes de luz programados para este sábado en ...   \n",
              "1781   Descifrando las calles: un viaje de 20 años po...   \n",
              "...                                                  ...   \n",
              "10047  Los Pumas seven lograron un brillante triunfo ...   \n",
              "10805  Reapareció Posse en una fiesta de la Embajada ...   \n",
              "2653   La obra de teatro \"Juana la intensa\" realizará...   \n",
              "12827  Juan Otero Peña se lanza en busca de su sueño ...   \n",
              "9305   Cómo se puso La Tora al enterarse del romance ...   \n",
              "\n",
              "                                                    text  \\\n",
              "12498  Los Tipitos son una banda que marcaron su esti...   \n",
              "6941   El capítulo de la ley Bases: “Régimen de incen...   \n",
              "9820   Talleres se cruzará con Colón de Santa Fe el j...   \n",
              "4462   En el sur santafesino Preocupación por el robo...   \n",
              "1781   En el edificio de Vía y Obras Venado Tuerto: d...   \n",
              "...                                                  ...   \n",
              "10047  ------------------ publicidad ----------------...   \n",
              "10805  Bahía Blanca | Domingo, 02 de junio Bahía Blan...   \n",
              "2653   “Juana la intensa” es la historia de Juana Azu...   \n",
              "12827  Juan Otero Peña, el hijo de la famosa actriz F...   \n",
              "9305   En medio de las pruebas que indicarían que Cot...   \n",
              "\n",
              "                                                keywords  \\\n",
              "12498  [séptimo disco, camaleón, canción, banda, músi...   \n",
              "6941   [régimen, inversiones, patrimonio nacional, tr...   \n",
              "9820   [colón, talleres, golazo, préstamo, almirante,...   \n",
              "4462   [cortes, cables, robo, preocupación, responsab...   \n",
              "1781   [nomenclatura, tuerto, viaje, calles, edificio...   \n",
              "...                                                  ...   \n",
              "10047  [pumas seven, corrida, posesión argentina, par...   \n",
              "10805  [temp, festejo, noticias, ex jefe, representac...   \n",
              "2653   [teatro, funciones gratuitas, obra, instituto ...   \n",
              "12827  [sueño artístico, flor peña, carrera artística...   \n",
              "9305   [nacho, anteriores romances, circo, cumpleaños...   \n",
              "\n",
              "           predicted_at_keywords  \\\n",
              "12498 2024-06-02 16:10:29.363180   \n",
              "6941  2024-06-02 12:36:30.679520   \n",
              "9820  2024-06-02 12:39:40.038500   \n",
              "4462  2024-06-02 12:44:14.984180   \n",
              "1781  2024-06-02 14:47:40.586200   \n",
              "...                          ...   \n",
              "10047 2024-06-02 23:48:18.920530   \n",
              "10805 2024-06-02 23:54:45.598360   \n",
              "2653  2024-06-02 23:54:50.932080   \n",
              "12827 2024-06-02 23:58:11.913650   \n",
              "9305  2024-06-02 23:52:12.608560   \n",
              "\n",
              "                                          truncated_text  \\\n",
              "12498  Los Tipitos son una banda que marcaron su esti...   \n",
              "6941   El capítulo de la ley Bases: “Régimen de incen...   \n",
              "9820   Talleres se cruzará con Colón de Santa Fe el j...   \n",
              "4462   En el sur santafesino Preocupación por el robo...   \n",
              "1781   En el edificio de Vía y Obras Venado Tuerto: d...   \n",
              "...                                                  ...   \n",
              "10047  ------------------ publicidad ----------------...   \n",
              "10805  Bahía Blanca | Domingo, 02 de junio Bahía Blan...   \n",
              "2653   “Juana la intensa” es la historia de Juana Azu...   \n",
              "12827  Juan Otero Peña, el hijo de la famosa actriz F...   \n",
              "9305   En medio de las pruebas que indicarían que Cot...   \n",
              "\n",
              "                                          title_and_text  \\\n",
              "12498  Los Tipitos vuelven a Mendoza para celebrar 20...   \n",
              "6941   El \"RIGI\" afecta al patrimonio nacional y a nu...   \n",
              "9820   El jugador de Talleres que anotó un golazo par...   \n",
              "4462   Cortes de luz programados para este sábado en ...   \n",
              "1781   Descifrando las calles: un viaje de 20 años po...   \n",
              "...                                                  ...   \n",
              "10047  Los Pumas seven lograron un brillante triunfo ...   \n",
              "10805  Reapareció Posse en una fiesta de la Embajada ...   \n",
              "2653   La obra de teatro \"Juana la intensa\" realizará...   \n",
              "12827  Juan Otero Peña se lanza en busca de su sueño ...   \n",
              "9305   Cómo se puso La Tora al enterarse del romance ...   \n",
              "\n",
              "       prediction_delay_predictions  prediction_delay  \n",
              "12498                      0.146472         37.174823  \n",
              "6941                       0.247803         33.608522  \n",
              "9820                       0.194462         33.341122  \n",
              "4462                       0.255352         32.616940  \n",
              "1781                       0.227456         34.373774  \n",
              "...                             ...               ...  \n",
              "10047                      0.127799          0.135533  \n",
              "10805                      0.004307          0.218222  \n",
              "2653                       0.004263          0.217203  \n",
              "12827                      0.146306          0.273032  \n",
              "9305                       0.004708          0.112391  \n",
              "\n",
              "[15042 rows x 19 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lote para pruebas\n",
        "chunk = 200\n",
        "data = data[:chunk]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StopWords\n",
        "Se genera una lista especial de stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "SPANISH_STOPWORDS = list(pd.read_csv(PATH+'spanish_stop_words.csv' )['stopwords'].values)\n",
        "SPANISH_STOPWORDS_SPECIAL = list(pd.read_csv(PATH+'spanish_stop_words_spec.csv' )['stopwords'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" import csv\n",
        "# Guardar la lista de stopwords especial en un archivo CSV\n",
        "with open(PATH+\"spanish_stop_words_spec.csv\", mode='w', newline='', encoding='utf-8') as archivo:\n",
        "    escritor = csv.writer(archivo)\n",
        "    escritor.writerow(['stopwords'])\n",
        "    for stopword in SPANISH_STOPWORDS_SPECIAL:\n",
        "        escritor.writerow([stopword]) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER - Named Entity Recognition\n",
        "Obtener entidades de las noticias \n",
        "\n",
        "-   Nota: Aunque el dataset original provee keywords y entities, se realiza el proceso de obtención de los mismos y se utilizan para el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el modelo de spaCy para español\n",
        "spa = spacy.load(\"es_core_news_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " \n",
        "# Cargar o saltar carga y procesar celda inferior\n",
        "with open(PATH+f'modelos/entities_{str(chunk)}.json', 'r') as json_file:\n",
        "    entities = json.load(json_file)\n",
        "\n",
        "with open(PATH+f'modelos/keywords_{str(chunk)}.json', 'r') as json_file:\n",
        "    keywords = json.load(json_file)\n",
        "\n",
        "with open(PATH+f'modelos/vocabulary_{str(chunk)}.json', 'r') as json_file:\n",
        "    vocab = json.load(json_file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:28<00:00,  7.11it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar entidades para todos los documentos usando spaCy\n",
        "# se procesa utilizando un criterio de seleccion\n",
        "\n",
        "entities = []\n",
        "for data_in in tqdm(data):\n",
        "\n",
        "    # Contabilizar palabras en doc original\n",
        "    normalized_text = re.sub(r'\\W+', ' ', data_in.lower())\n",
        "    words_txt_without_stopwords = [word for word in normalized_text.split() if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL]\n",
        "    words_txt_counter = Counter(words_txt_without_stopwords)\n",
        "    words_counter = {elemento: cuenta for elemento, cuenta in sorted(words_txt_counter.items(), key=lambda item:item[1], reverse=True) if cuenta > 1}\n",
        "\n",
        "    # Extraer entidades del doc segun atributos\n",
        "    extract = spa(data_in)\n",
        "    entidades_spacy = [(ent.text, ent.label_) for ent in extract.ents]\n",
        "    ent_select = [ent for ent in entidades_spacy if ent[1] == 'PER' or ent[1] == 'ORG' or ent[1] == 'LOC' ]\n",
        "\n",
        "    # Extraer entidades de \"maximo 3 palabras\"\n",
        "    ent_max_3 = [ent[0] for ent in ent_select if len(ent[0].split()) <= 3]\n",
        "    ent_clean = clean_all(ent_max_3, accents=False)\n",
        "    ent_unique = list(set([ word for word in ent_clean if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL] ))\n",
        "\n",
        "    ents_proc = {}\n",
        "    for ent in ent_unique:\n",
        "        \n",
        "        # Criterio de selección \n",
        "        weight = 0\n",
        "        for word in ent.split():\n",
        "            if word in words_counter:\n",
        "                weight += 1 /len(ent.split()) * words_counter[word]\n",
        "        \n",
        "        ents_proc[ent] = round(weight,4)\n",
        "\n",
        "    ents_proc_sorted = {k: v for k, v in sorted(ents_proc.items(), key=lambda item: item[1], reverse=True) if v > 0}\n",
        "\n",
        "    # Crear la lista preliminar de entidades procesadas por noticia \n",
        "    pre_entities = [key for key, _ in ents_proc_sorted.items()] \n",
        "\n",
        "    # Obtener las últimas palabras de cada entidad que tenga mas de una palabra por entidad\n",
        "    last_words = list(set([ent.split()[-1] for ent in pre_entities if len(ent.split()) > 1 ]))\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al final de una compuesta\n",
        "    pre_entities_without_last_word_equal = []\n",
        "    for idx, ent in enumerate(pre_entities):\n",
        "        if not (len(ent.split()) == 1 and ent in last_words):\n",
        "            pre_entities_without_last_word_equal.append(ent)\n",
        "\n",
        "    # Obtener las palabras únicas\n",
        "    unique_words = [ ent.split()[0] for ent in pre_entities_without_last_word_equal if len(ent.split()) > 1 ]\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al comienzo de una compuesta\n",
        "    pre_entities_without_first_word_equal = []\n",
        "    for idx, ent in enumerate(pre_entities_without_last_word_equal):\n",
        "        if not (len(ent.split()) == 1 and ent in unique_words):\n",
        "            pre_entities_without_first_word_equal.append(ent)\n",
        "\n",
        "    # obtener entidades filtradas\n",
        "    if len(pre_entities_without_first_word_equal) > 10:\n",
        "        umbral = 10 + (len(pre_entities_without_first_word_equal)-10) // 2\n",
        "        filter_entities = pre_entities_without_first_word_equal[:umbral] \n",
        "    else:\n",
        "        filter_entities = pre_entities_without_first_word_equal[:10]\n",
        "\n",
        "    pre_original_entities = []\n",
        "    # capturar las entidades en formato original\n",
        "    for ent in filter_entities:\n",
        "        pre_original_entities.append([elemento for elemento in ent_max_3 if elemento.lower() == ent.lower()])\n",
        "\n",
        "    sort_original_entities = sorted(pre_original_entities, key=len, reverse=True)\n",
        "    \n",
        "    try:\n",
        "        entities.append( [ent[0] for ent in sort_original_entities if ent] ) \n",
        "    except Exception as e:\n",
        "        entities.append([])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'modelos/entities_{str(chunk)}.json', 'w') as file:\n",
        "    json.dump(entities, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keywords\n",
        "Obtener palabras clave de las noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:24<00:00,  8.19it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar keywords para todos los documentos usando spaCy\n",
        "\n",
        "keywords_spa = []\n",
        "for doc in tqdm(data):\n",
        "    extract = spa(doc)\n",
        "    keywords_spa.append([(ext.text, ext.pos_) for ext in extract])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keyboards with neighboards\n",
        "- Se seleccionan keywords unigrama y bigrama mediante la funcion keywords_with_neighboards(), que a su vez llama a las funciones get_bigrams() y get_neighbor_words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funcion para obtener keywords con combinaciones de bigramas\n",
        "def get_bigrams(word_list, number_consecutive_words=2):\n",
        "    \n",
        "    ngrams = []\n",
        "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
        "    \n",
        "    for word_index in range(adj_length_of_word_list):\n",
        "        \n",
        "        # Indexar la lista \n",
        "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
        "        \n",
        "        # Agregar a la lista de \"ngrams\"\n",
        "        ngrams.append(ngram)\n",
        "        \n",
        "    return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# devolver las palabras más frecuentes que aparecen junto a una palabra clave en particular\n",
        "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
        "    \n",
        "    neighbor_words = []\n",
        "    keyword = keyword.lower()\n",
        "    \n",
        "    for bigram in bigrams:\n",
        "        \n",
        "        # Extrae solo las palabras en minúsculas (no las etiquetas) para cada bigrama\n",
        "        words = [word.lower() for word, label in bigram]        \n",
        "        \n",
        "        # Comprueba si la palabra clave está en el bigram\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            for word, label in bigram:\n",
        "                \n",
        "                #Ahora nos centramos en la palabra vecina, no en la palabra clave\n",
        "                if word.lower() != keyword:\n",
        "                    #Si la palabra vecina coincide con la pos_label correcta, agregarla a la lista maestra\n",
        "                    if label == pos_label or pos_label == None:\n",
        "                        if idx == 0:\n",
        "                            neighbor_words.append(\" \".join([keyword, word.lower()]))\n",
        "                        else:\n",
        "                            neighbor_words.append(\" \".join([word.lower(), keyword]))\n",
        "                    \n",
        "    return Counter(neighbor_words).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keywords_with_neighboards(keywords_spa, POS_1='NOUN', POS_2='ADJ'):\n",
        "    \"\"\"\n",
        "    Funcion que devuelve dos listas:\n",
        "    - lista de keywords with neighboards (segun argumentos POS_1 y POS_2)\n",
        "    - lista de keywords mas frecuentes (segun argumentos POS_1 y POS_2)\n",
        "    \"\"\"\n",
        "\n",
        "    doc_kwn = []\n",
        "    commons = []\n",
        "    for keywords in keywords_spa:\n",
        "    \n",
        "        # Obtenemos las keywords del tipo (Universal Dependences) mas frecuentes de cada doc (spaCy format)\n",
        "        words = []\n",
        "        for k_spa in keywords:\n",
        "            if k_spa[1] == POS_1:\n",
        "                words.append(k_spa[0])\n",
        "\n",
        "        cont_words = Counter(words)\n",
        "\n",
        "        common = cont_words.most_common()\n",
        "        commons.append( [com for com in common if com[1] > 1] )\n",
        "\n",
        "        # Calcular un umbral de corte (en repeticiones) para los keywords obtenidos\n",
        "            ## suma de todos los valores\n",
        "        valores = [valor for _, valor in common]\n",
        "\n",
        "            ## Calcular los pesos como proporcionales a los valores mismos\n",
        "        pesos = np.array(valores) / np.sum(valores)\n",
        "\n",
        "            ## Calcular el umbral ponderado, valor 2 o superior ( debe repetirse la keyword al menos una vez )\n",
        "        threshold = max(2, round(np.sum(np.array(valores) * pesos),4))\n",
        "\n",
        "\n",
        "        # Obtenemos los bigramas del doc        \n",
        "        tokens_and_labels = [(token[0], token[1]) for token in keywords if token[0].isalpha()]\n",
        "\n",
        "        bigrams = get_bigrams(tokens_and_labels)\n",
        "\n",
        "        keywords_neighbor = []\n",
        "        for item_common in common:\n",
        "            if item_common[1] >= threshold or len(keywords_neighbor) < 6: # corte por umbral o menor a 6\n",
        "                \n",
        "                kwn = get_neighbor_words(item_common[0], bigrams, pos_label=POS_2)\n",
        "                if kwn != []:\n",
        "                    keywords_neighbor.append( kwn )\n",
        "\n",
        "        sorted_keywords_neighbor = sorted([item for sublist in keywords_neighbor for item in sublist ], key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        doc_kwn.append(sorted_keywords_neighbor)\n",
        "\n",
        "    return doc_kwn, commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obtenemos keywords with neighboards y keywords mas frecuentes\n",
        "k_w_n, keyword_single = keywords_with_neighboards(keywords_spa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('generoso gesto', 1),\n",
              " ('galante actitud', 1),\n",
              " ('excéntricos años', 1),\n",
              " ('mesa completa', 1),\n",
              " ('ascendente carrera', 1),\n",
              " ('famoso restaurante', 1)]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# muestra\n",
        "k_w_n[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filtramos las que al menos se repiten una vez\n",
        "filtered_k_w_n = [ [tupla[0] for tupla in sublista if tupla[1] > 1] for sublista in k_w_n ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ahorro interno',\n",
              " 'sistema bancario',\n",
              " 'descomunal tasa',\n",
              " 'impuestos confiscatorios']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# muestra\n",
        "filtered_k_w_n[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ahorro', 12),\n",
              " ('dólares', 8),\n",
              " ('tasa', 7),\n",
              " ('interés', 6),\n",
              " ('argentino', 4),\n",
              " ('ahorros', 4),\n",
              " ('impuestos', 4),\n",
              " ('sistema', 4),\n",
              " ('dólar', 3),\n",
              " ('pesos', 3),\n",
              " ('otra', 3),\n",
              " ('cosa', 3),\n",
              " ('ingreso', 3),\n",
              " ('consumo', 3),\n",
              " ('futuro', 3),\n",
              " ('capacidad', 3),\n",
              " ('crecimiento', 3),\n",
              " ('rentabilidad', 3),\n",
              " ('actividad', 3),\n",
              " ('exterior', 3),\n",
              " ('economía', 3),\n",
              " ('países', 3),\n",
              " ('moneda', 2),\n",
              " ('caso', 2),\n",
              " ('peso', 2),\n",
              " ('tiempo', 2),\n",
              " ('aumento', 2),\n",
              " ('tipo', 2),\n",
              " ('cambio', 2),\n",
              " ('capitales', 2),\n",
              " ('inversión', 2),\n",
              " ('población', 2),\n",
              " ('margen', 2),\n",
              " ('argentinos', 2),\n",
              " ('devaluación', 2),\n",
              " ('plazo', 2),\n",
              " ('días', 2),\n",
              " ('mensual-', 2),\n",
              " ('renta', 2),\n",
              " ('forma', 2),\n",
              " ('millones', 2),\n",
              " ('familia', 2),\n",
              " ('pago', 2),\n",
              " ('expropiaciones', 2)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Analizamos los keywords unigrama\n",
        "keyword_single[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Si un keyword unigrama coincide en los bigramas elegidos se descarta\n",
        "# la cantidad de keywords se obtiene utilizando la media como umbral de corte\n",
        "\n",
        "# Umbral\n",
        "values = [value for sublist in keyword_single for _, value in sublist]\n",
        "threshold = np.mean(values)\n",
        "\n",
        "for i, sublist in enumerate(keyword_single):\n",
        "    lista_k_w_n = list(set([word for sentence in filtered_k_w_n[i] for word in sentence.split()]))\n",
        "    for tupla in sublist:\n",
        "        if tupla[1] >= threshold and tupla[0] not in lista_k_w_n:\n",
        "            filtered_k_w_n[i].append(tupla[0])\n",
        "\n",
        "keywords = filtered_k_w_n      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ahorro interno',\n",
              " 'sistema bancario',\n",
              " 'descomunal tasa',\n",
              " 'impuestos confiscatorios',\n",
              " 'dólares',\n",
              " 'interés',\n",
              " 'argentino',\n",
              " 'ahorros',\n",
              " 'dólar',\n",
              " 'pesos',\n",
              " 'otra',\n",
              " 'cosa',\n",
              " 'ingreso',\n",
              " 'consumo',\n",
              " 'futuro',\n",
              " 'capacidad',\n",
              " 'crecimiento',\n",
              " 'rentabilidad',\n",
              " 'actividad',\n",
              " 'exterior',\n",
              " 'economía',\n",
              " 'países']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'modelos/keywords_{chunk}.json', 'w') as file:\n",
        "    json.dump(keywords, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BOW - Armado del vocabulario con las entidades y keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2681"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Unificar Entities + Keywords + Keywords with neighboards\n",
        "vocab = list(set().union(*entities, *keywords))\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar vocabulario\n",
        "with open(PATH+f'modelos/vocabulary_{chunk}.json', 'w') as file:\n",
        "    json.dump(vocab, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocesar las noticias\n",
        "Se realiza un preprocesamiento mínimo del texto, pero no se le quita el sentido semántico para que mediante SentenceTransformer se puedan capturar embeddings de mejor calidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 786.02it/s]\n"
          ]
        }
      ],
      "source": [
        "clean_data = Cleaning_text()\n",
        "\n",
        "proc_data = []\n",
        "for data_in in tqdm(data):\n",
        "    aux = clean_data.unicode(data_in)\n",
        "    aux = clean_data.urls(aux)\n",
        "    aux = clean_data.simbols(aux)\n",
        "    aux = clean_data.escape_sequence(aux)\n",
        "    aux = \" \".join([ word for word in aux.split() if word.lower() not in SPANISH_STOPWORDS_SPECIAL])\n",
        "    proc_data.append(aux)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'modelos/proc_data_{chunk}.json', 'w') as file:\n",
        "    json.dump(proc_data, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# muestra\n",
        "proc_data[60]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar noticias en el indice news de la base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configurar  batch_size = ( ej.: 5000 ) si se supera el limite 100MB en elasticsearch por operacion\n",
        "index_name = 'news'\n",
        "bulk_data = []\n",
        "\n",
        "for idx, text_news in tqdm(enumerate(data)):\n",
        "    doc = {\n",
        "        'index': {\n",
        "            '_index': index_name,\n",
        "            '_id': int(df_parquet.index[idx])\n",
        "        }\n",
        "    }\n",
        "    reg = {\n",
        "        'title': str(df_parquet.iloc[idx].title),\n",
        "        'news' : str(text_news), \n",
        "        'author': str(df_parquet.iloc[idx]['media']),\n",
        "        'vector': None,\n",
        "        'keywords' : keywords[idx],\n",
        "        'entities' : entities[idx],\n",
        "        'created_at': parse(str(df_parquet.iloc[idx]['start_time_local'])).isoformat(),\n",
        "        'process': False\n",
        "    }\n",
        "    bulk_data.append(json.dumps(doc))\n",
        "    bulk_data.append(json.dumps(reg))\n",
        "\n",
        "# Convertir la lista en un solo string separado por saltos de línea\n",
        "bulk_request_body = '\\n'.join(bulk_data) + '\\n'\n",
        "\n",
        "# Enviar la solicitud bulk\n",
        "response = os_client.bulk(body=bulk_request_body)\n",
        "\n",
        "if response['errors']:\n",
        "    print(\"Errores encontrados al insertar los documentos\")\n",
        "else:\n",
        "    print(\"Documentos insertados correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Funciones de pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encontrar la posicion en el df segun su ID\n",
        "df_parquet.index.get_loc(105640350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def funcion_aux(ID):\n",
        "    keywords_df = df_parquet[df_parquet.index==ID]['Keyword Name'].values[0]\n",
        "    entities_df = df_parquet[df_parquet.index==ID]['Entity Name'].values[0]\n",
        "    fila = df_parquet.index.get_loc(ID)\n",
        "    print(f\"Noticia ID: {ID} {df_parquet[df_parquet.index==ID]['in__title'].values}\\n\")\n",
        "    print(f\"Entities de dataframe: {entities_df}\")\n",
        "    print(f\"Keywords de dataframe: {keywords_df}\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Fila: {fila}\")\n",
        "    print(f\"Entities calculadas: {entities[fila]}\")\n",
        "    print(f\"Keywords calculadas: {filtered_k_w_n[fila]}\")\n",
        "\n",
        "funcion_aux(105638862)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_out.to_parquet('2024-06-01.parquet')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGxTNO4yTgb9k2r4ffbTN0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
