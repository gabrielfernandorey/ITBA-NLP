{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6GzUxPz0r9-"
      },
      "source": [
        "# Trabajo Practico NLP - Detección de Tópicos y clasificación\n",
        "- ITBA 2024\n",
        "- Alumno: Gabriel Rey\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumen del problema\n",
        "\n",
        "- Calcular los tópicos de portales de noticias que se reciben \n",
        "- Frecuencia del cálculo de tópicos: diaria\n",
        "- Colección de noticias: diariamente, en lotes o de a un texto.\n",
        "- Identificar tópicos, entidades, keywords y análisis de sentimiento.\n",
        "\n",
        "### Datos\n",
        "- Se reciben las noticias con formato: Titulo, Texto, Fecha, Entidades, Keywords\n",
        "\n",
        "### Tareas\n",
        "- Modelo de detección de tópicos diario utilizando embeddings\n",
        "- Definir un criterio de agrupación de tópicos aplicado al mismo día y entre distintos días (merging)\n",
        "- Almacenar los embeddings de tópicos en una base de datos vectorial\n",
        "- Modelo de datos dado: \n",
        "    - Id del tópico\n",
        "    - Nombre del tópico\n",
        "    - Keywords\n",
        "    - Embbeding\n",
        "    - Fecha de creación\n",
        "    - Fecha de entrenamiento inicial\n",
        "    - Fecha de entrenamiento actualizada\n",
        "    - Umbral de detección\n",
        "    - Documento mas cercano\n",
        "---\n",
        "Tareas en esta notebook:\n",
        "- Inicializar la base de datos vectorial\n",
        "- Ingestar data\n",
        "- NER: Encontrar las entidades de cada documento\n",
        "- Limpiar data\n",
        "- Modelo: Armado del modelo BERTopic\n",
        "- Entrenamiento\n",
        "- Almacenamiento en base de datos vectorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NLP_01_data\n",
        "Esta notebook se utiliza para:\n",
        "- obtener un lote predefinido de las noticias de una fecha dada\n",
        "- se realiza el ejercicio de obtención de entities y keywords, mas allá de tenerlas provistas en el set de datos de origen.\n",
        "- se obtiene el vocabulario para utilizar en BERTopic\n",
        "\n",
        "Esta y las consecuentes notebooks son el desarrollo de base de procesos y funciones para la web app provista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7eCyxiT1rcu",
        "outputId": "1e5d8d12-903f-4a10-ddd3-6d7d9ae83cc7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "from dateutil.parser import parse\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "from NLP_tools import *\n",
        "\n",
        "# -->> levantar la base antes de ejecutar\n",
        "from opensearch_data_model import os_client\n",
        "from opensearch_io import init_opensearch\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicializamos la base vectorial\n",
        "- Se modifica el indice de la base \"Topic\" agregando referencias del documento mas cercano como el ID y el titulo\n",
        "- Se crea un nuevo indice para las noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El índice Topic ya existe. Saltando inicialización de base de datos.\n",
            "El índice News ya existe. Saltando inicialización de base de datos.\n"
          ]
        }
      ],
      "source": [
        "# Inicialización de indices\n",
        "init_opensearch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:/Users/gabri/OneDrive/Machine Learning/Github/ITBA-NLP/data/'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n",
        "PATH_REMOTO='/content/ITBA-NLP/data/'\n",
        "PATH=os.environ.get('PATH_LOCAL', PATH_REMOTO)\n",
        "PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLnakcu2MOq"
      },
      "source": [
        "### Obtenemos los datos "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "cmp3cLLv28-T",
        "outputId": "2d83d8fc-9241-448a-98a1-6230eb29ce2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>title_ch</th>\n",
              "      <th>Asset Destination</th>\n",
              "      <th>media</th>\n",
              "      <th>impact</th>\n",
              "      <th>start_time_utc</th>\n",
              "      <th>start_time_local</th>\n",
              "      <th>entities_curated</th>\n",
              "      <th>entities</th>\n",
              "      <th>predicted_at_entities</th>\n",
              "      <th>entities_raw_transformers</th>\n",
              "      <th>entities_transformers</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>keywords</th>\n",
              "      <th>predicted_at_keywords</th>\n",
              "      <th>truncated_text</th>\n",
              "      <th>title_and_text</th>\n",
              "      <th>prediction_delay_predictions</th>\n",
              "      <th>prediction_delay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>115028665</td>\n",
              "      <td>Jueves, 18 de julio de 2024 (04:00 GMT)</td>\n",
              "      <td>http://infobae.com/america/agencias/2024/07/18...</td>\n",
              "      <td>Infobae</td>\n",
              "      <td>290177</td>\n",
              "      <td>2024-07-18 04:13:21</td>\n",
              "      <td>2024-07-18 01:13:21</td>\n",
              "      <td>[Joe Biden, Partido Demócrata, Telegram, Unión...</td>\n",
              "      <td>[Trump, Lavrov, Viktor Orbán, Adam Schiff, Ser...</td>\n",
              "      <td>2024-07-18 04:35:11.725225</td>\n",
              "      <td>[{'entities': [{'end': 38, 'entity_group': 'MI...</td>\n",
              "      <td>[Newsroom Infobae Nuevo, EEUU, Biden, Washingt...</td>\n",
              "      <td>Jueves, 18 de julio de 2024 (04:00 GMT)</td>\n",
              "      <td>18 Jul, 2024 Por  Newsroom Infobae Nuevo EEUU ...</td>\n",
              "      <td>[elecciones presidenciales, campaña, apoyo pol...</td>\n",
              "      <td>2024-07-18 04:40:25.949330</td>\n",
              "      <td>18 Jul, 2024 Por  Newsroom Infobae Nuevo EEUU ...</td>\n",
              "      <td>Jueves, 18 de julio de 2024 (04:00 GMT)\\n18 Ju...</td>\n",
              "      <td>0.087284</td>\n",
              "      <td>0.451375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    asset_id                                 title_ch  \\\n",
              "0  115028665  Jueves, 18 de julio de 2024 (04:00 GMT)   \n",
              "\n",
              "                                   Asset Destination    media  impact  \\\n",
              "0  http://infobae.com/america/agencias/2024/07/18...  Infobae  290177   \n",
              "\n",
              "       start_time_utc    start_time_local  \\\n",
              "0 2024-07-18 04:13:21 2024-07-18 01:13:21   \n",
              "\n",
              "                                    entities_curated  \\\n",
              "0  [Joe Biden, Partido Demócrata, Telegram, Unión...   \n",
              "\n",
              "                                            entities  \\\n",
              "0  [Trump, Lavrov, Viktor Orbán, Adam Schiff, Ser...   \n",
              "\n",
              "       predicted_at_entities  \\\n",
              "0 2024-07-18 04:35:11.725225   \n",
              "\n",
              "                           entities_raw_transformers  \\\n",
              "0  [{'entities': [{'end': 38, 'entity_group': 'MI...   \n",
              "\n",
              "                               entities_transformers  \\\n",
              "0  [Newsroom Infobae Nuevo, EEUU, Biden, Washingt...   \n",
              "\n",
              "                                     title  \\\n",
              "0  Jueves, 18 de julio de 2024 (04:00 GMT)   \n",
              "\n",
              "                                                text  \\\n",
              "0  18 Jul, 2024 Por  Newsroom Infobae Nuevo EEUU ...   \n",
              "\n",
              "                                            keywords  \\\n",
              "0  [elecciones presidenciales, campaña, apoyo pol...   \n",
              "\n",
              "       predicted_at_keywords  \\\n",
              "0 2024-07-18 04:40:25.949330   \n",
              "\n",
              "                                      truncated_text  \\\n",
              "0  18 Jul, 2024 Por  Newsroom Infobae Nuevo EEUU ...   \n",
              "\n",
              "                                      title_and_text  \\\n",
              "0  Jueves, 18 de julio de 2024 (04:00 GMT)\\n18 Ju...   \n",
              "\n",
              "   prediction_delay_predictions  prediction_delay  \n",
              "0                      0.087284          0.451375  "
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ingestar noticias\n",
        "\n",
        "# Desde Hugginface\n",
        "date_choice = '2024-07-18'  # formato aaaa-mm-dd\n",
        "path_file = f\"jganzabalseenka/news_{date_choice}_24hs\"\n",
        "dataset = load_dataset(path_file)\n",
        "df_parquet = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Local \"\"\" el formato del archivo parque debe ser fecha aaaammdd \"\n",
        "# date_choice = '2024-07-15' \n",
        "# path_file = f\"{date_choice}.parquet\"\n",
        "# df_parquet = pd.read_parquet(PATH+path_file)\n",
        "\n",
        "df_parquet.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17242"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cantidad total de documentos\n",
        "len(df_parquet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validar datos para la fecha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registros para la fecha 20240718 -> 13743 de un total de 17242\n"
          ]
        }
      ],
      "source": [
        "choice = \"\".join(date_choice.split('-'))\n",
        "df_parquet.sort_values(\"start_time_local\", ascending=True, inplace=True)\n",
        "df_date_filtered = df_parquet[df_parquet['start_time_local'].dt.date == pd.to_datetime(choice).date()]\n",
        "print(f\"Registros para la fecha {choice} -> {len(df_date_filtered)} de un total de {len(df_parquet)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtenemos un lote de N noticias por dia (para agilizar el procesamiento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch: 500\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>asset_id</th>\n",
              "      <th>title_ch</th>\n",
              "      <th>Asset Destination</th>\n",
              "      <th>media</th>\n",
              "      <th>impact</th>\n",
              "      <th>start_time_utc</th>\n",
              "      <th>start_time_local</th>\n",
              "      <th>entities_curated</th>\n",
              "      <th>entities</th>\n",
              "      <th>...</th>\n",
              "      <th>entities_raw_transformers</th>\n",
              "      <th>entities_transformers</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>keywords</th>\n",
              "      <th>predicted_at_keywords</th>\n",
              "      <th>truncated_text</th>\n",
              "      <th>title_and_text</th>\n",
              "      <th>prediction_delay_predictions</th>\n",
              "      <th>prediction_delay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2713</td>\n",
              "      <td>115063920</td>\n",
              "      <td>Rosario: remodelan la avenida que conecta la c...</td>\n",
              "      <td>http://miradorprovincial.com/index.php/id_um/3...</td>\n",
              "      <td>Mirador Provincial</td>\n",
              "      <td>43</td>\n",
              "      <td>2024-07-18 14:17:43</td>\n",
              "      <td>2024-07-18 11:17:43</td>\n",
              "      <td>[Municipalidad de Rosario, Ciro Seisas, Maximi...</td>\n",
              "      <td>[Sergio Cardozo, Felipe Michlig, Tomás de Roca...</td>\n",
              "      <td>...</td>\n",
              "      <td>[{'entities': [], 'text': ': remodelan la aven...</td>\n",
              "      <td>[Lisandro Enrico, Jorge Newbery, Circunvalació...</td>\n",
              "      <td>Rosario: remodelan la avenida que conecta la c...</td>\n",
              "      <td>Con la presencia del ministro Lisandro Enrico...</td>\n",
              "      <td>[distintas obras públicas, rosario, aeropuerto...</td>\n",
              "      <td>2024-07-18 14:28:48.424140</td>\n",
              "      <td>Con la presencia del ministro Lisandro Enrico,...</td>\n",
              "      <td>Rosario: remodelan la avenida que conecta la c...</td>\n",
              "      <td>0.161595</td>\n",
              "      <td>0.18484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   index   asset_id                                           title_ch  \\\n",
              "0   2713  115063920  Rosario: remodelan la avenida que conecta la c...   \n",
              "\n",
              "                                   Asset Destination               media  \\\n",
              "0  http://miradorprovincial.com/index.php/id_um/3...  Mirador Provincial   \n",
              "\n",
              "   impact      start_time_utc    start_time_local  \\\n",
              "0      43 2024-07-18 14:17:43 2024-07-18 11:17:43   \n",
              "\n",
              "                                    entities_curated  \\\n",
              "0  [Municipalidad de Rosario, Ciro Seisas, Maximi...   \n",
              "\n",
              "                                            entities  ...  \\\n",
              "0  [Sergio Cardozo, Felipe Michlig, Tomás de Roca...  ...   \n",
              "\n",
              "                           entities_raw_transformers  \\\n",
              "0  [{'entities': [], 'text': ': remodelan la aven...   \n",
              "\n",
              "                               entities_transformers  \\\n",
              "0  [Lisandro Enrico, Jorge Newbery, Circunvalació...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Rosario: remodelan la avenida que conecta la c...   \n",
              "\n",
              "                                                text  \\\n",
              "0   Con la presencia del ministro Lisandro Enrico...   \n",
              "\n",
              "                                            keywords  \\\n",
              "0  [distintas obras públicas, rosario, aeropuerto...   \n",
              "\n",
              "       predicted_at_keywords  \\\n",
              "0 2024-07-18 14:28:48.424140   \n",
              "\n",
              "                                      truncated_text  \\\n",
              "0  Con la presencia del ministro Lisandro Enrico,...   \n",
              "\n",
              "                                      title_and_text  \\\n",
              "0  Rosario: remodelan la avenida que conecta la c...   \n",
              "\n",
              "  prediction_delay_predictions  prediction_delay  \n",
              "0                     0.161595           0.18484  \n",
              "\n",
              "[1 rows x 21 columns]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_news=os.environ.get('BATCH_NEWS', 1000)\n",
        "\n",
        "df_batch = df_date_filtered.sample(n=int(batch_news)).reset_index()\n",
        "data = list(df_batch['text'])\n",
        "print(f\"Batch: {len(data)}\")\n",
        "df_batch.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar a disco el dataframe filtrado \n",
        "df_batch.to_parquet(PATH+choice+\".parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StopWords\n",
        "Se genera una lista especial de stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "SPANISH_STOPWORDS = list(pd.read_csv(PATH+'spanish_stop_words.csv' )['stopwords'].values)\n",
        "SPANISH_STOPWORDS_SPECIAL = list(pd.read_csv(PATH+'spanish_stop_words_spec.csv' )['stopwords'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' import csv\\n# Guardar la lista de stopwords especial en un archivo CSV\\nwith open(PATH+\"spanish_stop_words_spec.csv\", mode=\\'w\\', newline=\\'\\', encoding=\\'utf-8\\') as archivo:\\n    escritor = csv.writer(archivo)\\n    escritor.writerow([\\'stopwords\\'])\\n    for stopword in SPANISH_STOPWORDS_SPECIAL:\\n        escritor.writerow([stopword]) '"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" import csv\n",
        "# Guardar la lista de stopwords especial en un archivo CSV\n",
        "with open(PATH+\"spanish_stop_words_spec.csv\", mode='w', newline='', encoding='utf-8') as archivo:\n",
        "    escritor = csv.writer(archivo)\n",
        "    escritor.writerow(['stopwords'])\n",
        "    for stopword in SPANISH_STOPWORDS_SPECIAL:\n",
        "        escritor.writerow([stopword]) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER - Named Entity Recognition\n",
        "Obtener entidades de las noticias \n",
        "\n",
        "-   Nota: Aunque el dataset original provee keywords y entities, se realiza el proceso de generacion propia de keywords y entites y se utilizan para modelar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el modelo de spaCy para español\n",
        "spa = spacy.load(\"es_core_news_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar o saltar carga y procesar celda inferior\n",
        "#with open(PATH+f'modelos/entities_{str(choice)}.json', 'r') as json_file:\n",
        "#    entities = json.load(json_file)\n",
        "\n",
        "#with open(PATH+f'modelos/keywords_{str(choice)}.json', 'r') as json_file:\n",
        "#    keywords = json.load(json_file)\n",
        "\n",
        "#with open(PATH+f'modelos/vocabulary_{str(choice)}.json', 'r') as json_file:\n",
        "#    vocab = json.load(json_file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:48<00:00, 10.32it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar entidades para todos los documentos usando spaCy\n",
        "# se procesa utilizando un criterio de seleccion reflejado en el codigo\n",
        "\n",
        "entities = []\n",
        "for data_in in tqdm(data):\n",
        "\n",
        "    # Contabilizar palabras en doc original\n",
        "    normalized_text = re.sub(r'\\W+', ' ', data_in.lower())\n",
        "    words_txt_without_stopwords = [word for word in normalized_text.split() if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL]\n",
        "    words_txt_counter = Counter(words_txt_without_stopwords)\n",
        "    words_counter = {elemento: cuenta for elemento, cuenta in sorted(words_txt_counter.items(), key=lambda item:item[1], reverse=True) if cuenta > 1}\n",
        "\n",
        "    # Extraer entidades del doc segun atributos\n",
        "    extract = spa(data_in)\n",
        "    entidades_spacy = [(ent.text, ent.label_) for ent in extract.ents]\n",
        "    ent_select = [ent for ent in entidades_spacy if ent[1] == 'PER' or ent[1] == 'ORG' or ent[1] == 'LOC' ]\n",
        "\n",
        "    # Extraer entidades de \"maximo 3 palabras\"\n",
        "    ent_max_3 = [ent[0] for ent in ent_select if len(ent[0].split()) <= 3]\n",
        "    ent_clean = clean_all(ent_max_3, accents=False)\n",
        "    ent_unique = list(set([ word for word in ent_clean if word not in SPANISH_STOPWORDS+SPANISH_STOPWORDS_SPECIAL] ))\n",
        "\n",
        "    ents_proc = {}\n",
        "    for ent in ent_unique:\n",
        "        \n",
        "        # Criterio de selección \n",
        "        weight = 0\n",
        "        for word in ent.split():\n",
        "            if word in words_counter:\n",
        "                weight += 1 /len(ent.split()) * words_counter[word]\n",
        "        \n",
        "        ents_proc[ent] = round(weight,4)\n",
        "\n",
        "    ents_proc_sorted = {k: v for k, v in sorted(ents_proc.items(), key=lambda item: item[1], reverse=True) if v > 0}\n",
        "\n",
        "    # Crear la lista preliminar de entidades procesadas por noticia \n",
        "    pre_entities = [key for key, _ in ents_proc_sorted.items()] \n",
        "\n",
        "    # Obtener las últimas palabras de cada entidad que tenga mas de una palabra por entidad\n",
        "    last_words = list(set([ent.split()[-1] for ent in pre_entities if len(ent.split()) > 1 ]))\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al final de una compuesta\n",
        "    pre_entities_without_last_word_equal = []\n",
        "    for idx, ent in enumerate(pre_entities):\n",
        "        if not (len(ent.split()) == 1 and ent in last_words):\n",
        "            pre_entities_without_last_word_equal.append(ent)\n",
        "\n",
        "    # Obtener las palabras únicas\n",
        "    unique_words = [ ent.split()[0] for ent in pre_entities_without_last_word_equal if len(ent.split()) > 1 ]\n",
        "\n",
        "    # Eliminar palabra única si la encuentra al comienzo de una compuesta\n",
        "    pre_entities_without_first_word_equal = []\n",
        "    for idx, ent in enumerate(pre_entities_without_last_word_equal):\n",
        "        if not (len(ent.split()) == 1 and ent in unique_words):\n",
        "            pre_entities_without_first_word_equal.append(ent)\n",
        "\n",
        "    # obtener entidades filtradas\n",
        "    if len(pre_entities_without_first_word_equal) > 10:\n",
        "        umbral = 10 + (len(pre_entities_without_first_word_equal)-10) // 2\n",
        "        filter_entities = pre_entities_without_first_word_equal[:umbral] \n",
        "    else:\n",
        "        filter_entities = pre_entities_without_first_word_equal[:10]\n",
        "\n",
        "    pre_original_entities = []\n",
        "    # capturar las entidades en formato original\n",
        "    for ent in filter_entities:\n",
        "        pre_original_entities.append([elemento for elemento in ent_max_3 if elemento.lower() == ent.lower()])\n",
        "\n",
        "    sort_original_entities = sorted(pre_original_entities, key=len, reverse=True)\n",
        "    \n",
        "    try:\n",
        "        entities.append( [ent[0] for ent in sort_original_entities if ent] ) \n",
        "    except Exception as e:\n",
        "        entities.append([])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'preproc_notebook/entities_{str(choice)}.json', 'w') as file:\n",
        "    json.dump(entities, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keywords\n",
        "Obtener palabras clave de las noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:45<00:00, 11.09it/s]\n"
          ]
        }
      ],
      "source": [
        "# Detectar keywords para todos los documentos usando spaCy\n",
        "\n",
        "keywords_spa = []\n",
        "for doc in tqdm(data):\n",
        "    extract = spa(doc)\n",
        "    keywords_spa.append([(ext.text, ext.pos_) for ext in extract])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keyboards with neighboards\n",
        "- Se seleccionan keywords unigrama y bigrama mediante la funcion keywords_with_neighboards(), que a su vez llama a las funciones get_bigrams() y get_neighbor_words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funcion para obtener keywords con combinaciones de bigramas\n",
        "def get_bigrams(word_list, number_consecutive_words=2):\n",
        "    \n",
        "    ngrams = []\n",
        "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
        "    \n",
        "    for word_index in range(adj_length_of_word_list):\n",
        "        \n",
        "        # Indexar la lista \n",
        "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
        "        \n",
        "        # Agregar a la lista de \"ngrams\"\n",
        "        ngrams.append(ngram)\n",
        "        \n",
        "    return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# devolver las palabras más frecuentes que aparecen junto a una palabra clave en particular\n",
        "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
        "    \n",
        "    neighbor_words = []\n",
        "    keyword = keyword.lower()\n",
        "    \n",
        "    for bigram in bigrams:\n",
        "        \n",
        "        # Extrae solo las palabras en minúsculas (no las etiquetas) para cada bigrama\n",
        "        words = [word.lower() for word, label in bigram]        \n",
        "        \n",
        "        # Comprueba si la palabra clave está en el bigram\n",
        "        if keyword in words:\n",
        "            idx = words.index(keyword)\n",
        "            for word, label in bigram:\n",
        "                \n",
        "                #Ahora nos centramos en la palabra vecina, no en la palabra clave\n",
        "                if word.lower() != keyword:\n",
        "                    #Si la palabra vecina coincide con la pos_label correcta, agregarla a la lista maestra\n",
        "                    if label == pos_label or pos_label == None:\n",
        "                        if idx == 0:\n",
        "                            neighbor_words.append(\" \".join([keyword, word.lower()]))\n",
        "                        else:\n",
        "                            neighbor_words.append(\" \".join([word.lower(), keyword]))\n",
        "                    \n",
        "    return Counter(neighbor_words).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keywords_with_neighboards(keywords_spa, POS_1='NOUN', POS_2='ADJ'):\n",
        "    \"\"\"\n",
        "    Funcion que devuelve dos listas:\n",
        "    - lista de keywords with neighboards (segun argumentos POS_1 y POS_2)\n",
        "    - lista de keywords mas frecuentes (segun argumentos POS_1 y POS_2)\n",
        "    \"\"\"\n",
        "\n",
        "    doc_kwn = []\n",
        "    commons = []\n",
        "    for keywords in keywords_spa:\n",
        "    \n",
        "        # Obtenemos las keywords del tipo (Universal Dependences) mas frecuentes de cada doc (spaCy format)\n",
        "        words = []\n",
        "        for k_spa in keywords:\n",
        "            if k_spa[1] == POS_1:\n",
        "                words.append(k_spa[0])\n",
        "\n",
        "        cont_words = Counter(words)\n",
        "\n",
        "        common = cont_words.most_common()\n",
        "        commons.append( [com for com in common if com[1] > 1] )\n",
        "\n",
        "        # Calcular un umbral de corte (en repeticiones) para los keywords obtenidos\n",
        "            ## suma de todos los valores\n",
        "        valores = [valor for _, valor in common]\n",
        "\n",
        "            ## Calcular los pesos como proporcionales a los valores mismos\n",
        "        pesos = np.array(valores) / np.sum(valores)\n",
        "\n",
        "            ## Calcular el umbral ponderado, valor 2 o superior ( debe repetirse la keyword al menos una vez )\n",
        "        threshold = max(2, round(np.sum(np.array(valores) * pesos),4))\n",
        "\n",
        "\n",
        "        # Obtenemos los bigramas del doc        \n",
        "        tokens_and_labels = [(token[0], token[1]) for token in keywords if token[0].isalpha()]\n",
        "\n",
        "        bigrams = get_bigrams(tokens_and_labels)\n",
        "\n",
        "        keywords_neighbor = []\n",
        "        for item_common in common:\n",
        "            if item_common[1] >= threshold or len(keywords_neighbor) < 6: # corte por umbral o menor a 6\n",
        "                \n",
        "                kwn = get_neighbor_words(item_common[0], bigrams, pos_label=POS_2)\n",
        "                if kwn != []:\n",
        "                    keywords_neighbor.append( kwn )\n",
        "\n",
        "        sorted_keywords_neighbor = sorted([item for sublist in keywords_neighbor for item in sublist ], key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        doc_kwn.append(sorted_keywords_neighbor)\n",
        "\n",
        "    return doc_kwn, commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obtenemos keywords with neighboards y keywords mas frecuentes\n",
        "k_w_n, keyword_single = keywords_with_neighboards(keywords_spa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('hecho implicado', 2),\n",
              " ('accidentes viales', 2),\n",
              " ('tránsito involucrado', 2),\n",
              " ('contravenciones correspondientes', 2),\n",
              " ('estacionados es', 2),\n",
              " ('daños materiales', 2)]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# muestra de keywords with neighboards\n",
        "k_w_n[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filtramos las que al menos se repiten una vez\n",
        "filtered_k_w_n = [ [tupla[0] for tupla in sublista if tupla[1] > 1] for sublista in k_w_n ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# muestra\n",
        "filtered_k_w_n[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Analizamos los keywords unigrama\n",
        "keyword_single[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Si un keyword unigrama coincide en los bigramas elegidos se descarta\n",
        "# la cantidad de keywords se obtiene utilizando la media como umbral de corte\n",
        "\n",
        "# Umbral\n",
        "values = [value for sublist in keyword_single for _, value in sublist]\n",
        "threshold = np.mean(values)\n",
        "\n",
        "for i, sublist in enumerate(keyword_single):\n",
        "    lista_k_w_n = list(set([word for sentence in filtered_k_w_n[i] for word in sentence.split()]))\n",
        "    for tupla in sublist:\n",
        "        if tupla[1] >= threshold and tupla[0] not in lista_k_w_n:\n",
        "            filtered_k_w_n[i].append(tupla[0])\n",
        "\n",
        "keywords = filtered_k_w_n      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grabar\n",
        "with open(PATH+f'preproc_notebook/keywords_{choice}.json', 'w') as file:\n",
        "    json.dump(keywords, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BOW - Armado del vocabulario con las entidades y keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Unificar Entities + Keywords + Keywords with neighboards\n",
        "vocab = list(set().union(*entities, *keywords))\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar vocabulario\n",
        "with open(PATH+f'preproc_notebook/vocabulary_{choice}.json', 'w') as file:\n",
        "    json.dump(vocab, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar noticias en el indice news de la base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "500it [00:00, 599.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documentos insertados correctamente\n"
          ]
        }
      ],
      "source": [
        "# configurar  batch_size = ( ej.: 5000 ) si se supera el limite 100MB en elasticsearch por operacion\n",
        "index_name = 'news'\n",
        "bulk_data = []\n",
        "\n",
        "for idx, text_news in tqdm(enumerate(data)):\n",
        "    doc = {\n",
        "        'index': {\n",
        "            '_index': index_name,\n",
        "            '_id': int(df_batch.iloc[idx]['asset_id'])\n",
        "        }\n",
        "    }\n",
        "    reg = {\n",
        "        'title': str(df_batch.iloc[idx].title),\n",
        "        'news' : str(text_news), \n",
        "        'author': str(df_batch.iloc[idx]['media']),\n",
        "        'vector': None,\n",
        "        'keywords' : keywords[idx],\n",
        "        'entities' : entities[idx],\n",
        "        'created_at': parse(str(df_batch.iloc[idx]['start_time_local'])).isoformat(),\n",
        "        'pos_id': -1,\n",
        "        'process': False\n",
        "    }\n",
        "    bulk_data.append(json.dumps(doc))\n",
        "    bulk_data.append(json.dumps(reg))\n",
        "\n",
        "# Convertir la lista en un solo string separado por saltos de línea\n",
        "bulk_request_body = '\\n'.join(bulk_data) + '\\n'\n",
        "\n",
        "# Enviar la solicitud bulk\n",
        "response = os_client.bulk(body=bulk_request_body)\n",
        "\n",
        "if response['errors']:\n",
        "    print(\"Errores encontrados al insertar los documentos\")\n",
        "else:\n",
        "    print(\"Documentos insertados correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Funcion para comparar resultados entre keywords y entities provistas y las generadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noticia ID: 115067934 ['El desafío de la hija de Cristiano Ronaldo y Georgina Rodríguez que provocó emoción en sus padres']\n",
            "\n",
            "Entities de dataframe: ['Bella Esmeralda', 'Lu', 'CR7', 'Georgina', 'Cristiano Ronaldo', 'Cristiano', 'Elon Musk', 'Georgina Rodríguez', 'Cris', 'Ronaldo', 'Al-Nassr F C', 'Twitter']\n",
            "Keywords de dataframe: ['merecidas vacaciones', 'los padres', 'emoción', 'el desafío', 'futbolista portugués', 'seguidores', 'profundo amor', 'conexión especial', 'cariño', 'la niña', 'ternura', 'faceta', 'complicidad familiar', 'admiración', 'reproducciones', 'interacción', 'clip compartido', 'traje', 'frases', 'ejemplos', 'grabación', 'intentos', 'publicaciones', 'voces críticas', 'qué felicidad', 'frase', 'estrella', 'celebración', 'alegría', 'plataformas', 'deporte', 'famoso delantero', 'mensajes afectuosos', 'color rosa', 'comentarios', 'publicación compartida', 'hija pequeña', 'plataforma', 'fútbol internacional', 'apoyo']\n",
            "--------------------------------------------------------------------------------\n",
            "Entities generadas: ['Cristiano Ronaldo', 'Bella Esmeralda']\n",
            "Keywords generadas: ['merecidas vacaciones', 'redes sociales', 'hija', 'millones', 'seguidores', 'amor', 'futbolista']\n",
            "\n",
            " [\"En las últimas horas, el famoso delantero del Al-Nassr F. C., Cristiano Ronaldo, ha compartido un emotivo video durante sus merecidas vacaciones que ha cautivado a millones de seguidores en redes sociales. En la grabación, se ve cómo Cristiano y su pareja enseñan a su hija pequeña Bella Esmeralda a decir 'Te amo, papá'. Tras varios intentos, la niña de dos años y medio logra el desafío, provocando la emoción y celebración de ambos padres. Cristiano Ronaldo logra que su hija repita la misma frase, pero esta vez sustituyendo 'papá' por 'mamá', generando un momento de ternura y alegría en la familia. El vídeo, que se ha vuelto viral en plataformas como Instagram y X (antes conocida como Twitter), muestra a los padres y la niña disfrutando de unas vacaciones veraniegas en traje de baño, demostrando el profundo amor que Georgina y Cristiano sienten por su pequeña. Una publicación compartida por Cuenta Oficial de ¡HOLA! TV (@holatv) El conmovedor clip compartido por el futbolista portugués\\xa0ha acumulado más de 41 millones de reproducciones en la plataforma de Elon Musk y ha recibido más de 6,3 millones de likes en la red social de color rosa. Los fanáticos de 'CR7' han mostrado su cariño y apoyo en los comentarios, expresando su admiración por el jugador y su conexión con la dulce Bella Esmeralda. Informa Voces Críticas. Los seguidores han llenado las publicaciones de Cristiano Ronaldo\\xa0con mensajes afectuosos y divertidos, reflejando el amor y la gratitud que sienten hacia el futbolista. Frases como ''Yo también te amo Cris'', ''Qué felicidad verte así'', ''Te amo Ronaldo'', y ''Linda Familia Cris'' son solo algunos ejemplos de la muestra de cariño y admiración hacia el ídolo del deporte y su hermosa familia.  Esta tierna y genuina interacción entre el Luso\\xa0y su hija Bella Esmeralda ha conmovido a las redes sociales, destacando la faceta más humana y familiar del reconocido futbolista en medio de sus merecidas vacaciones. La conexión especial entre padre e hija ha resonado entre los seguidores, quienes celebran este hermoso momento de amor y complicidad familiar compartido por la estrella del fútbol internacional. \\xa0\"]\n"
          ]
        }
      ],
      "source": [
        "def funcion_aux(ID):\n",
        "    keywords_df = df_parquet[df_parquet.asset_id==ID]['keywords'].values[0]\n",
        "    entities_df = df_parquet[df_parquet.asset_id==ID]['entities'].values[0]\n",
        "    print(f\"Noticia ID: {ID} {df_parquet[df_parquet.asset_id==ID]['title'].values}\\n\")\n",
        "    print(f\"Entities de dataframe: {entities_df}\")\n",
        "    print(f\"Keywords de dataframe: {keywords_df}\")\n",
        "    print(\"-\"*80)\n",
        "    condicion = df_batch['asset_id'] == ID\n",
        "    posicion = df_batch.index[condicion].tolist()[0]\n",
        "    posicion_ordinal = df_batch.index.get_loc(posicion)\n",
        "    print(f\"Entities generadas: {entities[posicion_ordinal]}\")\n",
        "    print(f\"Keywords generadas: {filtered_k_w_n[posicion_ordinal]}\")\n",
        "\n",
        "\n",
        "ID = df_batch.iloc[np.random.randint(len(df_batch))]['asset_id']\n",
        "funcion_aux(ID)\n",
        "print(f\"\\n {df_batch[df_batch.asset_id == ID].text.values}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El criterio de selección de entities y keywords se basa en seleccionar solo las que han sido mayormente nombradas, sin embargo es solo un criterio, y podrían obtenerse una mayor cantidad o la totalidad ajustando el criterio de seleccion."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGxTNO4yTgb9k2r4ffbTN0",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
